(Files content cropped to 300k characters, download full ingest to see more)
================================================
FILE: README.md
================================================
# Megakernels!

## Installation

Clone this repo and run:

```bash
git submodule update --init --recursive
pip install uv
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
uv pip install -e .
```

## Low-Latency Llama Demo

First, to compile the megakernel, run:

```bash

# from the repo root
export THUNDERKITTENS_ROOT=$(pwd)/ThunderKittens
export MEGAKERNELS_ROOT=$(pwd)
export PYTHON_VERSION=3.12 # adjust if yours is different
export GPU=H100 # options are {H100, B200}, else defaults to B200
cd demos/low-latency-llama
make

```

To start an interactive chat session with the model, run:

```bash

# from the repo root
python megakernels/scripts/llama_repl.py

```

To benchmark the megakernel, run:

```bash

# from the repo root
python megakernels/scripts/generate.py mode=mk prompt="tell me a funny joke about cookies" ntok=100

```



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 HazyResearch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "megakernels"
version = "0.0.1"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "transformers==4.48.3",
    "pydra-config>=0.0.13",
    "accelerate",
    "tabulate",
    "tqdm",
    "matplotlib",
    "einops",
    "pyright",
    "openai",
    "psutil",
    "art",
    "pybind11",
]

[tool.setuptools]
packages = {find = {}}


================================================
FILE: demos/low-latency-llama/attention_partial.cu
================================================
#include "llama.cuh"

using namespace kittens;
using namespace megakernel;

template <typename config, typename globals> struct attention_partial {
    static constexpr int opcode = OPCODE_PartialAttention;
    static constexpr int NUM_STAGES = 3;
    static constexpr int GQA_RATIO =
        LLAMA_1B_NUM_ATTENTION_HEADS / LLAMA_1B_NUM_KV_HEADS;
    static constexpr int QOL_PAGE = 0;
    static constexpr int KV_PAGE = 1;

    static_assert(GQA_RATIO == 4, "GQA_RATIO must be 4.");
    static_assert(NUM_STAGES <= 4, "Modify page allocation for KVs.");

    using q_rt = kittens::rt_bf<16, LLAMA_1B_HEAD_DIM>; // only 4 rows are used
    using q_st = kittens::st_bf<16, LLAMA_1B_HEAD_DIM>; // only 4 rows are used
    using k_rt = kittens::rt_bf<LLAMA_1B_KV_BLOCK_SIZE, LLAMA_1B_HEAD_DIM>;
    using v_rt = kittens::rt_bf<LLAMA_1B_KV_BLOCK_SIZE, LLAMA_1B_HEAD_DIM, col_l>;
    using kv_st = kittens::st_bf<LLAMA_1B_KV_BLOCK_SIZE, LLAMA_1B_HEAD_DIM>;
    using attn_fl_rt =
        kittens::rt_fl<16, LLAMA_1B_KV_BLOCK_SIZE>; // only 4 values are used
    using attn_bf_rt =
        kittens::rt_bf<16, LLAMA_1B_KV_BLOCK_SIZE>; // only 4 values are used
    using max_vec_rv =
        col_vec<kittens::rt_fl<16, LLAMA_1B_HEAD_DIM>>; // only 4 values are used
    using max_vec_sv = kittens::sv_fl<16>;              // only 4 values are used
    using norm_vec_rv =
        col_vec<kittens::rt_fl<16, LLAMA_1B_HEAD_DIM>>; // only 4 values are used
    using norm_vec_sv = kittens::sv_fl<16>;             // only 4 values are used
    using l_rv =
        col_vec<kittens::rt_fl<16, LLAMA_1B_HEAD_DIM>>; // only 4 values are used
    using l_sv = kittens::sv_fl<16>;                    // only 4 values are used
    using o_rt = kittens::rt_fl<16, LLAMA_1B_HEAD_DIM>; // only 4 rows are used
    using o_sv = kittens::sv_fl<LLAMA_1B_HEAD_DIM>;
    using o_sv_bf = kittens::sv_bf<LLAMA_1B_HEAD_DIM>;

    struct parsed_instruction {
        int layer_idx;
        int kv_head_idx;
        int num_partials;
        int partial_idx;
        __device__ inline parsed_instruction(
            typename config::instruction_t &instruction) {
            layer_idx = instruction[1];
            kv_head_idx = instruction[2];
            num_partials = instruction[3];
            partial_idx = instruction[4];
        }
        __device__ inline parsed_instruction(megakernel::state<config> &s)
            : parsed_instruction(s.instruction()) {}
    };

    // We have 32 dynamic kittens::semaphores total
    __device__ static inline kittens::semaphore &Q_arrived(megakernel::state<config> &s) {
        return s.semaphores()[0];
    }
    __device__ static inline kittens::semaphore &O_arrived(megakernel::state<config> &s) {
        return s.semaphores()[1];
    }
    __device__ static inline kittens::semaphore &L_arrived(megakernel::state<config> &s) {
        return s.semaphores()[2];
    }
    __device__ static inline kittens::semaphore &K_arrived(megakernel::state<config> &s, int stage) {
        return s.semaphores()[3 + stage * 2];
    }
    __device__ static inline kittens::semaphore &V_arrived(megakernel::state<config> &s, int stage) {
        return s.semaphores()[3 + stage * 2 + 1];
    }
    __device__ static inline kittens::semaphore &K_finished(megakernel::state<config> &s,
                                                   int stage) {
        return s.semaphores()[3 + NUM_STAGES * 2 + stage * 2];
    }
    __device__ static inline kittens::semaphore &V_finished(megakernel::state<config> &s,
                                                   int stage) {
        return s.semaphores()[3 + NUM_STAGES * 2 + stage * 2 + 1];
    }

    __device__ static inline void wait_QOL_page(megakernel::state<config> &s) {
        s.wait_page_ready(s.pid(QOL_PAGE));
    }
    __device__ static inline void wait_KV_page(megakernel::state<config> &s) {
        s.wait_page_ready(s.pid(KV_PAGE));
    }
    __device__ static inline void finish_QOL_page(megakernel::state<config> &s) {
        if (kittens::warp::laneid() == 0)
            s.finish_page(s.pid(QOL_PAGE), config::NUM_CONSUMER_WARPS);
    }
    __device__ static inline void finish_KV_page(megakernel::state<config> &s) {
        if (kittens::warp::laneid() == 0)
            s.finish_page(s.pid(KV_PAGE), config::NUM_CONSUMER_WARPS);
    }
    __device__ static inline q_st &get_Q_smem(megakernel::state<config> &s) {
        int pid = s.pid(QOL_PAGE);
        return *reinterpret_cast<q_st *>(s.pages[pid].data);
    }
    __device__ static inline o_sv (&get_O_smem(megakernel::state<config> &s))[4] {
        int pid = s.pid(QOL_PAGE);
        return *reinterpret_cast<o_sv(*)[4]>(
            reinterpret_cast<char *>(s.pages[pid].data) + sizeof(q_st));
    }
    __device__ static inline l_sv &get_L_smem(megakernel::state<config> &s) {
        int pid = s.pid(QOL_PAGE);
        return *reinterpret_cast<l_sv *>(
            reinterpret_cast<char *>(s.pages[pid].data) + sizeof(q_st) +
            sizeof(o_sv) * 4);
    }
    __device__ static inline kv_st &get_K_smem(megakernel::state<config> &s, int stage) {
        int pid = s.pid(KV_PAGE);
        return *reinterpret_cast<kv_st *>(
            reinterpret_cast<char *>(s.pages[pid].data) +
            sizeof(kv_st) * (stage * 2));
    }
    __device__ static inline kv_st &get_V_smem(megakernel::state<config> &s, int stage) {
        int pid = s.pid(KV_PAGE);
        return *reinterpret_cast<kv_st *>(
            reinterpret_cast<char *>(s.pages[pid].data) +
            sizeof(kv_st) * (1 + stage * 2));
    }

    template <ducks::sv::all SV, ducks::rt::all RT>
    __device__ static inline void
    store_4_rows(SV (&dst)[4], const RT &src, int row4idx /*= 0, 1, 2, or 3*/) {
        static_assert(RT::rows == 16, "src rows must be 16.");
        static_assert(SV::length == src.cols,
                      "dst length must match src cols.");

        using T2 = RT::dtype;
        using T = base_types::packing<T2>::unpacked_type;
        using U = SV::dtype;
        using U2 = base_types::packing<U>::packed_type;

        uint32_t dst_ptr[4];
        dst_ptr[0] =
            static_cast<uint32_t>(__cvta_generic_to_shared(&dst[0].data[0]));
        dst_ptr[1] =
            static_cast<uint32_t>(__cvta_generic_to_shared(&dst[1].data[0]));
        dst_ptr[2] =
            static_cast<uint32_t>(__cvta_generic_to_shared(&dst[2].data[0]));
        dst_ptr[3] =
            static_cast<uint32_t>(__cvta_generic_to_shared(&dst[3].data[0]));

        int laneid = kittens::warp::laneid();
        int local_row_idx = (laneid % 16) / 4;
        int local_col_idx = laneid % 4;

        if (row4idx % 2 == 0 && laneid < 16) { // rows 0~3 or 8~11
            if (row4idx / 2 == 0) {            // rows 0~3
                for (int j = 0; j < src.width; j++) {
                    U2 tmp[2];
                    tmp[0] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[0]);
                    tmp[1] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[2]); // note 2, not 1
                    int col_idx = local_col_idx * 2 + j * 16;
                    move<U2>::sts(dst_ptr[local_row_idx] + sizeof(U) * col_idx,
                                  tmp[0]);
                    move<U2>::sts(dst_ptr[local_row_idx] +
                                      sizeof(U) * (col_idx + 8),
                                  tmp[1]);
                }
            } else { // rows 8~11
                for (int j = 0; j < src.width; j++) {
                    U2 tmp[2];
                    tmp[0] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[1]);
                    tmp[1] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[3]);
                    int col_idx = local_col_idx * 2 + j * 16;
                    move<U2>::sts(dst_ptr[local_row_idx] + sizeof(U) * col_idx,
                                  tmp[0]);
                    move<U2>::sts(dst_ptr[local_row_idx] +
                                      sizeof(U) * (col_idx + 8),
                                  tmp[1]);
                }
            }
        } else if (row4idx % 2 == 1 && laneid >= 16) { // rows 4~7 or 12~15
            if (row4idx / 2 == 0) {                    // rows 4~7
                for (int j = 0; j < src.width; j++) {
                    U2 tmp[2];
                    tmp[0] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[0]);
                    tmp[1] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[2]); // note 2, not 1
                    int col_idx = local_col_idx * 2 + j * 16;
                    move<U2>::sts(dst_ptr[local_row_idx] + sizeof(U) * col_idx,
                                  tmp[0]);
                    move<U2>::sts(dst_ptr[local_row_idx] +
                                      sizeof(U) * (col_idx + 8),
                                  tmp[1]);
                }
            } else { // rows 12~15
                for (int j = 0; j < src.width; j++) {
                    U2 tmp[2];
                    tmp[0] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[1]);
                    tmp[1] = base_types::convertor<U2, T2>::convert(
                        src.tiles[0][j].data[3]);
                    int col_idx = local_col_idx * 2 + j * 16;
                    move<U2>::sts(dst_ptr[local_row_idx] + sizeof(U) * col_idx,
                                  tmp[0]);
                    move<U2>::sts(dst_ptr[local_row_idx] +
                                      sizeof(U) * (col_idx + 8),
                                  tmp[1]);
                }
            }
        }
    }
    template <ducks::rt::row_layout RT>
    __device__ static inline void right_fill(
        RT &dst, const RT &src, const int col_idx,
        const typename base_types::packing<typename RT::dtype>::unpacked_type
            &val = 0) {
        if (col_idx >= dst.cols)
            return;
#pragma unroll
        for (int i = 0; i < dst.height; i++) {
#pragma unroll
            for (int j = 0; j < dst.width; j++) {
#pragma unroll
                for (int k = 0; k < dst.packed_per_tile; k++) {
                    const int col_idx_x = (j * dst.tile_size_col) +
                                          ((k / 2) * 8) +
                                          ((kittens::warp::laneid() % 4) * 2);
                    const int col_idx_y = (j * dst.tile_size_col) +
                                          ((k / 2) * 8) +
                                          ((kittens::warp::laneid() % 4) * 2) + 1;
                    if (col_idx_x >= col_idx) {
                        dst.tiles[i][j].data[k].x = val;
                    } else {
                        dst.tiles[i][j].data[k].x = src.tiles[i][j].data[k].x;
                    }
                    if (col_idx_y >= col_idx) {
                        dst.tiles[i][j].data[k].y = val;
                    } else {
                        dst.tiles[i][j].data[k].y = src.tiles[i][j].data[k].y;
                    }
                }
            }
        }
    }
    // This is super specific to loading Q in a single kittens::warp
    // Mainly two things are different:
    //   1. Ignores Q global dimensions
    //   2. Only loads 4 rows of Q, not 16 (assumes GQA_RATIO == 4) --> only 32
    //   calls needed, so single call per thread
    __device__ static inline void
    load_Q_async(q_st &dst, const globals::activations_t &src,
                 const int q_head_start_idx /*0, 4, 8, ...*/) {
        static_assert(LLAMA_1B_HEAD_DIM == 64 && GQA_RATIO == 4,
                      "Fix this function.");
        using T = typename q_st::dtype;
        constexpr int elem_per_memcpy =
            sizeof(float4) / sizeof(typename q_st::dtype);                  // 8
        constexpr int memcpy_per_row = LLAMA_1B_HEAD_DIM / elem_per_memcpy; // 8

        typename globals::activations_t::dtype *src_ptr =
            &src.raw_ptr[q_head_start_idx * LLAMA_1B_HEAD_DIM];
        uint32_t dst_ptr = static_cast<uint32_t>(__cvta_generic_to_shared(
            &dst.data[(q_head_start_idx % 16) * LLAMA_1B_HEAD_DIM]));

        int laneid = kittens::warp::laneid();
        int row = laneid / memcpy_per_row;
        int col = (laneid * elem_per_memcpy) % LLAMA_1B_HEAD_DIM;

        // everything should fit!
        asm volatile(
            "cp.async.cg.shared.global.L2::128B [%0], [%1], 16;\n" ::"r"(
                dst.idx(dst_ptr, {row, col})),
            "l"(&src_ptr[row * LLAMA_1B_HEAD_DIM + col])
            : "memory");
        asm volatile("cp.async.commit_group;\n" ::: "memory");
    }

    struct controller {
        static __device__ int
        release_lid(const globals &g,
                    typename config::instruction_t &instruction, int &query) {
            int ret_order[13] = {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1};
            return ret_order[query];
        }
        static __device__ int init_semaphores(const globals &g,
                                              megakernel::state<config> &s) {
            init_semaphore(Q_arrived(s), 0, 1);
            init_semaphore(O_arrived(s), 0, 1);
            init_semaphore(L_arrived(s), 0, 1);
            for (int i = 0; i < NUM_STAGES; i++) {
                init_semaphore(K_arrived(s, i), 0, 1);
                init_semaphore(V_arrived(s, i), 0, 1);
                init_semaphore(K_finished(s, i), 0, 1);
                init_semaphore(V_finished(s, i), 0, 1);
            }
            return 3 + 4 * NUM_STAGES;
        }
    };
    struct loader {
        static __device__ void run(const globals &g, megakernel::state<config> &s) {
            auto laneid = kittens::warp::laneid();
            if (laneid >= 2 && laneid < config::NUM_PAGES) {
                int unused_page = s.pid(laneid);
                s.wait_page_ready(unused_page);
                s.finish_page(unused_page, config::NUM_CONSUMER_WARPS);
            }
        }
    };
    struct launcher {
        static __device__ void wait_for_kv(const globals &g, megakernel::state<config> &s,
                                           parsed_instruction &inst) {
            s.record(megakernel::TEVENT_AT_GMEM_WAIT);

            // Wait for the previous ops to finish (16 dims each, so 4 ops on
            // the same head)
            while (*(volatile int *)&g.Bar[{
                       inst.layer_idx, OPCODE_RMS_QKV_MatVecRopeAppend - 1,
                       LLAMA_1B_NUM_ATTENTION_HEADS + inst.kv_head_idx}] < 4) {
                __nanosleep(config::GMEM_SPIN_LOOP_SLEEP_NANOS);
            }

            while (
                *(volatile int *)&g
                     .Bar[{inst.layer_idx, OPCODE_RMS_QKV_MatVecRopeAppend - 1,
                           LLAMA_1B_NUM_ATTENTION_HEADS +
                               LLAMA_1B_NUM_KV_HEADS + inst.kv_head_idx}] < 4) {
                __nanosleep(config::GMEM_SPIN_LOOP_SLEEP_NANOS);
            }

            s.record(megakernel::TEVENT_DONE_GMEM_WAIT);
        }

        static __device__ void run(const globals &g, megakernel::state<config> &s) {
            if (kittens::warp::laneid() == 0) {
#ifdef KITTENS_BLACKWELL
                s.wait_tensor_ready();
                arrive(s.tensor_finished, config::NUM_CONSUMER_WARPS);
#endif

                // Setup
                parsed_instruction inst{s};
                int seq_len = g.pos_id + 1;
                int total_attn_blocks = (seq_len + LLAMA_1B_KV_BLOCK_SIZE - 1) /
                                        LLAMA_1B_KV_BLOCK_SIZE;
                int blocks_per_partial =
                    (total_attn_blocks + inst.num_partials - 1) /
                    inst.num_partials;
                int start_blk_idx = inst.partial_idx * blocks_per_partial;
                int end_blk_idx =
                    min(start_blk_idx + blocks_per_partial, total_attn_blocks);

                // Wait for the KV page
                wait_KV_page(s);

                if (start_blk_idx >= end_blk_idx)
                    finish_KV_page(s);

                // Run the pipeline!
                for (int i = 0; i + start_blk_idx < end_blk_idx; ++i) {
                    auto cur_blk_idx = start_blk_idx + i;
                    int stage = cur_blk_idx % NUM_STAGES;
                    kv_st &K_smem = get_K_smem(s, stage);
                    kv_st &V_smem = get_V_smem(s, stage);

                    if (i >= NUM_STAGES) {
                        kittens::wait(K_finished(s, stage), (i / NUM_STAGES - 1) % 2);
                        kittens::wait(V_finished(s, stage), (i / NUM_STAGES - 1) % 2);
                    }

                    if (cur_blk_idx == end_blk_idx - 1 &&
                        inst.partial_idx == inst.num_partials - 1) {
                        wait_for_kv(g, s, inst);
                    }

                    kittens::tma::expect(K_arrived(s, stage), K_smem);
                    kittens::tma::load_async<dim::DEPTH, cache_policy::EVICT_FIRST>(
                        K_smem, g.k_cache,
                        {inst.layer_idx, cur_blk_idx, inst.kv_head_idx, 0},
                        K_arrived(s, stage));
                    kittens::tma::expect(V_arrived(s, stage), V_smem);
                    kittens::tma::load_async<dim::DEPTH, cache_policy::EVICT_FIRST>(
                        V_smem, g.v_cache,
                        {inst.layer_idx, cur_blk_idx, inst.kv_head_idx, 0},
                        V_arrived(s, stage));
                }
            }
        }
    };
    struct consumer {
        static __device__ void run(const globals &g, megakernel::state<config> &s) {

            if (kittens::warpid() == 0) {
                // Wait for the previous ops to finish1
                parsed_instruction inst{s};
                int q_head_start_idx = inst.kv_head_idx * GQA_RATIO;

                if (kittens::laneid() == 0) {
                    for (int head_offset = 0; head_offset < GQA_RATIO;
                         head_offset++) {
                        while (*(volatile int *)&g
                                    .Bar[{inst.layer_idx,
                                          OPCODE_RMS_QKV_MatVecRopeAppend - 1,
                                          q_head_start_idx + head_offset}] <
                               4) {
                            __nanosleep(config::GMEM_SPIN_LOOP_SLEEP_NANOS);
                        }
                    }
                }
                kittens::warp::sync();

                // Setup
                int q_head_local_idx =
                    (q_head_start_idx % q_rt::tile_size_row) / 4;
                int seq_len = g.pos_id + 1;
                int total_attn_blocks = (seq_len + LLAMA_1B_KV_BLOCK_SIZE - 1) /
                                        LLAMA_1B_KV_BLOCK_SIZE;
                int blocks_per_partial =
                    (total_attn_blocks + inst.num_partials - 1) /
                    inst.num_partials;
                int start_blk_idx = inst.partial_idx * blocks_per_partial;
                int end_blk_idx =
                    min(start_blk_idx + blocks_per_partial, total_attn_blocks);
                float softmax_temp =
                    g.attn_scale * 1.44269504089f; // 1 / (sqrt(D_h) * ln(2))
                q_rt Q_reg;
                k_rt K_reg;
                v_rt V_reg;
                l_rv L_reg;
                o_rt O_reg;
                attn_fl_rt attn_fl_reg;
                attn_bf_rt attn_bf_reg;
                max_vec_rv max_vec_reg;
                max_vec_rv scaled_max_vec_reg;
                max_vec_rv last_scaled_max_vec_reg;
                max_vec_rv diff_scaled_max_vec_reg;
                norm_vec_rv norm_vec_reg;
                kittens::warp::neg_infty(max_vec_reg);
                kittens::warp::zero(last_scaled_max_vec_reg); // just not +-inf
                kittens::warp::zero(norm_vec_reg);
                kittens::warp::zero(O_reg);
                o_sv(&O_smem)[4] = get_O_smem(s);
                l_sv &L_smem = get_L_smem(s);

                // Initiate the load on Q
                wait_QOL_page(s);

                q_st &Q_smem = get_Q_smem(s);

                load_Q_async(Q_smem, g.q_post_rope, q_head_start_idx);

                // Wait for Q to arrive
                kittens::warp::load_async_wait();

                kittens::warp::load(Q_reg, Q_smem);

                // kittens::sv_bf<256> &true_qsmem = *reinterpret_cast<kittens::sv_bf<256>
                // *>(&Q_smem);

                // kittens::warp::load(true_qsmem, g.q_post_rope, {inst.kv_head_idx});
                // kittens::warp::sync();
                // kittens::warp::load(Q_reg, Q_smem);
                // kittens::warp::sync();

                // Run the pipeline!
                for (int i = 0; i + start_blk_idx < end_blk_idx; ++i) {
                    int stage = i % NUM_STAGES;
                    kv_st &K_smem = get_K_smem(s, stage);
                    kv_st &V_smem = get_V_smem(s, stage);

                    // Perform Q @ K.T
                    kittens::warp::zero(attn_fl_reg);
                    kittens::warp::wait(K_arrived(s, stage), (i / NUM_STAGES) % 2);

                    kittens::warp::load(K_reg, K_smem);
                    kittens::warp::mma_ABt(attn_fl_reg, Q_reg, K_reg, attn_fl_reg);
                    kittens::warp::sync();
                    kittens::warp::arrive(K_finished(s, stage));

                    // Mask out invalid positions at the end
                    if ((i + start_blk_idx + 1) * LLAMA_1B_KV_BLOCK_SIZE >
                        seq_len)
                        right_fill(attn_fl_reg, attn_fl_reg,
                                   seq_len % LLAMA_1B_KV_BLOCK_SIZE,
                                   -999999999999.f);

                    // Obtain maximums per row (which is per head)
                    kittens::warp::row_max(max_vec_reg, attn_fl_reg,
                                  max_vec_reg); // includes previous max

                    // Scale attention block and maximums by sqrt(D_h)
                    kittens::warp::mul(attn_fl_reg, attn_fl_reg, softmax_temp);
                    kittens::warp::mul(scaled_max_vec_reg, max_vec_reg, softmax_temp);

                    // Calculate sofkittens::tmax numerator
                    kittens::warp::sub_row(attn_fl_reg, attn_fl_reg, scaled_max_vec_reg);
                    kittens::warp::exp2(attn_fl_reg, attn_fl_reg);

                    // Calculate sofkittens::tmax denominator
                    kittens::warp::sub(diff_scaled_max_vec_reg, last_scaled_max_vec_reg,
                              scaled_max_vec_reg);
                    kittens::warp::exp2(diff_scaled_max_vec_reg,
                               diff_scaled_max_vec_reg);

                    // Normalize and accumulate numerator (A @ V)
                    kittens::warp::mul_row(O_reg, O_reg, diff_scaled_max_vec_reg);
                    kittens::warp::wait(V_arrived(s, stage), (i / NUM_STAGES) % 2);

                    kittens::warp::load(V_reg, V_smem);
                    kittens::warp::copy(attn_bf_reg,
                               attn_fl_reg); // Convert to bf16 to do matmul
                    kittens::warp::mma_AB(O_reg, attn_bf_reg, V_reg, O_reg);
                    kittens::warp::sync();
                    kittens::warp::arrive(V_finished(s, stage));

                    // Normalize and accumulate demoniator
                    kittens::warp::mul(norm_vec_reg, norm_vec_reg,
                              diff_scaled_max_vec_reg);
                    kittens::warp::row_sum(norm_vec_reg, attn_fl_reg, norm_vec_reg);

                    // Save for next iteration
                    kittens::warp::copy(last_scaled_max_vec_reg, scaled_max_vec_reg);
                }

                // Finish
                kittens::warp::sync();

                if (start_blk_idx < end_blk_idx) {
                    finish_KV_page(s);
                    kittens::warp::div_row(O_reg, O_reg, norm_vec_reg);
                    kittens::warp::log2(L_reg, norm_vec_reg);
                    kittens::warp::add(
                        L_reg, L_reg,
                        last_scaled_max_vec_reg); // now L_reg contains the LSE
                } else {
                    // Very edgy case where no blocks are processed.
                    // Make the math work out during attention reduction!
                    kittens::warp::neg_infty(L_reg);
                }

                // Store the results
                store_4_rows(O_smem, O_reg, q_head_local_idx);
                kittens::warp::sync();

                kittens::warp::arrive(O_arrived(s));
                kittens::warp::store(L_smem, L_reg);
                kittens::warp::sync();
                kittens::warp::arrive(L_arrived(s));
            }
        }
    };
    struct storer {

        static inline __device__ void
        store_o_skip(const globals &g, megakernel::state<config> &s, int q_head_start_idx) {
            auto O_smem = get_O_smem(s);

            if (kittens::laneid() == 0) {
                kittens::wait(O_arrived(s), 0);
                s.record(megakernel::TEVENT_OUTPUT_READY);
            }
            kittens::warp::sync();

            kittens::rv_bf<globals::head_dim> O_bf;
            for (int head_offset = 0; head_offset < GQA_RATIO; head_offset++) {
                auto &smem_fl = O_smem[head_offset];
                auto &smem_bf = *reinterpret_cast<o_sv_bf *>(&smem_fl);

                kittens::warp::load(O_bf, smem_fl);
                kittens::warp::sync();
                kittens::warp::store(smem_bf, O_bf);
                kittens::warp::sync();
            }

            if (kittens::laneid() == 0) {
                for (int head_offset = 0; head_offset < GQA_RATIO;
                     head_offset++) {
                    auto &smem_bf =
                        *reinterpret_cast<o_sv_bf *>(&O_smem[head_offset]);
                    kittens::tma::store_async<cache_policy::EVICT_LAST>(
                        g.attn_out, smem_bf, {q_head_start_idx + head_offset});
                }
            }
        }

        static inline __device__ void
        store_o_no_skip(const globals &g, megakernel::state<config> &s,
                        int q_head_start_idx, parsed_instruction &inst) {
            // Store partial attention output to global memory
            if (laneid == 0) {
                o_sv(&O_smem)[GQA_RATIO] = get_O_smem(s);
                kittens::wait(O_arrived(s), 0);
                s.record(megakernel::TEVENT_OUTPUT_READY);

                for (int head_offset = 0; head_offset < GQA_RATIO;
                     head_offset++) {
                    kittens::tma::store_async<cache_policy::EVICT_LAST>(
                        g.attn_out_intermediates, O_smem[head_offset],
                        {0, q_head_start_idx + head_offset, inst.partial_idx,
                         0});
                }
            }
        }

        static __device__ void run(const globals &g, megakernel::state<config> &s) {
            parsed_instruction inst{s};
            int laneid = kittens::warp::laneid();
            int q_head_start_idx =
                inst.kv_head_idx * GQA_RATIO; // 0, 4, 8, 12, 16, 20, 24, 28
            int q_head_vec_start_idx = q_head_start_idx % 16;

            auto skip_attn_reduction = g.skip_attn_reduction;

            if (skip_attn_reduction) {
                store_o_skip(g, s, q_head_start_idx);
            } else {
                store_o_no_skip(g, s, q_head_start_idx, inst);
            }

            // Store LSE to global memory
            if (laneid < GQA_RATIO && !skip_attn_reduction) {
                l_sv &L_smem = get_L_smem(s);
                kittens::wait(L_arrived(s), 0);

                // Can't do anything fancy with writing 4 spread-out values.
                // We can do this in the consumer if we want to (without using
                // smem)
                float tmp;
                uint32_t src_ptr =
                    static_cast<uint32_t>(__cvta_generic_to_shared(
                        &L_smem.data[q_head_vec_start_idx + laneid]));
                float *dst_ptr =
                    (float *)&g.attn_lse_intermediates
                        .raw_ptr[(q_head_start_idx + laneid) *
                                     g.attn_lse_intermediates.cols() +
                                 inst.partial_idx];
                asm volatile("ld.shared.f32 %0, [%1];\n"
                             : "=f"(tmp)
                             : "r"(src_ptr));
                asm volatile("st.global.f32 [%0], %1;\n"
                             :
                             : "l"(dst_ptr), "f"(tmp));
            }
            kittens::warp::sync(); // ensure all writes are committed
            // asm volatile("fence.acq_rel.gpu;");

            kittens::tma::store_async_wait();
            if (laneid == 0) {
                s.record(123 + laneid);
                finish_QOL_page(s);
            }

            // Wait and finish
            if (laneid < GQA_RATIO) {

                if (laneid == 0) {
                    s.record(megakernel::TEVENT_AT_GMEM_STORE);
                }

                if (skip_attn_reduction) {
                    atomicAdd(&g.Bar[{inst.layer_idx,
                                      OPCODE_AttentionReduction - 1, 0}],
                              1);
                } else {
                    // Adding only at 0, 4, 8, ... should be sufficient for the
                    // reduction op!
                    atomicAdd(&g.Bar[{inst.layer_idx, opcode - 1,
                                      q_head_start_idx + laneid}],
                              1);
                }

                if (laneid == 0) {
                    s.record(megakernel::TEVENT_DONE_GMEM_STORE);
                }
            }
        }
    };
};


================================================
FILE: demos/low-latency-llama/attention_reduction.cu
================================================
#include "llama.cuh"
#include <limits>

using namespace kittens;
using namespace megakernel;

using globals = llama_1b_globals;

constexpr int Q_HEADS_PER_INSTRUCTION = 4;
constexpr int MAX_ATTN_PARTIALS = globals::sm_count;
constexpr int ROUNDED_MAX_ATTN_PARTIALS = ((MAX_ATTN_PARTIALS + 15) / 16) * 16;

using l_partial_sv = kittens::sv_fl<ROUNDED_MAX_ATTN_PARTIALS>;
using o_sv = kittens::sv_fl<globals::head_dim>;
using o_rv = kittens::rv_fl<globals::head_dim>;
using o_final_sv = kittens::sv_bf<globals::head_dim>;

template <typename Config, typename Globals> struct attention_reduction {
    static constexpr int opcode = OPCODE_AttentionReduction;
    static constexpr int prev_opcode = OPCODE_PartialAttention;
    static constexpr int NUM_STAGES = 2;
    static_assert(NUM_STAGES <= 2,
                  "Reduction NUM_STAGES must be less than or equal to 2");

    struct parsed_instruction {
        int layer_idx;
        int q_head_start_idx;
        int num_partials;
        int is_terminal;
        int reduction_list_length;
        int reduction_list[ROUNDED_MAX_ATTN_PARTIALS];

        __device__ inline parsed_instruction(megakernel::state<Config> &s) {
            layer_idx = s.instruction()[1];
            q_head_start_idx = s.instruction()[2];
            num_partials = s.instruction()[3];
            is_terminal = s.instruction()[4]; // not used
            reduction_list_length = s.instruction()[5];

#pragma unroll
            for (int k = 0; k < MAX_ATTN_PARTIALS; ++k) {
                if (k < reduction_list_length) {
                    reduction_list[k] = s.instruction()[6 + k];
                }
            }
        }
    };

    // --- kittens::semaphore Access Helpers ---
    __device__ static constexpr int
    O_partial_sem_idx(int q_head_local_idx, int stage, bool is_finished) {
        return q_head_local_idx * (NUM_STAGES * 2) + stage * 2 +
               (is_finished ? 1 : 0);
    }
    __device__ static constexpr int L_partial_sem_idx(int q_head_local_idx,
                                                      bool is_finished) {
        return (Q_HEADS_PER_INSTRUCTION * NUM_STAGES * 2) +
               q_head_local_idx * 2 + (is_finished ? 1 : 0);
    }
    __device__ static constexpr int
    Final_O_ready_sem_idx(int q_head_local_idx) {
        return (Q_HEADS_PER_INSTRUCTION * NUM_STAGES * 2) +
               (Q_HEADS_PER_INSTRUCTION * 2) + q_head_local_idx;
    }

    __device__ static inline kittens::semaphore &
    O_partial_arrived(megakernel::state<config> &s, int q_head_local_idx, int stage) {
        return s
            .semaphores()[O_partial_sem_idx(q_head_local_idx, stage, false)];
    }
    __device__ static inline kittens::semaphore &
    O_partial_finished(megakernel::state<config> &s, int q_head_local_idx, int stage) {
        return s.semaphores()[O_partial_sem_idx(q_head_local_idx, stage, true)];
    }
    __device__ static inline kittens::semaphore &
    L_partial_all_arrived(megakernel::state<config> &s, int q_head_local_idx) {
        return s.semaphores()[L_partial_sem_idx(q_head_local_idx, false)];
    }
    __device__ static inline kittens::semaphore &
    L_partial_all_finished(megakernel::state<config> &s, int q_head_local_idx) {
        return s.semaphores()[L_partial_sem_idx(q_head_local_idx, true)];
    }
    __device__ static inline kittens::semaphore &final_O_ready(megakernel::state<config> &s,
                                                      int q_head_local_idx) {
        return s.semaphores()[Final_O_ready_sem_idx(q_head_local_idx)];
    }

    // --- Shared Memory Page Management Helpers ---
    static constexpr int SHARED_DATA_PAGE =
        0; // Use only the first logical page

    __device__ static inline void wait_shared_page(megakernel::state<Config> &s) {
        if (kittens::warp::laneid() == 0) {
            s.wait_page_ready(s.pid(SHARED_DATA_PAGE));
        }
    }
    __device__ static inline void finish_shared_page(megakernel::state<Config> &s) {
        if (kittens::warp::laneid() == 0) {
            s.finish_page(s.pid(SHARED_DATA_PAGE), Config::NUM_CONSUMER_WARPS);
        }
    }

    // --- Shared Memory Layout and Access Helpers (Single Page) ---
    // Calculate the size needed for partials buffering
    static constexpr size_t size_per_head =
        sizeof(l_partial_sv) + NUM_STAGES * sizeof(o_sv) + sizeof(o_final_sv);
    static constexpr size_t total_smem_needed =
        Q_HEADS_PER_INSTRUCTION * size_per_head;
    static_assert(total_smem_needed <= config::PAGE_SIZE,
                  "Required shared memory exceeds configured page size.");

    __device__ static inline l_partial_sv &
    get_L_partial_smem(megakernel::state<config> &s, int q_head_local_idx) {
        int pid = s.pid(SHARED_DATA_PAGE);
        char *page_base_ptr = reinterpret_cast<char *>(s.pages[pid].data);
        char *head_base_ptr = page_base_ptr + q_head_local_idx * size_per_head;
        return *reinterpret_cast<l_partial_sv *>(head_base_ptr);
    }
    __device__ static inline o_sv &
    get_O_partial_smem(megakernel::state<config> &s, int q_head_local_idx, int stage) {
        int pid = s.pid(SHARED_DATA_PAGE);
        char *page_base_ptr = reinterpret_cast<char *>(s.pages[pid].data);
        char *head_base_ptr = page_base_ptr + q_head_local_idx * size_per_head;
        size_t offset = sizeof(l_partial_sv) + stage * sizeof(o_sv);
        return *reinterpret_cast<o_sv *>(head_base_ptr + offset);
    }
    __device__ static inline o_final_sv &
    get_O_final_smem(megakernel::state<config> &s, int q_head_local_idx) {
        int pid = s.pid(SHARED_DATA_PAGE);
        char *page_base_ptr = reinterpret_cast<char *>(s.pages[pid].data);
        char *head_base_ptr = page_base_ptr + q_head_local_idx * size_per_head;
        size_t offset = sizeof(l_partial_sv) + NUM_STAGES * sizeof(o_sv);
        return *reinterpret_cast<o_final_sv *>(head_base_ptr + offset);
    }

    struct controller {
        static __device__ int
        release_lid(const Globals &g,
                    typename Config::instruction_t &instruction, int &query) {
            int ret_order[13] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0};
            return ret_order[query];
        }
        static __device__ int init_semaphores(const Globals &g,
                                              megakernel::state<Config> &s) {
            for (int q_head = 0; q_head < Q_HEADS_PER_INSTRUCTION; ++q_head) {
                for (int stage = 0; stage < NUM_STAGES; stage++) {
                    init_semaphore(O_partial_arrived(s, q_head, stage), 0, 1);
                    init_semaphore(O_partial_finished(s, q_head, stage), 0, 1);
                }
                init_semaphore(L_partial_all_arrived(s, q_head), 0, 1);
                init_semaphore(L_partial_all_finished(s, q_head), 0, 1);

                init_semaphore(final_O_ready(s, q_head), 0, 1);
            }
            return 4 * ((NUM_STAGES * 2) + 3);
        }
    };

    struct loader {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            auto laneid = kittens::warp::laneid();

            if (laneid == 0) {
                wait_shared_page(s);
            } else if (laneid < Config::NUM_PAGES) {
                s.wait_page_ready(s.pid(laneid));
                s.finish_page(s.pid(laneid), Config::NUM_CONSUMER_WARPS);
            }
            kittens::warp::sync(); // Have to make sure lane 0 finished waiting
        }
    };

    struct launcher {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            if (kittens::warp::laneid() == 0) {
#ifdef KITTENS_BLACKWELL
                s.wait_tensor_ready();
                arrive(s.tensor_finished, Config::NUM_CONSUMER_WARPS);
#endif

                parsed_instruction inst{s};

                s.record(megakernel::TEVENT_AT_GMEM_WAIT);
                while (*(volatile int *)&g.Bar[{inst.layer_idx, prev_opcode - 1,
                                                inst.q_head_start_idx + 0}] <
                           inst.num_partials ||
                       *(volatile int *)&g.Bar[{inst.layer_idx, prev_opcode - 1,
                                                inst.q_head_start_idx + 1}] <
                           inst.num_partials ||
                       *(volatile int *)&g.Bar[{inst.layer_idx, prev_opcode - 1,
                                                inst.q_head_start_idx + 2}] <
                           inst.num_partials ||
                       *(volatile int *)&g.Bar[{inst.layer_idx, prev_opcode - 1,
                                                inst.q_head_start_idx + 3}] <
                           inst.num_partials) {
                    __nanosleep(Config::GMEM_SPIN_LOOP_SLEEP_NANOS);
                }
                s.record(megakernel::TEVENT_DONE_GMEM_WAIT);

                for (int i = 0; i < 4; ++i) {
                    l_partial_sv &L_smem = get_L_partial_smem(s, i);
                    kittens::tma::expect(L_partial_all_arrived(s, i), L_smem);
                    kittens::tma::load_async<cache_policy::EVICT_FIRST>(
                        L_smem, g.attn_lse_intermediates,
                        {0, 0, inst.q_head_start_idx + i, 0},
                        L_partial_all_arrived(s, i));
                }

                for (int i = 0; i < inst.num_partials; ++i) {
                    int stage = i % NUM_STAGES;
                    int cur_partial_idx = inst.reduction_list[i];
                    for (int j = 0; j < 4; ++j) {
                        o_sv &O_smem = get_O_partial_smem(s, j, stage);

                        if (i >= NUM_STAGES) {
                            int prev_phase = (i / NUM_STAGES - 1) % 2;
                            kittens::wait(O_partial_finished(s, j, stage), prev_phase);
                        }

                        kittens::tma::expect(O_partial_arrived(s, j, stage), O_smem);
                        kittens::tma::load_async<cache_policy::EVICT_FIRST>(
                            O_smem, g.attn_out_intermediates,
                            {0, inst.q_head_start_idx + j, cur_partial_idx, 0},
                            O_partial_arrived(s, j, stage));
                    }
                }
            }
        }
    };

    struct consumer {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {

            if (kittens::warpid() < Q_HEADS_PER_INSTRUCTION) {

                parsed_instruction inst{s};
                int q_head_local_idx = kittens::warpid();

                o_rv accumulated_out;
                float accumulated_lse = -INFINITY;

                o_rv current_out;
                float current_lse;

                kittens::warp::zero(accumulated_out);

                kittens::warp::wait(L_partial_all_arrived(s, q_head_local_idx), 0);
                if (kittens::laneid() == 0)
                    s.record(megakernel::TEVENT_CONSUMER_START + 16 + kittens::warpid());
                l_partial_sv &L_smem = get_L_partial_smem(s, q_head_local_idx);

                // --- Reduction Pipeline ---
                for (int i = 0; i < inst.num_partials; ++i) {
                    int stage = i % NUM_STAGES;
                    kittens::warp::wait(O_partial_arrived(s, q_head_local_idx, stage),
                               (i / NUM_STAGES) % 2);

                    o_sv &O_smem =
                        get_O_partial_smem(s, q_head_local_idx, stage);

                    // Load cur L_partial value
                    int cur_partial_idx = inst.reduction_list[i];
                    uint32_t src_ptr_L =
                        static_cast<uint32_t>(__cvta_generic_to_shared(
                            &L_smem.data[cur_partial_idx]));
                    move<float>::lds(current_lse, src_ptr_L);
                    // Load O_partial_reg
                    kittens::warp::load(current_out, O_smem);

                    float max_lse = max(accumulated_lse, current_lse);

                    float accumulated_exp = exp2f(accumulated_lse - max_lse);
                    float current_exp = exp2f(current_lse - max_lse);

                    float new_denom = accumulated_exp + current_exp;

                    float accumulated_scale = accumulated_exp / new_denom;
                    float current_scale = current_exp / new_denom;

                    kittens::warp::mul(accumulated_out, accumulated_out,
                              accumulated_scale);
                    kittens::warp::mul(current_out, current_out, current_scale);
                    kittens::warp::add(accumulated_out, accumulated_out, current_out);

                    // Update LSE accumulator:
                    accumulated_lse = max_lse + log2f(new_denom);

                    kittens::warp::arrive(
                        O_partial_finished(s, q_head_local_idx, stage));
                }
                kittens::warp::arrive(L_partial_all_finished(s, q_head_local_idx));

                o_final_sv &O_final_smem =
                    get_O_final_smem(s, q_head_local_idx);
                kittens::warp::store(O_final_smem, accumulated_out);
                kittens::warp::sync();

                kittens::warp::arrive(final_O_ready(s, q_head_local_idx));
            }
        }
    };

    // Storer kittens::warp: Responsible for storing data from shared memory back to
    // global memory.
    struct storer {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            parsed_instruction inst{s};
            if (kittens::warp::laneid() < Q_HEADS_PER_INSTRUCTION) {
                int q_head_local_idx = kittens::warp::laneid();

                o_final_sv &O_final_smem =
                    get_O_final_smem(s, q_head_local_idx);
                kittens::wait(final_O_ready(s, q_head_local_idx), 0);
                if (kittens::warp::laneid() == 0) {
                    s.record(megakernel::TEVENT_OUTPUT_READY);
                }

                kittens::tma::store_async<cache_policy::NORMAL>(
                    g.attn_out, O_final_smem,
                    {0, 0, 0, inst.q_head_start_idx + q_head_local_idx});
                kittens::tma::store_async_wait();

                // atomicAdd(&g.Bar[{inst.layer_idx, opcode - 1,
                // inst.q_head_start_idx + q_head_local_idx}], 1);
            }
            finish_shared_page(s);

            kittens::warp::sync();
            if (kittens::warp::laneid() == 0) {
                s.record(megakernel::TEVENT_AT_GMEM_STORE);
                // asm volatile("fence.acq_rel.gpu;");

                // simple signalling strat for now
                atomicAdd(&g.Bar[{inst.layer_idx, opcode - 1, 0}],
                          Q_HEADS_PER_INSTRUCTION);
                s.record(megakernel::TEVENT_DONE_GMEM_STORE);
            }
        }
    };
};


================================================
FILE: demos/low-latency-llama/llama.cu
================================================
#include "llama.cuh"

#include "rms_matvec_rope_append.cu"
#include "attention_partial.cu"
#include "attention_reduction.cu"
#include "matvec_adds.cu"
#include "upgate.cu"
#include "rms_lm_head.cu"

#include "pyutils/pyutils.cuh"

using namespace kittens;
using namespace megakernel;

using rms_qkv_rope_append_op =
    rms_qkv_rope_append<default_config, llama_1b_globals>;
using attention_partial_op =
    attention_partial<default_config, llama_1b_globals>;
using attention_reduction_op =
    attention_reduction<default_config, llama_1b_globals>;
using o_proj_op = o_proj<default_config, llama_1b_globals>;
using rms_upgate_silu_op = rms_upgate_silu<default_config, llama_1b_globals>;
using downproj_op = downproj<default_config, llama_1b_globals>;
using rms_lm_head_op = rms_lm_head<default_config, llama_1b_globals>;

PYBIND11_MODULE(mk_llama, m) {
    m.doc() = "";
    kittens::py::bind_kernel<
        mk<default_config, llama_1b_globals, attention_partial_op,
            attention_reduction_op, rms_qkv_rope_append_op, downproj_op,
            o_proj_op, rms_upgate_silu_op, rms_lm_head_op>>(
        m, "mk_llama", &llama_1b_globals::Bar, &llama_1b_globals::instructions,
        &llama_1b_globals::timings,

        &llama_1b_globals::qkv_weights, &llama_1b_globals::attn_norm_weights,
        &llama_1b_globals::o_weights, &llama_1b_globals::mlp_norm_weights,
        &llama_1b_globals::up_weights, &llama_1b_globals::gate_weights,
        &llama_1b_globals::down_weights,
        &llama_1b_globals::lm_head_norm_weights,
        &llama_1b_globals::lm_head_weights, &llama_1b_globals::k_cache,
        &llama_1b_globals::v_cache,

        &llama_1b_globals::rope_cos, &llama_1b_globals::rope_sin,

        &llama_1b_globals::hidden_states, &llama_1b_globals::q_post_rope,
        &llama_1b_globals::attn_out, &llama_1b_globals::attn_lse_intermediates,
        &llama_1b_globals::attn_out_intermediates, &llama_1b_globals::silu_out,
        &llama_1b_globals::logits,

        &llama_1b_globals::pos_id, &llama_1b_globals::attn_scale,
        &llama_1b_globals::rms_norm_eps,
        &llama_1b_globals::skip_attn_reduction);
}


================================================
FILE: demos/low-latency-llama/llama.cuh
================================================
#pragma once

#include "kittens.cuh"
#include "megakernel.cuh"
#include <iostream>

#define OPCODE_RMS_QKV_MatVecRopeAppend 1
#define OPCODE_PartialAttention 2
#define OPCODE_AttentionReduction 3
#define OPCODE_O_ProjResidual 4
#define OPCODE_RMS_DoubleMatVecSiLU 5
#define OPCODE_DownProjResidual 6
#define OPCODE_RMS_LM_Head 7

#define LLAMA_1B_NUM_LAYERS 16
#define LLAMA_1B_HIDDEN_DIM 2048
#define LLAMA_1B_INTERMEDIATE_DIM 8192
#define LLAMA_1B_HEAD_DIM 64
#define LLAMA_1B_NUM_ATTENTION_HEADS 32
#define LLAMA_1B_NUM_KV_HEADS 8
#define LLAMA_1B_KV_BLOCK_SIZE 16
#define LLAMA_1B_MATVEC_BLOCK_SIZE 16
#define LLAMA_1B_LM_HEAD_BLOCK_SIZE 32
#define LLAMA_1B_VOCAB_SIZE 128256
#define H100_SM_COUNT 132
#define B200_SM_COUNT 148

constexpr int ATOMIC_ADD_START = megakernel::FREE_SLOTS_START;
constexpr int ATOMIC_ADD_END = ATOMIC_ADD_START + 1;
constexpr int EPILOGUE_START = ATOMIC_ADD_END + 1;
constexpr int ACT_WAIT_DONE = EPILOGUE_START + 2;
constexpr int WEIGHT_WAIT_START = ACT_WAIT_DONE + 3;
constexpr int WEIGHT_WAIT_DONE = WEIGHT_WAIT_START + 4;
constexpr int RMS_START = WEIGHT_WAIT_DONE + 4;
constexpr int RMS_SCALE_WAIT_START = RMS_START + 1;
constexpr int RMS_SCALE_WAIT_DONE = RMS_SCALE_WAIT_START + 1;
constexpr int RMS_DONE = RMS_SCALE_WAIT_DONE + 1;

constexpr int TEMP1 = RMS_DONE + 1;
constexpr int TEMP2 = TEMP1 + 1;
constexpr int TEMP3 = TEMP2 + 1;
constexpr int TEMP4 = TEMP3 + 1;
constexpr int TEMP5 = TEMP4 + 1;
constexpr int TEMP6 = TEMP5 + 1;

using config = megakernel::default_config;

template <int _num_layers, int _hidden_dim, int _intermediate_dim,
          int _head_dim, int _num_attention_heads, int _num_kv_heads,
          int _kv_block_size, int _matvec_block_size, int _sm_count>
struct globals_t {

    // constexpr static unsigned int num_layers = _num_layers;
    // constexpr static unsigned int matvec_block_size = _matvec_block_size;
    // constexpr static unsigned int kv_block_size = _kv_block_size;
    // constexpr static unsigned int head_dim = _head_dim;
    // constexpr static unsigned int hidden_dim = _hidden_dim;
    // constexpr static unsigned int intermediate_dim = _intermediate_dim;
    // constexpr static unsigned int num_attention_heads = _num_attention_heads;
    // constexpr static unsigned int num_kv_heads = _num_kv_heads;
    // constexpr static unsigned int sm_count = _sm_count;

    constexpr static int num_layers = _num_layers;
    constexpr static int matvec_block_size = _matvec_block_size;
    constexpr static int kv_block_size = _kv_block_size;
    constexpr static int head_dim = _head_dim;
    constexpr static int hidden_dim = _hidden_dim;
    constexpr static int intermediate_dim = _intermediate_dim;
    constexpr static int num_attention_heads = _num_attention_heads;
    constexpr static int num_kv_heads = _num_kv_heads;
    constexpr static int sm_count = _sm_count;

    using instruction_layout =
        megakernel::instruction_layout<config>;
    using timing_layout = megakernel::timing_layout<config>;

    using weights_t =
        kittens::gl<kittens::bf16, 1, -1, -1, hidden_dim,
           kittens::st_bf<matvec_block_size, 512>>; // assumed to be N by 2048 (X@W.T).
    using weights_big_indim_t =
        kittens::gl<kittens::bf16, 1, -1, -1, intermediate_dim,
           kittens::st_bf<matvec_block_size, 512>>; // assumed to be N by 2048 (X@W.T).

    using activations_t = kittens::gl<kittens::bf16, 1, 1, 1, hidden_dim, kittens::sv_bf<hidden_dim>,
                             kittens::sv_bf<head_dim>, kittens::sv_bf<matvec_block_size>>;
    using activations_big_indim_t =
        kittens::gl<kittens::bf16, 1, 1, 1, intermediate_dim, kittens::sv_bf<intermediate_dim>,
           kittens::sv_bf<hidden_dim>, kittens::sv_bf<matvec_block_size>>;
    using logits_t = kittens::gl<kittens::bf16, 1, 1, 1, -1, kittens::sv_bf<matvec_block_size>>;

    using norm_weights_t = kittens::gl<kittens::bf16, 1, 1, -1, hidden_dim, kittens::sv_bf<hidden_dim>,
                              kittens::sv_bf<matvec_block_size>>;
    using rope_table_t = kittens::gl<float, 1, 1, -1, head_dim, kittens::sv_fl<head_dim>>;
    using kv_cache_t = kittens::gl<kittens::bf16, -1, -1, -1, head_dim, kittens::sv_bf<matvec_block_size>,
                          kittens::tma::descriptor<kittens::st_bf<kv_block_size, head_dim>, 1>>;

    // max attention partials == sm_count
    using attn_out_intermediates_t =
        kittens::gl<float, 1, num_attention_heads, -1, head_dim, kittens::sv_fl<head_dim>>;
    using attn_lse_intermediates_t = kittens::gl<float, 1, 1, num_attention_heads, -1,
                                        kittens::sv_fl<((sm_count + 15) / 16) * 16>>;

    // num_layers by 6 ops per layer by up to 48 heads (Q + K + V)
    using barriers =
        kittens::gl<uint, 1, -1, -1, num_attention_heads + 2 * num_kv_heads>;

    // vm stuff
    barriers Bar;
    instruction_layout instructions;
    timing_layout timings;

    // model weights
    weights_t qkv_weights;
    norm_weights_t attn_norm_weights;
    weights_t o_weights;
    norm_weights_t mlp_norm_weights;
    weights_t up_weights;
    weights_t gate_weights;
    weights_big_indim_t down_weights;
    norm_weights_t lm_head_norm_weights;
    weights_t lm_head_weights;
    // kv cache
    kv_cache_t k_cache;
    kv_cache_t v_cache;

    // other buffers
    rope_table_t rope_cos;
    rope_table_t rope_sin;

    // activation buffers
    activations_t hidden_states;
    activations_t q_post_rope;
    activations_t attn_out;
    attn_lse_intermediates_t attn_lse_intermediates;
    attn_out_intermediates_t attn_out_intermediates;
    activations_big_indim_t silu_out;
    logits_t logits;

    unsigned int pos_id;
    float attn_scale;
    float rms_norm_eps;
    bool skip_attn_reduction;

    dim3 grid() { return dim3(sm_count); }
    dim3 block() { return dim3(config::NUM_THREADS); }
    int dynamic_shared_memory() { return config::DYNAMIC_SHARED_MEMORY; }
};

typedef globals_t<LLAMA_1B_NUM_LAYERS, LLAMA_1B_HIDDEN_DIM,
                  LLAMA_1B_INTERMEDIATE_DIM, LLAMA_1B_HEAD_DIM,
                  LLAMA_1B_NUM_ATTENTION_HEADS, LLAMA_1B_NUM_KV_HEADS,
                  LLAMA_1B_KV_BLOCK_SIZE, LLAMA_1B_MATVEC_BLOCK_SIZE,
#ifndef KITTENS_BLACKWELL
                  H100_SM_COUNT>
#else
                  B200_SM_COUNT>
#endif
    llama_1b_globals;

template <typename config = config, typename globals = llama_1b_globals>
struct attention_partial;

template <typename config = config, typename globals = llama_1b_globals>
struct attention_reduction;

template <typename config = config, typename globals = llama_1b_globals>
struct rms_qkv_rope_append;

template <typename config = config, typename globals = llama_1b_globals>
struct downproj;

template <typename config = config, typename globals = llama_1b_globals>
struct o_proj;

template <typename config = config, typename globals = llama_1b_globals>
struct rms_upgate_silu;



================================================
FILE: demos/low-latency-llama/Makefile
================================================
# Compiler
NVCC?=nvcc

ifndef GPU
GPU=B200
endif

TARGET=mk_llama
SRC=llama.cu

# Python version configuration
ifndef PYTHON_VERSION
PYTHON_VERSION=3.13
endif

NVCCFLAGS=-DNDEBUG -Xcompiler=-fPIE --expt-extended-lambda --expt-relaxed-constexpr -Xcompiler=-Wno-psabi -Xcompiler=-fno-strict-aliasing --use_fast_math -forward-unknown-to-host-compiler -O3 -Xnvlink=--verbose -Xptxas=--verbose -Xptxas=--warn-on-spills -std=c++20 -x cu -lrt -lpthread -ldl -lcuda -lcudadevrt -lcudart_static -lcublas -lineinfo
NVCCFLAGS+= -I${THUNDERKITTENS_ROOT}/include -I${MEGAKERNELS_ROOT}/include $(shell python3 -m pybind11 --includes) $(shell python3-config --ldflags) -shared -fPIC -lpython${PYTHON_VERSION}

# Conditional setup based on the target GPU
ifeq ($(GPU),4090)
NVCCFLAGS+= -DKITTENS_4090 -arch=sm_89
else ifeq ($(GPU),A100)
NVCCFLAGS+= -DKITTENS_A100 -arch=sm_80
else ifeq ($(GPU),H100)
NVCCFLAGS+= -DKITTENS_HOPPER -arch=sm_90a
else
NVCCFLAGS+= -DKITTENS_HOPPER -DKITTENS_BLACKWELL -arch=sm_100a
endif

# Default target
all: $(TARGET)

# run: $(TARGET)
# 	python test.py

$(TARGET): $(SRC)
	$(NVCC) $(SRC) $(NVCCFLAGS) -o $(TARGET)$(shell python3-config --extension-suffix)

# Clean target
clean:
	rm -f $(TARGET)$(shell python3-config --extension-suffix)



================================================
FILE: demos/low-latency-llama/matvec_adds.cu
================================================
#pragma once

#include "llama.cuh"
#include "utils.cuh"
#include "matvec_pipeline.cuh"

using namespace kittens;
using namespace megakernel;

template <int _EXPECTED_ARRIVAL_COUNT, auto WeightsPtr,
          auto InputActivationsPtr, auto OutputActivationsPtr, int _opcode,
          int _prev_opcode = 0,
          typename Config = default_config,
          typename Globals = llama_1b_globals>

struct MatVecAddOp {
    static constexpr int opcode = _opcode;
    static constexpr int prev_opcode = _prev_opcode;
    static constexpr int EXPECTED_ARRIVAL_COUNT = _EXPECTED_ARRIVAL_COUNT;

    struct parsed_instruction {
        int layer, start_block_idx, end_block_idx, reduction_block_idx,
            start_reduction_col, iters;
        __device__ inline parsed_instruction(
            typename Config::instruction_t &instruction) {
            layer = instruction[1]; // in units of 1
            start_block_idx =
                instruction[2]; // in units of 1 (0, 16, 32, ..., 2032)
            end_block_idx =
                instruction[3]; // in units of 1 (0, 16, 32, ..., 2032)
            reduction_block_idx = instruction[4]; // in units of hidden_dim=2048
                                                  // (0, 2048, 4096, 6144)
            start_reduction_col = reduction_block_idx * Globals::hidden_dim;
            iters = end_block_idx - start_block_idx;
        }
        __device__ inline parsed_instruction(megakernel::state<Config> &s)
            : parsed_instruction(s.instruction()) {}
    };

    struct pipeline_specifics {

        static __device__ inline void
        load_iter(megakernel::state<Config> &s, const globals &g, parsed_instruction &inst,
                  int iter, int col_idx, kittens::st_bf<16, 512> &weight_chunk,
                  kittens::semaphore &sem) {
            kittens::tma::load_async<dim::ROW, cache_policy::EVICT_FIRST>(
                weight_chunk, g.*WeightsPtr,
                coord<>{inst.layer,
                        (inst.start_block_idx + iter) *
                            Globals::matvec_block_size,
                        inst.start_reduction_col + 512 * col_idx},
                sem);
        }

        static __device__ inline void store(megakernel::state<Config> &s, const globals &g,
                                            parsed_instruction &inst,
                                            int output_idx, int output_stage) {

            int block_idx = inst.start_block_idx + output_idx;

            uint8_t *output_scratch_start =
                pipeline::get_output_start(s, output_stage);
            kittens::sv_bf<16> &output_smem_bf =
                *reinterpret_cast<kittens::sv_bf<16> *>(output_scratch_start);

            kittens::rv_fl<16> output_rv;
            matvec_reduce<Config, kittens::sv_fl<16>, kittens::rv_fl<16>,
                          pipeline::SCRATCH_BYTES_PER_WARP>(
                output_scratch_start, output_rv);

            kittens::warp::sync();
            kittens::warp::store(output_smem_bf, output_rv);
            kittens::warp::sync();

            if (kittens::warp::laneid() == 0) {
                auto &OutputActivations =
                    g.*OutputActivationsPtr; // object in global memory
                kittens::tma::store_add_async<cache_policy::EVICT_LAST>(
                    OutputActivations, output_smem_bf, {block_idx});
                kittens::tma::store_async_read_wait();
            }

            kittens::warp::sync();
        }
    };
    using pipeline = matvec_pipeline<Config, Globals, parsed_instruction,
                                     pipeline_specifics>;

    struct controller {
        static __device__ int
        release_lid(const Globals &g,
                    typename Config::instruction_t &instruction, int &query) {
            return pipeline::release_lid(g, instruction, query);
        }

        static __device__ int init_semaphores(const Globals &g,
                                              megakernel::state<Config> &s) {
            return pipeline::init_semaphores(s);
        }
    };
    struct loader {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::loader_loop(s, g);
        }
    };
    struct launcher {
        static __device__ void run(const globals &g, megakernel::state<Config> &s) {
            if (kittens::laneid() == 0) {
#ifdef KITTENS_BLACKWELL
                s.wait_tensor_ready();
                arrive(s.tensor_finished, Config::NUM_CONSUMER_WARPS);
#endif
            }
        }
    };
    struct consumer {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {

            using sv_t = kittens::sv_bf<pipeline::REDUCTION_DIM_PER_WARP>;
            using rv_t = kittens::rv_fl<pipeline::REDUCTION_DIM_PER_WARP>;
            parsed_instruction inst{s};

            if (kittens::laneid() == 0 && kittens::warpid() == 0) {

                int activation_page = pipeline::get_activation_page(s);
                s.wait_page_ready(activation_page);

                s.record(megakernel::TEVENT_AT_GMEM_WAIT);
                while (*(volatile int *)&g.Bar[{inst.layer, prev_opcode - 1,
                                                inst.reduction_block_idx}] <
                       EXPECTED_ARRIVAL_COUNT) {
                    __nanosleep(Config::GMEM_SPIN_LOOP_SLEEP_NANOS);
                }
                s.record(megakernel::TEVENT_DONE_GMEM_WAIT);

                auto &activations = pipeline::get_activations(s);
                auto &InputActivations =
                    g.*InputActivationsPtr; // object in global memory
            }
            group<Config::NUM_CONSUMER_WARPS>::sync(4);

            sv_t &activations_smem = reinterpret_cast<sv_t *>(
                &pipeline::get_activations(s))[kittens::warpid()];

            kittens::warp::load(activations_smem, g.*InputActivationsPtr,
                       coord<>{inst.start_reduction_col +
                               kittens::warpid() * pipeline::REDUCTION_DIM_PER_WARP});
            kittens::warp::sync();

            rv_t activations_vec;
            kittens::warp::load(activations_vec, activations_smem);
            kittens::warp::sync();

            s.warp_finish_page(pipeline::get_activation_page(s), 1);

            pipeline::consumer_loop(s, g, activations_vec);
        }
    };
    struct storer {
        // Uses 4 full pages for outputs.
        static __device__ void run(const globals &g, megakernel::state<Config> &s) {
            pipeline::storer_loop(s, g);
            kittens::warp::sync();

            if (kittens::laneid() == 0) {
                s.record(megakernel::TEVENT_AT_GMEM_STORE);
                parsed_instruction inst{s};

                kittens::tma::store_async_wait(); // not just read wait! full wait! must
                                         // be visible in global!

                // asm volatile("fence.acq_rel.gpu;\n"); // possible we need sc
                // here but I don't think so.
                atomicAdd(&g.Bar[{inst.layer, opcode - 1, 0}], inst.iters);
                s.record(megakernel::TEVENT_DONE_GMEM_STORE);
            }
        }
    };
};

template <typename Config, typename Globals>
struct downproj : MatVecAddOp<llama_1b_globals::hidden_dim /
                                  llama_1b_globals::matvec_block_size,
                              &Globals::down_weights, &Globals::silu_out,
                              &Globals::hidden_states, OPCODE_DownProjResidual,
                              OPCODE_DownProjResidual - 1, Config, Globals> {};

template <typename Config, typename Globals>
struct o_proj : MatVecAddOp<llama_1b_globals::num_attention_heads,
                            &Globals::o_weights, &Globals::attn_out,
                            &Globals::hidden_states, OPCODE_O_ProjResidual,
                            OPCODE_O_ProjResidual - 1, Config, Globals> {};




================================================
FILE: demos/low-latency-llama/matvec_pipeline.cuh
================================================
#pragma once

#include "llama.cuh"

template <typename Config, typename Globals, typename parsed_instruction,
          typename pipeline_specifics>
struct matvec_pipeline {
    static constexpr int INPUT_PIPELINE_STAGES = 3;
    static constexpr int OUTPUT_PIPELINE_STAGES = 3;
    static constexpr int STAGE_PAGES = 4;
    static constexpr int ACTIVATION_PAGE = 0;
    static constexpr int WEIGHTS_START_PAGE = 1;

    static constexpr int REDUCTION_DIM_PER_WARP =
        Globals::hidden_dim / Config::NUM_CONSUMER_WARPS;

    static constexpr int SEM_COUNT =
        1 + (INPUT_PIPELINE_STAGES + OUTPUT_PIPELINE_STAGES) * 2;

    static constexpr int SCRATCH_BYTES_PER_WARP = 16 * sizeof(float);
    static constexpr int SCRATCH_BYTES_PER_STAGE =
        SCRATCH_BYTES_PER_WARP * Config::NUM_CONSUMER_WARPS;
    static constexpr int USED_SCRATCH_BYTES =
        OUTPUT_PIPELINE_STAGES * SCRATCH_BYTES_PER_STAGE;
    static_assert(USED_SCRATCH_BYTES <= Config::SCRATCH_BYTES,
                  "USED_SCRATCH_BYTES must be less than SCRATCH_BYTES");

    // Pages (very naive for now, no fine-grained usage)
    __device__ static inline int get_activation_page(megakernel::state<Config> &s) {
        return s.pid(ACTIVATION_PAGE);
    }

    __device__ static inline int get_weight_page(megakernel::state<Config> &s, int stage,
                                                 int offset) {
        return s.pid(WEIGHTS_START_PAGE + stage * STAGE_PAGES + offset);
    }

    __device__ static inline kittens::semaphore &activations_arrived(megakernel::state<Config> &s) {
        return s.semaphores()[0];
    }
    __device__ static inline kittens::semaphore &weights_arrived(megakernel::state<Config> &s,
                                                        int stage) {
        return s.semaphores()[1 + stage];
    }
    __device__ static inline kittens::semaphore &weights_finished(megakernel::state<Config> &s,
                                                         int stage) {
        return s.semaphores()[1 + INPUT_PIPELINE_STAGES + stage];
    }
    __device__ static inline kittens::semaphore &outputs_arrived(megakernel::state<Config> &s,
                                                        int stage) {
        return s.semaphores()[1 + 2 * INPUT_PIPELINE_STAGES + stage];
    }
    __device__ static inline kittens::semaphore &outputs_finished(megakernel::state<Config> &s,
                                                         int stage) {
        return s.semaphores()[1 + 2 * INPUT_PIPELINE_STAGES +
                              OUTPUT_PIPELINE_STAGES + stage];
    }

    __device__ static inline kittens::sv_bf<Globals::hidden_dim> &
    get_activations(megakernel::state<Config> &s) {
        return *reinterpret_cast<kittens::sv_bf<Globals::hidden_dim> *>(
            s.pages[get_activation_page(s)].ptr());
    }

    __device__ static inline uint8_t *get_output_start(megakernel::state<Config> &s,
                                                       int stage) {
        return (uint8_t *)s.scratch() + (stage * SCRATCH_BYTES_PER_STAGE);
    }

    __device__ static inline int
    release_lid(const Globals &g, typename Config::instruction_t &instruction,
                int &query) {
        // NOTE: assumes a three stage pipeline

        parsed_instruction inst{instruction};
        // unused pages, then activation, then weights

        static_assert(INPUT_PIPELINE_STAGES == 3,
                      "INPUT_PIPELINE_STAGES must be 3");

        auto iters = inst.iters;
        auto remainder = iters % INPUT_PIPELINE_STAGES;

        // special handling for 1 and 2 because only then do
        // we free pages before the activation/rms scale (page 0)
        if (iters == 1) {
            int ret_order[13] = {5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4};
            return ret_order[query];
        } else if (iters == 2) {
            int ret_order[13] = {9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8};
            return ret_order[query];
        } else if (remainder == 1) {
            int ret_order[13] = {0, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4};
            return ret_order[query];
        } else if (remainder == 2) {
            int ret_order[13] = {0, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8};
            return ret_order[query];
        } else {
            int ret_order[13] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};
            return ret_order[query];
        }
    }

    __device__ static inline int init_semaphores(megakernel::state<Config> &s) {
        init_semaphore(activations_arrived(s), 1);
        for (int i = 0; i < INPUT_PIPELINE_STAGES; i++) {
            init_semaphore(weights_arrived(s, i), 1);
            init_semaphore(weights_finished(s, i), Config::NUM_CONSUMER_WARPS);
        }
        for (int i = 0; i < OUTPUT_PIPELINE_STAGES; i++) {
            init_semaphore(outputs_arrived(s, i), Config::NUM_CONSUMER_WARPS);
            init_semaphore(outputs_finished(s, i), 1);
        }
        return SEM_COUNT;
    }

    __device__ static inline void loader_loop(megakernel::state<Config> &s,
                                              const Globals &g) {
        parsed_instruction inst{s};

        auto needed_pages =
            1 + min(inst.iters, INPUT_PIPELINE_STAGES) * STAGE_PAGES;

        if (kittens::laneid() == 0) {

            int input_stage = 0;
            for (int iter = 0; iter < inst.iters; iter++) {
                kittens::wait(weights_finished(s, input_stage),
                     (iter % (2 * INPUT_PIPELINE_STAGES)) <
                         INPUT_PIPELINE_STAGES);

                auto &sem = weights_arrived(s, input_stage);
                kittens::tma::expect_bytes(sem, sizeof(kittens::bf16) * 2048 * 16);
#pragma unroll
                for (int i = 0; i < 4; i++) {
                    int weight_page = get_weight_page(s, input_stage, i);
                    if (iter < INPUT_PIPELINE_STAGES) {
                        s.wait_page_ready(weight_page);
                    }
                    auto &weight_chunk = reinterpret_cast<kittens::st_bf<16, 512> &>(
                        s.pages[weight_page]);

                    if (iter == 0 && i == 0) {
                        s.record(megakernel::TEVENT_FIRST_LOAD);
                    } else if (iter == inst.iters - 1 && i == 3) {
                        s.record(megakernel::TEVENT_LAST_LOAD);
                    }

                    pipeline_specifics::load_iter(s, g, inst, iter, i,
                                                  weight_chunk, sem);
                }

                input_stage = (input_stage + 1) % INPUT_PIPELINE_STAGES;
            }
        } else if (kittens::laneid() >= needed_pages && kittens::laneid() < Config::NUM_PAGES) {
            auto pid = s.pid(kittens::laneid());
            s.wait_page_ready(pid);
            s.finish_page(pid, Config::NUM_CONSUMER_WARPS);
        }
    }

    template <typename rv_t>
    __device__ static inline void
    consumer_loop(megakernel::state<Config> &s, const Globals &g, rv_t &activations_vec) {
        // Setup
        parsed_instruction inst{s};

        static_assert(Config::NUM_CONSUMER_WARPS % STAGE_PAGES == 0,
                      "NUM_CONSUMER_WARPS must be divisible by STAGE_PAGES");
        constexpr int WARPS_PER_PAGE = Config::NUM_CONSUMER_WARPS / STAGE_PAGES;

        int page_index = kittens::warpid() / WARPS_PER_PAGE;

        int input_stage = 0, output_stage = 0;
        for (int i = 0; i < inst.iters; i++) {
            int weight_page = get_weight_page(s, input_stage, page_index);
            kittens::wait(weights_arrived(s, input_stage),
                 (i % (2 * INPUT_PIPELINE_STAGES)) >= INPUT_PIPELINE_STAGES);
            kittens::wait(outputs_finished(s, output_stage),
                 (i % (2 * OUTPUT_PIPELINE_STAGES)) < OUTPUT_PIPELINE_STAGES);
            kittens::st_bf<16, REDUCTION_DIM_PER_WARP> &weights =
                reinterpret_cast<kittens::st_bf<16, REDUCTION_DIM_PER_WARP> *>(
                    s.pages[weight_page].ptr())[kittens::warpid() % WARPS_PER_PAGE];

            kittens::sv_fl<16> &out_smem = *reinterpret_cast<kittens::sv_fl<16> *>(
                get_output_start(s, output_stage) +
                (kittens::warpid() * SCRATCH_BYTES_PER_WARP));

            if (i == 0) {
                s.record(megakernel::TEVENT_FIRST_USE);
            } else if (i == inst.iters - 1) {
                s.record(megakernel::TEVENT_LAST_USE);
            }

            matvec(out_smem, weights, activations_vec);

            kittens::warp::sync();
            kittens::warp::arrive(outputs_arrived(s, output_stage));
            kittens::warp::arrive(weights_finished(s, input_stage));

            if (i >= inst.iters - INPUT_PIPELINE_STAGES) {
// Release pages.
#pragma unroll
                for (int j = 0; j < STAGE_PAGES; j++) {
                    s.warp_finish_page(get_weight_page(s, input_stage, j), 1);
                }
            }

            input_stage = (input_stage + 1) % INPUT_PIPELINE_STAGES;
            output_stage = (output_stage + 1) % OUTPUT_PIPELINE_STAGES;
        }
    }

    template <int iter_scale = 1>
    __device__ static inline void storer_loop(megakernel::state<Config> &s,
                                              const Globals &g) {
        parsed_instruction inst{s};

        int output_stage = 0;
        for (int i = 0; i < inst.iters; i++) {
            auto &sem = outputs_arrived(s, output_stage);
            auto bit =
                (i % (2 * OUTPUT_PIPELINE_STAGES)) >= OUTPUT_PIPELINE_STAGES;

            kittens::wait(sem, bit);

            if (i == 0) {
                s.record(megakernel::TEVENT_FIRST_STORE);
            } else if (i == inst.iters - 1) {
                s.record(megakernel::TEVENT_LAST_STORE);
            }

            pipeline_specifics::store(s, g, inst, i, output_stage);

            if ((i + 1) % iter_scale == 0) {
                for (int j = 0; j < iter_scale; j++) {
                    auto stage_to_arrive = (i - j) % OUTPUT_PIPELINE_STAGES;
                    kittens::warp::arrive(outputs_finished(s, stage_to_arrive));
                }
            }
            output_stage = (output_stage + 1) % OUTPUT_PIPELINE_STAGES;
        }
    }
};

template <typename Config, typename Globals, typename parsed_instruction,
          typename pipeline_specifics, auto ActPtr, auto RmsPtr>
struct rms_matvec_pipeline
    : public matvec_pipeline<Config, Globals, parsed_instruction,
                             pipeline_specifics> {
    using pipeline = matvec_pipeline<Config, Globals, parsed_instruction,
                                     pipeline_specifics>;

    static constexpr int REDUCTION_DIM_PER_WARP =
        Globals::hidden_dim / Config::NUM_CONSUMER_WARPS;

    static constexpr int SEM_COUNT = 1 + pipeline::SEM_COUNT;

    __device__ static inline kittens::semaphore &rms_scale_arrived(megakernel::state<Config> &s) {
        return s.semaphores()[pipeline::SEM_COUNT];
    }

    __device__ static inline kittens::sv_bf<Globals::hidden_dim> &
    get_rms_scale(megakernel::state<Config> &s) {
        return *reinterpret_cast<kittens::sv_bf<Globals::hidden_dim> *>(
            s.pages[get_activation_page(s)].ptr(
                sizeof(kittens::sv_bf<Globals::hidden_dim>)));
    }

    __device__ static inline int init_semaphores(megakernel::state<Config> &s) {
        pipeline::init_semaphores(s);
        init_semaphore(rms_scale_arrived(s), 1);
        return SEM_COUNT;
    }

    __device__ static inline void loader_loop(megakernel::state<Config> &s,
                                              const Globals &g, int layer_idx) {
        if (kittens::laneid() == 0) {
            int activation_page = get_activation_page(s);
            s.wait_page_ready(activation_page);

            auto &rms_scale = get_rms_scale(s);
            auto &sem = rms_scale_arrived(s);

            kittens::tma::expect(sem, rms_scale);
            kittens::tma::load_async<kittens::cache_policy::EVICT_LAST>(rms_scale, g.*RmsPtr,
                                                      {layer_idx, 0}, sem);
        }

        pipeline::loader_loop(s, g);
    }

    __device__ static inline void launcher_loop(megakernel::state<Config> &s,
                                                const Globals &g) {
        if (kittens::laneid() == 0) {
#ifdef KITTENS_BLACKWELL
            s.wait_tensor_ready();
            arrive(s.tensor_finished, Config::NUM_CONSUMER_WARPS);
#endif
        }
    }

    __device__ static inline void consumer_loop(megakernel::state<Config> &s,
                                                const Globals &g) {

        using sv_t = kittens::sv_bf<REDUCTION_DIM_PER_WARP>;
        auto &rms_scale_smem =
            reinterpret_cast<sv_t *>(&get_rms_scale(s))[kittens::warpid()];
        auto &activations_smem =
            reinterpret_cast<sv_t *>(&get_activations(s))[kittens::warpid()];

        if (kittens::laneid() == 0 && kittens::warpid() == 0) {
            parsed_instruction inst{s};

            int activation_page = get_activation_page(s);

            s.wait_page_ready(activation_page);
            auto &activations = get_activations(s);

            auto &sem = activations_arrived(s);

            // kittens::tma::expect(sem, activations);

            // Activation
            s.record(megakernel::TEVENT_AT_GMEM_WAIT);
            pipeline_specifics::gmem_wait(g, s);
            s.record(megakernel::TEVENT_DONE_GMEM_WAIT);

            // kittens::tma::load_async<cache_policy::EVICT_LAST>(activations, g.*ActPtr,
            // {}, sem);
        }
        kittens::group<Config::NUM_CONSUMER_WARPS>::sync(3);

        kittens::warp::load(activations_smem, g.*ActPtr, {kittens::warpid()});

        auto activation_page = get_activation_page(s);

        kittens::wait(rms_scale_arrived(s), 0);

        auto activations_vec = rms_norm<Config>(
            rms_scale_smem, activations_smem, g.rms_norm_eps,
            pipeline::get_output_start(s, pipeline::OUTPUT_PIPELINE_STAGES));

        kittens::warp::sync();
        s.warp_finish_page(activation_page, 1);

        pipeline::consumer_loop(s, g, activations_vec);
    }
};


================================================
FILE: demos/low-latency-llama/rms_lm_head.cu
================================================
#include "llama.cuh"
#include "utils.cuh"
#include "matvec_pipeline.cuh"

using namespace kittens;
using namespace megakernel;

using globals = llama_1b_globals;

template <typename Config, typename Globals> struct rms_lm_head {
    static constexpr int opcode =
        OPCODE_RMS_LM_Head; // Op index within the layer -- controls which
                            // barrier to listen to.
    static constexpr int EXPECTED_ARRIVAL_COUNT = 512;

    struct parsed_instruction {
        int start_block_idx, end_block_idx, iters;
        __device__ inline parsed_instruction(
            typename Config::instruction_t &instruction) {
            start_block_idx = instruction[1];
            end_block_idx = instruction[2];
            iters = end_block_idx - start_block_idx;
        }
        __device__ inline parsed_instruction(megakernel::state<Config> &s)
            : parsed_instruction(s.instruction()) {}
    };

    struct pipeline_specifics {
        static __device__ inline void gmem_wait(const Globals &g,
                                                megakernel::state<Config> &s) {
            parsed_instruction inst{s};
            while (*(volatile int *)&g.Bar[{globals::num_layers - 1,
                                            OPCODE_DownProjResidual - 1, 0}] <
                   EXPECTED_ARRIVAL_COUNT) {
                __nanosleep(Config::GMEM_SPIN_LOOP_SLEEP_NANOS);
            }
        }

        static __device__ inline void
        load_iter(megakernel::state<Config> &s, const globals &g, parsed_instruction &inst,
                  int iter, int col_idx, kittens::st_bf<16, 512> &weight_chunk,
                  kittens::semaphore &sem) {
            auto block_idx = inst.start_block_idx + iter;
            kittens::tma::load_async<dim::ROW, cache_policy::EVICT_FIRST>(
                weight_chunk, g.lm_head_weights, {block_idx, col_idx}, sem);
        }

        static __device__ inline void store(megakernel::state<Config> &s, const Globals &g,
                                            parsed_instruction &inst,
                                            int output_idx, int output_stage) {

            int block_idx = inst.start_block_idx + output_idx;

            uint8_t *output_scratch_start =
                pipeline::get_output_start(s, output_stage);
            kittens::sv_bf<16> &logits_smem_bf =
                *reinterpret_cast<kittens::sv_bf<16> *>(output_scratch_start);

            kittens::rv_fl<16> logits_rv;
            matvec_reduce<Config, kittens::sv_fl<16>, kittens::rv_fl<16>,
                          pipeline::SCRATCH_BYTES_PER_WARP>(
                output_scratch_start, logits_rv);

            kittens::warp::sync();
            kittens::warp::store(logits_smem_bf, logits_rv);
            kittens::warp::sync();

            if (kittens::warp::laneid() == 0) {
                s.record(megakernel::TEVENT_OUTPUT_READY);

                kittens::tma::store_async<cache_policy::EVICT_LAST>(
                    g.logits, logits_smem_bf, {0, 0, 0, block_idx});
                kittens::tma::store_async_read_wait();
            }

            kittens::warp::sync();
        }
    };

    using pipeline =
        rms_matvec_pipeline<Config, Globals, parsed_instruction,
                            pipeline_specifics, &Globals::hidden_states,
                            &Globals::lm_head_norm_weights>;

    struct controller {
        static __device__ int
        release_lid(const Globals &g,
                    typename Config::instruction_t &instruction, int &query) {
            return pipeline::release_lid(g, instruction, query);
        }

        static __device__ int init_semaphores(const Globals &g,
                                              megakernel::state<Config> &s) {
            return pipeline::init_semaphores(s);
        }
    };
    struct loader {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::loader_loop(s, g, 0);
        }
    };
    struct launcher {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::launcher_loop(s, g);
        }
    };
    struct consumer {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::consumer_loop(s, g);
        }
    };
    struct storer {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::storer_loop(s, g);
        }
    };
};



================================================
FILE: demos/low-latency-llama/rms_matvec_rope_append.cu
================================================
#include "llama.cuh"
#include "utils.cuh"
#include "matvec_pipeline.cuh"

using namespace kittens;
using namespace megakernel;

using globals = llama_1b_globals;

template <typename Config, typename Globals> struct rms_qkv_rope_append {
    static constexpr int opcode =
        OPCODE_RMS_QKV_MatVecRopeAppend; // Op index within the layer --
                                         // controls which barrier to listen to.

    static constexpr int K_BLK_START = 2048 / Globals::matvec_block_size;
    static constexpr int V_BLK_START = 2560 / Globals::matvec_block_size;
    static constexpr int EXPECTED_ARRIVAL_COUNT = 512;

    using rope_t = kittens::sv_fl<Globals::head_dim>;

    __device__ static inline uint8_t *get_rope_cos_ptr(megakernel::state<Config> &s) {
        return (uint8_t *)s.scratch() + Config::SCRATCH_BYTES - 512;
    }
    __device__ static inline uint8_t *get_rope_sin_ptr(megakernel::state<Config> &s) {
        return (uint8_t *)s.scratch() + Config::SCRATCH_BYTES - 256;
    }
    __device__ static inline rope_t &get_rope_cos(megakernel::state<Config> &s) {
        return *reinterpret_cast<rope_t *>(get_rope_cos_ptr(s));
    }
    __device__ static inline rope_t &get_rope_sin(megakernel::state<Config> &s) {
        return *reinterpret_cast<rope_t *>(get_rope_sin_ptr(s));
    }

    struct parsed_instruction {
        int layer_idx, start_block_idx, end_block_idx, iters;
        __device__ inline parsed_instruction(
            typename Config::instruction_t &instruction) {
            layer_idx = instruction[1];       // in units of 1
            start_block_idx = instruction[2]; // in units of 16 elements
            end_block_idx = instruction[3];   // in units of 16 elements
            iters = end_block_idx - start_block_idx;
        }
        __device__ inline parsed_instruction(megakernel::state<Config> &s)
            : parsed_instruction(s.instruction()) {}
    };

    struct pipeline_specifics {

        static __device__ inline void gmem_wait(const Globals &g,
                                                megakernel::state<Config> &s) {
            parsed_instruction inst{s};
            if (inst.layer_idx > 0) {
                while (
                    *(volatile int *)&g.Bar[{inst.layer_idx - 1,
                                             OPCODE_DownProjResidual - 1, 0}] <
                    EXPECTED_ARRIVAL_COUNT) {
                    __nanosleep(Config::GMEM_SPIN_LOOP_SLEEP_NANOS);
                }
            }
        }

        static __device__ inline void
        load_iter(megakernel::state<Config> &s, const globals &g, parsed_instruction &inst,
                  int iter, int col_idx, kittens::st_bf<16, 512> &weight_chunk,
                  kittens::semaphore &sem) {
            auto block_idx = inst.start_block_idx + iter;
            kittens::tma::load_async<dim::ROW, cache_policy::EVICT_FIRST>(
                weight_chunk, g.qkv_weights,
                {inst.layer_idx, block_idx, col_idx}, sem);
        }

        static __device__ inline void store(megakernel::state<Config> &s, const Globals &g,
                                            parsed_instruction &inst,
                                            int output_idx, int output_stage) {
            int block_idx = inst.start_block_idx + output_idx;

            // apply rope

            // even for V, we need to cast from float to bf16
            uint8_t *output_scratch_start =
                pipeline::get_output_start(s, output_stage);

            // kittens::sv_fl<16> &qkv_proj_smem = *reinterpret_cast<kittens::sv_fl<16>
            // *>(output_scratch_start);
            kittens::sv_bf<16> &qkv_proj_smem_bf =
                *reinterpret_cast<kittens::sv_bf<16> *>(output_scratch_start);

            kittens::rv_fl<16> qkv_proj, rope_cos, rope_sin;

            matvec_reduce<Config, kittens::sv_fl<16>, kittens::rv_fl<16>,
                          pipeline::SCRATCH_BYTES_PER_WARP>(
                output_scratch_start, qkv_proj);

            kittens::wait(rope_arrived(s), 0);

            auto head_chunk = block_idx % 4;

            kittens::sv_fl<16> &rope_cos_sv = *reinterpret_cast<kittens::sv_fl<16> *>(
                get_rope_cos_ptr(s) + head_chunk * 64);
            kittens::sv_fl<16> &rope_sin_sv = *reinterpret_cast<kittens::sv_fl<16> *>(
                get_rope_sin_ptr(s) + head_chunk * 64);

            kittens::warp::load(rope_cos, rope_cos_sv);
            kittens::warp::load(rope_sin, rope_sin_sv);

            if (block_idx < V_BLK_START) { // only Q & K need RoPE

                // Fetch the neighbor values
                int mod = (kittens::laneid() & 0b1) ? -1 : 1; // 1 for even, -1 for odd
                kittens::warp::sync();
                float pair_val =
                    __shfl_sync(MASK_ALL, qkv_proj[0][0], kittens::laneid() + mod);

                // Compute RoPE in-place
                if (kittens::laneid() < 16) {
                    // will clean this up later
                    qkv_proj[0][0] =
                        float(qkv_proj[0][0]) * rope_cos[0][0] +
                        float(-1 * mod) * float(pair_val) * rope_sin[0][0];
                }
            }

            kittens::warp::sync();
            kittens::warp::store(qkv_proj_smem_bf, qkv_proj);
            kittens::warp::sync();

            if (kittens::laneid() == 0) {

                if (block_idx < K_BLK_START) { // Q
                    kittens::tma::store_async<cache_policy::EVICT_LAST>(
                        g.q_post_rope, qkv_proj_smem_bf, {0, 0, 0, block_idx});
                } else if (block_idx < V_BLK_START) { // K
                    int base_index =
                        (block_idx - K_BLK_START) * Globals::matvec_block_size;
                    int head_idx = base_index / Globals::head_dim;
                    int dim_idx = (base_index % Globals::head_dim) /
                                  Globals::matvec_block_size;
                    kittens::tma::store_async<cache_policy::EVICT_LAST>(
                        g.k_cache, qkv_proj_smem_bf,
                        {inst.layer_idx, static_cast<int>(g.pos_id), head_idx,
                         dim_idx});
                } else { // V
                    int base_index =
                        (block_idx - V_BLK_START) * Globals::matvec_block_size;
                    int head_idx = base_index / Globals::head_dim;
                    int dim_idx = (base_index % Globals::head_dim) /
                                  Globals::matvec_block_size;
                    kittens::tma::store_async<cache_policy::EVICT_LAST>(
                        g.v_cache, qkv_proj_smem_bf,
                        {inst.layer_idx, static_cast<int>(g.pos_id), head_idx,
                         dim_idx});
                }

                s.record(megakernel::TEVENT_AT_GMEM_STORE);

                kittens::tma::store_async_wait(); // not just read wait! full wait! must
                                         // be visible in global!
                // asm volatile("fence.acq_rel.gpu;\n"); // possible we need sc
                // here but I don't think so.

                atomicAdd(&g.Bar[{inst.layer_idx, opcode - 1, block_idx / 4}],
                          1);
                s.record(megakernel::TEVENT_DONE_GMEM_STORE);
            }

            kittens::warp::sync();
        }
    };

    using pipeline =
        rms_matvec_pipeline<Config, Globals, parsed_instruction,
                            pipeline_specifics, &Globals::hidden_states,
                            &Globals::attn_norm_weights>;

    __device__ static inline kittens::semaphore &rope_arrived(megakernel::state<Config> &s) {
        return s.semaphores()[pipeline::SEM_COUNT];
    }

    struct controller {
        static __device__ int
        release_lid(const Globals &g,
                    typename Config::instruction_t &instruction, int &query) {
            return pipeline::release_lid(g, instruction, query);
        }
        static __device__ int init_semaphores(const Globals &g,
                                              megakernel::state<Config> &s) {
            pipeline::init_semaphores(s);
            init_semaphore(rope_arrived(s), 1);
            return pipeline::SEM_COUNT + 1;
        }
    };
    struct loader {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            if (kittens::laneid() == 0) {
                auto &rope_cos = get_rope_cos(s);
                auto &rope_sin = get_rope_sin(s);

                auto &sem = rope_arrived(s);
                kittens::tma::expect(sem, rope_cos, rope_sin);

                kittens::tma::load_async<cache_policy::EVICT_LAST>(
                    rope_cos, g.rope_cos, {0, 0, static_cast<int>(g.pos_id), 0},
                    sem);
                kittens::tma::load_async<cache_policy::EVICT_LAST>(
                    rope_sin, g.rope_sin, {0, 0, static_cast<int>(g.pos_id), 0},
                    sem);
            }

            parsed_instruction inst{s};
            pipeline::loader_loop(s, g, inst.layer_idx);
        }
    };
    struct launcher {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {

            parsed_instruction inst{s};
            pipeline::launcher_loop(s, g);
        }
    };
    struct consumer {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::consumer_loop(s, g);
        }
    };
    struct storer {
        // Uses 4 full pages for outputs.
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::storer_loop(s, g);
        }
    };
};



================================================
FILE: demos/low-latency-llama/upgate.cu
================================================
#include "llama.cuh"
#include "utils.cuh"

using namespace kittens;
using namespace megakernel;

using globals = llama_1b_globals;
using config = default_config;

template <typename Config, typename Globals> struct rms_upgate_silu {
    static constexpr int opcode =
        OPCODE_RMS_DoubleMatVecSiLU; // Op index within the layer -- controls
                                     // which barrier to listen to.
    static constexpr int prev_opcode = OPCODE_O_ProjResidual;
    static constexpr int EXPECTED_ARRIVAL_COUNT =
        Globals::hidden_dim / Globals::matvec_block_size;

    struct parsed_instruction {
        int layer_idx, start_block_idx, end_block_idx, iters;
        int *block_idxs;
        __device__ inline parsed_instruction(
            typename Config::instruction_t &instruction) {
            layer_idx = instruction[1];
            iters = 2 * instruction[2];
            block_idxs = instruction + 3;
        }
        __device__ inline parsed_instruction(megakernel::state<Config> &s)
            : parsed_instruction(s.instruction()) {}
    };

    struct pipeline_specifics {
        static __device__ inline void gmem_wait(const Globals &g,
                                                megakernel::state<Config> &s) {
            parsed_instruction inst{s};
            while (
                *(volatile int *)&g.Bar[{inst.layer_idx, prev_opcode - 1, 0}] <
                EXPECTED_ARRIVAL_COUNT) {
                __nanosleep(Config::GMEM_SPIN_LOOP_SLEEP_NANOS);
            }
        }

        static __device__ inline void
        load_iter(megakernel::state<Config> &s, const globals &g, parsed_instruction &inst,
                  int iter, int col_idx, kittens::st_bf<16, 512> &weight_chunk,
                  kittens::semaphore &sem) {
            auto block_idx = inst.block_idxs[iter / 2];
            if (iter % 2 == 0) {
                kittens::tma::load_async<dim::ROW, cache_policy::EVICT_FIRST>(
                    weight_chunk, g.up_weights,
                    {inst.layer_idx, block_idx, col_idx}, sem);
            } else {
                kittens::tma::load_async<dim::ROW, cache_policy::EVICT_FIRST>(
                    weight_chunk, g.gate_weights,
                    {inst.layer_idx, block_idx, col_idx}, sem);
            }
        }

        static __device__ inline void store(megakernel::state<Config> &s, const Globals &g,
                                            parsed_instruction &inst,
                                            int output_idx, int output_stage) {
            if (output_idx % 2 == 0) {
                return;
            }

            auto true_output_idx = output_idx / 2;

            // NOTE: hardcoding to 3 output stages for now
            auto prev_output_idx = (output_idx - 1);
            auto prev_output_stage = prev_output_idx % 3;

            int block_idx = inst.block_idxs[true_output_idx];

            uint8_t *output_scratch_start =
                pipeline::get_output_start(s, output_stage);
            uint8_t *prev_output_scratch_start =
                pipeline::get_output_start(s, prev_output_stage);

            kittens::sv_bf<16> &out_smem =
                *reinterpret_cast<kittens::sv_bf<16> *>(output_scratch_start);

            kittens::rv_fl<16> up_out, gate_out, gate_scratch;

            // TODO we can do better here and reduce up before gate is ready.
            matvec_reduce<Config, kittens::sv_fl<16>, kittens::rv_fl<16>,
                          pipeline::SCRATCH_BYTES_PER_WARP>(
                prev_output_scratch_start, up_out);
            matvec_reduce<Config, kittens::sv_fl<16>, kittens::rv_fl<16>,
                          pipeline::SCRATCH_BYTES_PER_WARP>(
                output_scratch_start, gate_out);

            // neg
            kittens::warp::mul(gate_scratch, gate_out, -1.f);
            kittens::warp::exp(gate_scratch, gate_scratch);
            kittens::warp::add(gate_scratch, gate_scratch, 1.f);
            kittens::warp::div(gate_out, gate_out, gate_scratch);

            // gating
            kittens::warp::mul(gate_out, up_out, gate_out);

            // wait before we overwrite gate_out
            kittens::warp::sync();

            kittens::warp::store(out_smem, gate_out);

            // wait before we store results to global memory
            kittens::warp::sync();

            if (kittens::laneid() == 0) {
                kittens::tma::store_async<cache_policy::EVICT_LAST>(g.silu_out, out_smem,
                                                           {block_idx});
                kittens::tma::store_async_wait();

                s.record(megakernel::TEVENT_AT_GMEM_STORE);
                // asm volatile("fence.acq_rel.gpu;");

                parsed_instruction inst{s};
                atomicAdd(&g.Bar[{inst.layer_idx, opcode - 1,
                                  block_idx * globals::matvec_block_size /
                                      globals::hidden_dim}],
                          1);

                s.record(megakernel::TEVENT_DONE_GMEM_STORE);
            }

            kittens::warp::sync();
        }
    };

    using pipeline =
        rms_matvec_pipeline<Config, Globals, parsed_instruction,
                            pipeline_specifics, &Globals::hidden_states,
                            &Globals::mlp_norm_weights>;
    static_assert(pipeline::OUTPUT_PIPELINE_STAGES == 3);

    struct controller {
        static __device__ int
        release_lid(const Globals &g,
                    typename Config::instruction_t &instruction, int &query) {
            return pipeline::release_lid(g, instruction, query);
        }
        static __device__ int init_semaphores(const Globals &g,
                                              megakernel::state<Config> &s) {
            return pipeline::init_semaphores(s);
        }
    };

    struct loader {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            parsed_instruction inst{s};
            pipeline::loader_loop(s, g, inst.layer_idx);
        }
    };

    struct launcher {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::launcher_loop(s, g);
        }
    };

    struct consumer {
        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::consumer_loop(s, g);
        }
    };

    struct storer {

        static __device__ void run(const Globals &g, megakernel::state<Config> &s) {
            pipeline::storer_loop<2>(s, g);
        }
    };
};



================================================
FILE: demos/low-latency-llama/utils.cuh
================================================
#pragma once

#include "llama.cuh"

template <typename Config, kittens::ducks::sv::all sv_t>
__device__ static inline auto
rms_norm(const sv_t &rms_scale_smem, const sv_t &activations_smem,
         float rms_norm_eps, void *scratch_memory) {
    using rv_t = kittens::rv_fl<sv_t::length>;
    rv_t activations_vec, sq_activations_vec, rms_scale_vec;

    kittens::warp::load(activations_vec, activations_smem);
    kittens::warp::copy(sq_activations_vec, activations_vec);
    kittens::warp::mul(sq_activations_vec, sq_activations_vec, sq_activations_vec);
    float partial_sum = kittens::warp::sum(sq_activations_vec);

    float *smem_rms_partial_sums = (float *)scratch_memory;
    if (kittens::laneid() == 0) {
        smem_rms_partial_sums[kittens::warpid()] = partial_sum;
    }
    kittens::group<Config::NUM_CONSUMER_WARPS>::sync(0);

    float full_sum = 0;
#pragma unroll
    for (int i = 0; i < Config::NUM_CONSUMER_WARPS; i++) {
        full_sum += smem_rms_partial_sums[i];
    }

    float variance = full_sum / 2048.0f;
    float rms_scale = rsqrtf(variance + rms_norm_eps);

    kittens::warp::mul(activations_vec, activations_vec, rms_scale);
    kittens::warp::load(rms_scale_vec, rms_scale_smem);
    kittens::warp::mul(activations_vec, activations_vec, rms_scale_vec);

    return activations_vec;
}

#ifdef KITTENS_BLACKWELL
template <kittens::ducks::st::all st_t>
__device__ static inline void matvec(kittens::sv_fl<st_t::rows> &out_smem,
                                     st_t &weights_smem,
                                     kittens::rv_fl<st_t::cols> &activations) {
    using rt_t = kittens::rt_bf<st_t::rows, st_t::cols>;
    using rrv_t = typename rt_t::row_vec;
    using rcv_t = typename kittens::rt_fl<16, 16>::col_vec;
    // using rcv_t = typename rt_t::col_vec;
    using rv_t = kittens::rv_fl<st_t::rows>;
    using sv_t = kittens::sv_bf<st_t::rows>;

    rrv_t row_activations;
    kittens::warp::copy(row_activations, activations);

    rt_t broadcast_activations, weights;
    kittens::warp::broadcast_col(broadcast_activations, row_activations);
    kittens::warp::load(weights, weights_smem);
    kittens::rt_fl<16, 16> out_activations;
    kittens::warp::zero(out_activations);
    kittens::warp::mma_ABt(out_activations, weights, broadcast_activations,
                  out_activations);
    rcv_t sum_col_vec;
    kittens::warp::row_max(sum_col_vec, out_activations);

    rv_t sum_vec;
    kittens::warp::copy(sum_vec, sum_col_vec);

    if (kittens::laneid() < 16) {
        out_smem[kittens::laneid()] = sum_vec[0][0];
    }
    kittens::warp::sync();
}
#else
template <kittens::ducks::st::all st_t>
__device__ static inline void matvec(kittens::sv_fl<st_t::rows> &out_smem,
                                     st_t &weights_smem,
                                     kittens::rv_fl<st_t::cols> &activations) {
    using rt_t = kittens::rt_fl<st_t::rows, st_t::cols>;
    using rrv_t = typename rt_t::row_vec;
    using rcv_t = typename rt_t::col_vec;
    using rv_t = kittens::rv_fl<st_t::rows>;
    using sv_t = kittens::sv_bf<st_t::rows>;

    rrv_t row_activations;
    kittens::warp::copy(row_activations, activations);

    rt_t broadcast_activations, weights;
    kittens::warp::broadcast_col(broadcast_activations, row_activations);
    kittens::warp::load(weights, weights_smem);
    kittens::warp::mul(broadcast_activations, broadcast_activations, weights);
    rcv_t sum_col_vec;
    kittens::warp::row_sum(sum_col_vec, broadcast_activations);

    rv_t sum_vec;
    kittens::warp::copy(sum_vec, sum_col_vec);

    if (kittens::laneid() < 16) {
        out_smem[kittens::laneid()] = sum_vec[0][0];
    }
    kittens::warp::sync();
}
#endif

template <typename Config, kittens::ducks::sv::all sv_t, typename rv_t,
          int SCRATCH_BYTES_PER_WARP>
__device__ static inline void matvec_reduce(uint8_t *scratch, rv_t &sum_vec) {
    rv_t part_vec;
    kittens::warp::zero(sum_vec);

#pragma unroll
    for (int i = 0; i < Config::NUM_CONSUMER_WARPS; i++) {

        // TODO: for now, deliberately not using sizeof(sv_t) here because we've
        // had alignment issues before.
        sv_t &part =
            *reinterpret_cast<sv_t *>(scratch + (i * SCRATCH_BYTES_PER_WARP));

        kittens::warp::load(part_vec, part);
        kittens::warp::add(sum_vec, sum_vec, part_vec);
    }
}




================================================
FILE: include/config.cuh
================================================
#pragma once

#include "kittens.cuh"

namespace megakernel {

struct default_config {
    // Instruction pipeline
    static constexpr int INSTRUCTION_PIPELINE_STAGES = 2;

    // num bits required to represent num pipeline stages
    static constexpr int INSTRUCTION_PIPELINE_STAGES_BITS = 1;

    static constexpr int INSTRUCTION_WIDTH = 32; // 128 bytes per instruction.
    using instruction_t = int[INSTRUCTION_WIDTH];

    // Timing info
    static constexpr int TIMING_WIDTH = 128;
    using timing_t = int[TIMING_WIDTH];

    // How many semaphores are available for dynamic use?
    static constexpr int DYNAMIC_SEMAPHORES = 32;

    // One controller warp, one load warp, one store warp, and one mma warp.
    static constexpr int NUM_CONSUMER_WARPS = 16;
    static constexpr int NUM_WARPS = 4 + NUM_CONSUMER_WARPS;
    static constexpr int NUM_THREADS = NUM_WARPS * ::kittens::WARP_THREADS;
    static constexpr int NUM_BLOCKS = 1;
    static constexpr int CLUSTER_BLOCKS = 1;
    static constexpr int MAX_SHARED_MEMORY = ::kittens::MAX_SHARED_MEMORY;

    // Shared memory declared statically
    static constexpr int SCRATCH_BYTES = 4096;
    static constexpr int STATIC_SHARED_MEMORY =
        512 + INSTRUCTION_PIPELINE_STAGES *
                  (SCRATCH_BYTES + (INSTRUCTION_WIDTH + TIMING_WIDTH) * 4 +
                   DYNAMIC_SEMAPHORES * 8);
    static constexpr int DYNAMIC_SHARED_MEMORY =
        ::kittens::MAX_SHARED_MEMORY - STATIC_SHARED_MEMORY;

    // Shared memory declared dynamically
    static constexpr int PAGE_SIZE = 16384;
    static constexpr int NUM_PAGES = DYNAMIC_SHARED_MEMORY / PAGE_SIZE;
    static_assert(NUM_PAGES == 13, "NUM_PAGES must be 13");

    static constexpr bool TIMING_RECORD_ENABLED = false;

    static constexpr bool GMEM_SPIN_LOOP_SLEEP_NANOS = 20;

    static constexpr int CONSUMER_REGISTERS = 104;
    static constexpr int NON_CONSUMER_REGISTERS = 64;
};
template <typename config>
using instruction_layout = kittens::gl<int, 1, -1, -1, config::INSTRUCTION_WIDTH>;
template <typename config>
using timing_layout = kittens::gl<int, 1, -1, -1, config::TIMING_WIDTH>;

template <typename config> void print_config() {
    std::cout << "---------------- CONFIG INFO ----------------" << std::endl;
    std::cout << "INSTRUCTION_PIPELINE_STAGES: "
              << config::INSTRUCTION_PIPELINE_STAGES << std::endl;
    std::cout << "INSTRUCTION_WIDTH: " << config::INSTRUCTION_WIDTH
              << std::endl;
    std::cout << "TIMING_WIDTH: " << config::TIMING_WIDTH << std::endl;
    std::cout << "NUM_CONSUMER_WARPS: " << config::NUM_CONSUMER_WARPS
              << std::endl;
    std::cout << "NUM_WARPS: " << config::NUM_WARPS << std::endl;
    std::cout << "NUM_THREADS: " << config::NUM_THREADS << std::endl;
    std::cout << "NUM_BLOCKS: " << config::NUM_BLOCKS << std::endl;
    std::cout << "CLUSTER_BLOCKS: " << config::CLUSTER_BLOCKS << std::endl;
    std::cout << "MAX_SHARED_MEMORY: " << config::MAX_SHARED_MEMORY
              << std::endl;
    std::cout << "STATIC_SHARED_MEMORY: " << config::STATIC_SHARED_MEMORY
              << std::endl;
    std::cout << "PAGE_SIZE: " << config::PAGE_SIZE << std::endl;
    std::cout << "NUM_PAGES: " << config::NUM_PAGES << std::endl;
    std::cout << "SCRATCH_BYTES: " << config::SCRATCH_BYTES << std::endl;
    std::cout << "DYNAMIC_SEMAPHORES: " << config::DYNAMIC_SEMAPHORES
              << std::endl;
    std::cout << "---------------------------------------------" << std::endl;
}

} // namespace megakernel


================================================
FILE: include/consumer.cuh
================================================
#pragma once

#include "kittens.cuh"
#include "util.cuh"

MAKE_WORKER(consumer, TEVENT_CONSUMER_START, true)


================================================
FILE: include/launcher.cuh
================================================
#pragma once

#include "kittens.cuh"

#include "util.cuh"

MAKE_WORKER(launcher, TEVENT_LAUNCHER_START, false)


================================================
FILE: include/loader.cuh
================================================
#pragma once

#include "kittens.cuh"

#include "util.cuh"

MAKE_WORKER(loader, TEVENT_LOADER_START, false)


================================================
FILE: include/megakernel.cuh
================================================
#pragma once

#include "kittens.cuh"
#include "config.cuh"
#include "util.cuh"
#include "controller/controller.cuh"
#include "launcher.cuh"
#include "storer.cuh"
#include "loader.cuh"
#include "consumer.cuh"
#include "noop.cuh"

namespace megakernel {

template <typename config, typename globals, typename... ops>
__device__ inline void mk_internal(const globals &g) {
    uint64_t start_time = (uint64_t)clock64();
#ifdef MK_DEBUG
    if (threadIdx.x == 0)
        printf("Thread %d: Kernel launched\n", threadIdx.x);
    group<config::NUM_WARPS>::sync(15);
#endif
    __shared__ alignas(128) instruction_state_t<config>
        instruction_state[config::INSTRUCTION_PIPELINE_STAGES];
    __shared__ kittens::semaphore
        page_finished[config::NUM_PAGES]
                     [config::INSTRUCTION_PIPELINE_STAGES_BITS],
        instruction_arrived[config::INSTRUCTION_PIPELINE_STAGES],
        instruction_finished[config::INSTRUCTION_PIPELINE_STAGES],
#ifdef KITTENS_BLACKWELL
        tensor_finished,
#endif
        semaphores_ready;
    extern __shared__ int __shm[];
    void *aligned_shm_addr =
        (void *)((1023 + (uint64_t)&__shm[0]) & ~(uint64_t)1023);
    typename state<config>::page_array_t &pages =
        *reinterpret_cast<typename state<config>::page_array_t *>(
            aligned_shm_addr);
#ifdef KITTENS_BLACKWELL
    typename state<config>::tensor_allocator_t tensor_alloc{};
#endif

#ifdef MK_DEBUG
    if (threadIdx.x == 0)
        printf("Thread %d: Pre-MKS creation\n", threadIdx.x);
    group<config::NUM_WARPS>::sync(15);
#endif
    state<config> mks{instruction_state,
                      instruction_arrived,
                      instruction_finished,
                      0,
                      0,
                      {/* ... */},
                      pages,
                      page_finished,
#ifdef KITTENS_BLACKWELL
                      tensor_finished,
#endif
                      semaphores_ready,
                      start_time
#ifdef KITTENS_BLACKWELL
                      ,
                      tensor_alloc
#endif
    }; // megakernel state

#ifdef MK_DEBUG
    if (threadIdx.x == 0)
        printf("Thread %d: Created MKS\n", threadIdx.x);
    group<config::NUM_WARPS>::sync(15);
#endif

    // Zero initial timings memory.
    if (threadIdx.x < config::TIMING_WIDTH) {
#pragma unroll
        for (int i = 0; i < config::INSTRUCTION_PIPELINE_STAGES; i++) {
            instruction_state[i].timings[threadIdx.x] = 0;
        }
    }

    if (threadIdx.x < config::INSTRUCTION_PIPELINE_STAGES) {
        init_semaphore(instruction_arrived[threadIdx.x], 1);
        init_semaphore(instruction_finished[threadIdx.x],
                       config::NUM_WARPS - 1);
    }
    if (threadIdx.x < config::NUM_PAGES) {
        for (int i = 0; i < config::INSTRUCTION_PIPELINE_STAGES_BITS; i++) {
            auto count = config::NUM_CONSUMER_WARPS * (1 << i);
            init_semaphore(page_finished[threadIdx.x][i], count);
            arrive(page_finished[threadIdx.x][i], count);
        }
    }
    if (threadIdx.x == 0) {
#ifdef KITTENS_BLACKWELL
        init_semaphore(tensor_finished, config::NUM_CONSUMER_WARPS);
        arrive(tensor_finished,
               config::NUM_CONSUMER_WARPS); // Flip to state 0, to mark that it
                                            // starts as available.
#endif
        init_semaphore(semaphores_ready, 1);
    }

    asm volatile("fence.proxy.async.shared::cta;\n" ::: "memory");
    __syncthreads();

    if (config::CLUSTER_BLOCKS == 1)
        kittens::everyone::sync(15); // all warps must arrive here, confirming semaphore
                            // initialization is visible to all threads.
    else
        kittens::everyone::tma::cluster::sync();

#ifdef MK_DEBUG
    if (blockIdx.x == 0 && threadIdx.x == 0)
        mks.print();
#endif

    if (kittens::warpid() < config::NUM_CONSUMER_WARPS) {
        kittens::warpgroup::increase_registers<config::CONSUMER_REGISTERS>();
        ::megakernel::consumer::main_loop<config, globals, ops...>(g, mks);
    } else {
        kittens::warpgroup::decrease_registers<config::NON_CONSUMER_REGISTERS>();
        switch (kittens::warpgroup::warpid()) {
        case 0:
            ::megakernel::loader::main_loop<config, globals, ops...>(g, mks);
            break;
        case 1:
            ::megakernel::storer::main_loop<config, globals, ops...>(g, mks);
            break;
        case 2:
            ::megakernel::launcher::main_loop<config, globals, ops...>(g, mks);
            break;
        case 3:
            ::megakernel::controller::main_loop<config, globals, ops...>(g,
                                                                         mks);
            break;
        default:
            asm volatile("trap;");
        }
    }

#ifdef MK_DEBUG
    printf("Thread %d arriving at final barrier\n", threadIdx.x);
#endif

    if (config::CLUSTER_BLOCKS > 1)
        kittens::everyone::tma::cluster::sync();
    else
        kittens::everyone::sync(15);

#ifdef MK_DEBUG
    uint64_t end_time = (uint64_t)clock64();
    if (threadIdx.x == 0)
        printf("Overall VM execution time: %lu\n", end_time - start_time);
#endif
}

// Forward a NoOp to the VM, to ensure that the VM can support zeros.
template <typename config, typename globals, typename... ops>
struct megakernel_wrapper {
    __device__ inline static void run(const globals &g) {
        mk_internal<config, globals, NoOp<config>, ops...>(g);
    }
};

template <typename config, typename globals, typename... ops>
__launch_bounds__(config::NUM_THREADS, 1)
    __cluster_dims__(config::CLUSTER_BLOCKS) __global__
    void mk(const __grid_constant__ globals g) {
    megakernel_wrapper<config, globals, ops...>::run(g);
}

} // namespace megakernel


================================================
FILE: include/noop.cuh
================================================

#pragma once

#include "util.cuh"

namespace megakernel {

template <typename config> struct NoOp {
    static constexpr int opcode = 0;

    struct controller {
        template <typename globals>
        static __device__ int
        release_lid(const globals &g,
                    typename config::instruction_t &instruction, int &query) {
            return query;
        }
        template <typename globals>
        static __device__ int init_semaphores(const globals &g,
                                              state<config> &s) {
            return 0;
        }
    };
    struct loader {
        template <typename globals>
        static __device__ void run(const globals &g, state<config> &s) {
            if (kittens::laneid() < config::NUM_PAGES) { // Release all pages, ASAP.
                auto pid = s.pid(kittens::laneid());
                s.wait_page_ready(pid);
                s.finish_page(pid, config::NUM_CONSUMER_WARPS);
            }
        }
    };
    struct launcher { // launches mma's
        // launcher does nothing here, since this doesn't use tensor cores.
        template <typename globals>
        static __device__ void run(const globals &g, state<config> &s) {
#ifdef KITTENS_BLACKWELL
            s.wait_tensor_ready();
            if (kittens::laneid() == 0)
                arrive(s.tensor_finished, config::NUM_CONSUMER_WARPS);
#endif
        }
    };
    struct consumer {
        template <typename globals>
        static __device__ void run(const globals &g, state<config> &s) {}
    };
    struct storer {
        // Uses 4 full pages for outputs.
        template <typename globals>
        static __device__ void run(const globals &g, state<config> &s) {}
    };
};

} // namespace megakernel


================================================
FILE: include/storer.cuh
================================================
#pragma once

#include "kittens.cuh"
#include "util.cuh"

MAKE_WORKER(storer, TEVENT_STORER_START, false)


================================================
FILE: include/util.cuh
================================================
#pragma once

#include "kittens.cuh"
#include "config.cuh"

namespace megakernel {

// pid -- physical page id
// lid -- logical page id

template <typename config> struct __align__(128) instruction_state_t {
    config::instruction_t instructions;
    config::timing_t timings;
    int pid_order[config::NUM_PAGES];
    int padding[((config::NUM_PAGES + 31) & ~31) -
                config::NUM_PAGES]; // Round up to multiple of 32
    kittens::semaphore semaphores[config::DYNAMIC_SEMAPHORES];
    int scratch[config::SCRATCH_BYTES / 4];
};

__device__ inline unsigned int get_smid() {
    unsigned int ret;
    asm volatile("mov.u32 %0, %smid;" : "=r"(ret));
    return ret;
}

__device__ inline unsigned int get_worker_id() {
    return get_smid();
    // return blockIdx.x;
}

template <template <typename> typename op_dispatcher, typename... ops>
struct dispatch_op {
    template <typename return_t, typename config, typename globals,
              typename... args>
    __device__ static inline return_t run(int opcode, const globals &g,
                                          args &...a) {
        asm volatile("trap;\n"); // we want to blow up in this case.
        return return_t{};
    } // do nothing, base case
};
template <template <typename> typename op_dispatcher, typename op,
          typename... ops>
struct dispatch_op<op_dispatcher, op, ops...> {
    template <typename return_t, typename config, typename globals,
              typename... args>
    __device__ static inline return_t run(int opcode, const globals &g,
                                          args &...a) {
        if (opcode == op::opcode)
            return op_dispatcher<op>::run(g, a...);
        else
            return dispatch_op<op_dispatcher, ops...>::template run<
                return_t, config, globals, args...>(opcode, g, a...);
    }
};

template<int N> __device__ static inline int ring_advance(int ring, int distance=1) { return (ring + distance) % N; }
template<int N> __device__ static inline int ring_retreat(int ring, int distance=1) { return (ring + 16*N - distance) % N; }

template <typename config> struct page {
    int data[config::PAGE_SIZE / sizeof(int)];
    __device__ inline void *ptr(int byte_offset = 0) {
        return (void *)(data + byte_offset / sizeof(int));
    }
    __device__ inline const void *ptr(int byte_offset = 0) const {
        return (const void *)(data + byte_offset / sizeof(int));
    }
};
template <typename config> struct mini_page {
    int data[config::MINI_PAGE_SIZE / sizeof(int)];
};

template <typename config> struct state {
    using instruction_state_array_t =
        instruction_state_t<config>[config::INSTRUCTION_PIPELINE_STAGES];
    instruction_state_array_t &all_instructions;
    using instruction_semaphore_array_t =
        kittens::semaphore[config::INSTRUCTION_PIPELINE_STAGES];
    instruction_semaphore_array_t &instruction_arrived, &instruction_finished;
    int instruction_index, instruction_ring;
    int reg_pid_order[config::NUM_PAGES];

    __device__ inline int (&instruction())[config::INSTRUCTION_WIDTH] {
        return all_instructions[instruction_ring].instructions;
    }
    __device__ inline const int (&instruction()
                                     const)[config::INSTRUCTION_WIDTH] {
        return all_instructions[instruction_ring].instructions;
    }
    __device__ inline int (&timing())[config::TIMING_WIDTH] {
        return all_instructions[instruction_ring].timings;
    }
    __device__ inline const int (&timing() const)[config::TIMING_WIDTH] {
        return all_instructions[instruction_ring].timings;
    }
    __device__ inline int (&pid_order())[config::NUM_PAGES] {
        return all_instructions[instruction_ring].pid_order;
    }
    __device__ inline const int (&pid_order() const)[config::NUM_PAGES] {
        return all_instructions[instruction_ring].pid_order;
    }
    __device__ inline void *scratch() const {
        return (void *)&all_instructions[instruction_ring].scratch[0];
    }

    template <int num_bytes> __device__ inline void zero_scratch() {
        static_assert(num_bytes % 4 == 0, "num_bytes must be a multiple of 4");
        constexpr auto num_floats = num_bytes / 4;
        auto &scratch_vec = *reinterpret_cast<kittens::sv_fl<num_floats> *>(scratch());
        kittens::warp::zero(scratch_vec);
        kittens::warp::sync();
    }

    __device__ inline kittens::semaphore (
        &semaphores())[config::DYNAMIC_SEMAPHORES] {
        return all_instructions[instruction_ring].semaphores;
    }
    __device__ inline const kittens::semaphore (
        &semaphores() const)[config::DYNAMIC_SEMAPHORES] {
        return all_instructions[instruction_ring].semaphores;
    }
    __device__ inline void await_instruction() {
        kittens::wait(instruction_arrived[instruction_ring],
             (instruction_index / config::INSTRUCTION_PIPELINE_STAGES) & 1);
        pid_order_shared_addr =
            static_cast<uint32_t>(__cvta_generic_to_shared(&(pid_order()[0])));
    }
    __device__ inline void next_instruction() {
        __syncwarp();
        if (kittens::laneid() == 0) {
#ifdef MK_DEBUG
            printf("Thread %d: arriving at instruction finished %d\n",
                   threadIdx.x, instruction_ring);
#endif
            kittens::arrive(instruction_finished[instruction_ring]);
        }
        instruction_index++;
        instruction_ring =
            ring_advance<config::INSTRUCTION_PIPELINE_STAGES>(instruction_ring);
    }

    using page_array_t = page<config>[config::NUM_PAGES];
    page_array_t &pages;

    using page_semaphore_array_t =
        kittens::semaphore[config::NUM_PAGES]
                          [config::INSTRUCTION_PIPELINE_STAGES_BITS];
    page_semaphore_array_t &page_finished;

    __device__ inline int pid(int lid) {
        int ret;
        kittens::move<int>::lds(ret, pid_order_shared_addr + lid * sizeof(int));
        return ret;
    }
    __device__ inline void wait_page_ready(int pid) {
#pragma unroll
        for (int i = 0; i < config::INSTRUCTION_PIPELINE_STAGES_BITS; i++) {
            auto bit = (instruction_index >> i) & 1;
            kittens::wait(page_finished[pid][i], bit);
        }
    }

    __device__ inline void finish_page(int pid, int count) {
#pragma unroll
        for (int i = 0; i < config::INSTRUCTION_PIPELINE_STAGES_BITS; i++) {
            arrive(page_finished[pid][i], count);
        }
    }

    __device__ inline void warp_finish_page(int pid, int count) {
        if (kittens::warp::laneid() == 0) {
            finish_page(pid, count);
        }
    }

#ifdef KITTENS_BLACKWELL
    kittens::semaphore &tensor_finished;
    __device__ inline void wait_tensor_ready() {
        kittens::wait(tensor_finished, instruction_index % 2);
    }
#endif

    kittens::semaphore &semaphores_ready;
    __device__ inline void wait_semaphores_ready() {
        kittens::wait(semaphores_ready, instruction_index % 2);
    }

    uint64_t start_clock;

    __device__ inline void record(int event_id) {
        if constexpr (config::TIMING_RECORD_ENABLED) {
            uint64_t current = clock64();
            int diff = (int)(current - start_clock);
            timing()[event_id] = diff;
        }
    }

#ifdef KITTENS_BLACKWELL
    static constexpr int NCTA_TENSOR_ALLOC = config::CLUSTER_BLOCKS > 1 ? 2 : 1;
    using tensor_allocator_t =
        ::kittens::tensor_allocator<1, NCTA_TENSOR_ALLOC>;
    tensor_allocator_t &tensor_alloc;
#endif

    uint32_t pid_order_shared_addr;

    __device__ inline void print() {
        printf("Kittens Virtual Machine State being printed by thread %d, "
               "block %d\n  Instruction index: %d, Instruction ring: %d\n",
               threadIdx.x, blockIdx.x, instruction_index, instruction_ring);
    }
};

// timing event convention
constexpr int TEVENT_CONTROLLER_START = 0;
constexpr int TEVENT_IFETCH_DONE = 1;
constexpr int TEVENT_PAGE_ALLOC_DONE = 2;
constexpr int TEVENT_SEMS_SETUP = 3;
constexpr int TEVENT_CONTROLLER_END = 4;
constexpr int TEVENT_LOADER_START = 5;
constexpr int TEVENT_LAUNCHER_START = 7;
constexpr int TEVENT_STORER_START = 9;
// need NUM_CONSUMER_WARPS * 2 slots here
constexpr int TEVENT_CONSUMER_START = 11;

constexpr int TEVENT_AT_GMEM_WAIT = 44;
constexpr int TEVENT_DONE_GMEM_WAIT = 45;
constexpr int TEVENT_AT_GMEM_STORE = 46;
constexpr int TEVENT_DONE_GMEM_STORE = 47;

constexpr int TEVENT_FIRST_LOAD = 48;
constexpr int TEVENT_FIRST_USE = 49;
constexpr int TEVENT_FIRST_STORE = 50;

constexpr int TEVENT_LAST_LOAD = 51;
constexpr int TEVENT_LAST_USE = 52;
constexpr int TEVENT_LAST_STORE = 53;

constexpr int TEVENT_OUTPUT_READY = 54;

constexpr int FREE_SLOTS_START = 55;

constexpr int TEVENT_TRIPLES_START = 100;
constexpr int TEVENT_TRIPLES_END = 110;
constexpr int TEVENT_TRIPLES_STORE_START = 124;
constexpr int TEVENT_TRIPLES_OUTPUT_READY = 125;

} // namespace megakernel

#ifdef MK_DEBUG
#define MK_DEBUG_PRINT_START(msg)                                              \
    printf("Thread %d: starting main loop for %s\n", threadIdx.x, msg);
#define MK_DEBUG_PRINT_END(msg)                                                \
    printf("Thread %d: exiting main loop for %s\n", threadIdx.x, msg);
#else
#define MK_DEBUG_PRINT_START(msg)
#define MK_DEBUG_PRINT_END(msg)
#endif

#define MAKE_WORKER(name, start_event, is_consumer)                            \
    namespace megakernel {                                                     \
    namespace name {                                                           \
                                                                               \
    template <typename config, typename globals> struct name##_op_dispatcher { \
        template <typename op> struct dispatcher {                             \
            __device__ static inline void                                      \
            run(const globals &g, ::megakernel::state<config> &mks) {          \
                op::name::run(g, mks);                                         \
            }                                                                  \
        };                                                                     \
    };                                                                         \
                                                                               \
    template <typename config, typename globals, typename... ops>              \
    __device__ void main_loop(const globals &g,                                \
                              ::megakernel::state<config> &mks) {              \
        MK_DEBUG_PRINT_START(#name);                                           \
        int num_iters = g.instructions.rows();                                 \
        for (mks.instruction_index = 0, mks.instruction_ring = 0;              \
             mks.instruction_index < num_iters; mks.next_instruction()) {      \
            mks.await_instruction();                                           \
            if (kittens::laneid() == 0) {                                               \
                if (is_consumer) {                                             \
                    mks.record(start_event + 2 * kittens::warpid());                    \
                } else {                                                       \
                    mks.record(start_event);                                   \
                }                                                              \
            }                                                                  \
            dispatch_op<name##_op_dispatcher<config, globals>::dispatcher,     \
                        ops...>::template run<void, config, globals,           \
                                              ::megakernel::state<config>>(    \
                mks.instruction()[0], g, mks);                                 \
            if (kittens::laneid() == 0) {                                               \
                if (is_consumer) {                                             \
                    mks.record(start_event + 2 * kittens::warpid() + 1);                \
                } else {                                                       \
                    mks.record(start_event + 1);                               \
                }                                                              \
            }                                                                  \
        }                                                                      \
        __syncwarp();                                                          \
        MK_DEBUG_PRINT_END(#name);                                             \
    }                                                                          \
    }                                                                          \
    }



================================================
FILE: include/controller/controller.cuh
================================================
#pragma once

#include "kittens.cuh"

#include "../util.cuh"
#include "instruction_fetch.cuh"
#include "timings_store.cuh"
#include "semaphore_constructor.cuh"
#include "page_allocator.cuh"

namespace megakernel {
namespace controller {

template <typename config, typename globals, typename... ops>
__device__ void main_loop(const globals &g, ::megakernel::state<config> &kvms) {
    auto laneid = ::kittens::laneid();
    int num_iters = g.instructions.rows();
    int num_semaphores[config::INSTRUCTION_PIPELINE_STAGES];

    // for warps
    static_assert(config::DYNAMIC_SEMAPHORES <= 32);
    static_assert(config::NUM_PAGES <= 32);

    for (kvms.instruction_index = 0, kvms.instruction_ring = 0;
         kvms.instruction_index < num_iters;
         kvms.instruction_index++,
        kvms.instruction_ring =
             ring_advance<config::INSTRUCTION_PIPELINE_STAGES>(
                 kvms.instruction_ring)) {

        // Step 0. if the slot was used in the previous iteration, wait for the
        // previous instruction to complete & invalidate its semaphores
        if (kvms.instruction_index >= config::INSTRUCTION_PIPELINE_STAGES) {
            auto last_slot_instruction_index =
                kvms.instruction_index - config::INSTRUCTION_PIPELINE_STAGES;

            int phasebit = (last_slot_instruction_index /
                            config::INSTRUCTION_PIPELINE_STAGES) &
                           1;
            kittens::wait(kvms.instruction_finished[kvms.instruction_ring], phasebit);

            if (laneid < num_semaphores[kvms.instruction_ring]) {
                invalidate_semaphore(
                    kvms.all_instructions[kvms.instruction_ring]
                        .semaphores[laneid]);
            }

            // TODO needed?
            kittens::warp::sync();

            if (laneid == 0) {
                if constexpr (config::TIMING_RECORD_ENABLED) {
                    kvms.record(TEVENT_CONTROLLER_END);
                    store_timings_and_reset<config, globals>(
                        &kvms.all_instructions[kvms.instruction_ring]
                             .timings[0],
                        last_slot_instruction_index, g);
                }
            }
        }

        if (laneid == 0) {
            kvms.record(TEVENT_CONTROLLER_START);
        }

        // Step 1. Load instructions (no semaphores used)
        load_instructions<config, globals>(&kvms.instruction()[0],
                                           kvms.instruction_index, g);

        if (laneid == 0) {
            kvms.record(TEVENT_IFETCH_DONE);
        }

        // Step 2. Establish physical page order
        int last_instruction_ring =
            (kvms.instruction_ring + config::INSTRUCTION_PIPELINE_STAGES - 1) %
            config::INSTRUCTION_PIPELINE_STAGES;

        if (kvms.instruction_index == 0) {
            if (laneid < config::NUM_PAGES) {
                kvms.pid_order()[laneid] = laneid;
            }
        } else {
            auto last_opcode =
                kvms.all_instructions[last_instruction_ring].instructions[0];

            if (laneid < config::NUM_PAGES) {
                int lid = dispatch_op<
                    page_allocator_op_dispatcher<config, globals>::dispatcher,
                    ops...>::template run<int, config, globals,
                                          config::instruction_t, int>(
                    last_opcode, g,
                    kvms.all_instructions[last_instruction_ring].instructions,
                    laneid);

                kvms.pid_order()[laneid] =
                    kvms.all_instructions[last_instruction_ring].pid_order[lid];
            }
        }

        if (laneid == 0) {
            kvms.record(TEVENT_PAGE_ALLOC_DONE);
        }

        // Step 3. Construct semaphores
        int opcode = kvms.instruction()[0];
        if (opcode == 0) {
            num_semaphores[kvms.instruction_ring] = 0;
        } else {
            if (laneid == 0) {
                num_semaphores[kvms.instruction_ring] = dispatch_op<
                    semaphore_constructor_op_dispatcher<config,
                                                        globals>::dispatcher,
                    ops...>::template run<int, config, globals,
                                          ::megakernel::state<config>>(opcode,
                                                                       g, kvms);
            }

            auto shfl_val = __shfl_sync(
                0xffffffff, num_semaphores[kvms.instruction_ring], 0);

            // broadcast the result to all lanes
            num_semaphores[kvms.instruction_ring] = shfl_val;
        }

        if (laneid == 0) {
            kvms.record(TEVENT_SEMS_SETUP);
            // Step 4. Let the rest of the world know that next instruction is
            // ready to roll!
            arrive(kvms.instruction_arrived[kvms.instruction_ring], 1);
        }
    }

    // invalidate remaining semaphores and write out remaining timings
    for (int i = 0; i < config::INSTRUCTION_PIPELINE_STAGES; i++) {
        auto instruction_index =
            num_iters - config::INSTRUCTION_PIPELINE_STAGES + i;
        if (instruction_index < 0) {
            continue;
        }

        auto instruction_ring =
            instruction_index % config::INSTRUCTION_PIPELINE_STAGES;

        auto phasebit =
            (instruction_index / config::INSTRUCTION_PIPELINE_STAGES) & 1;
        kittens::wait(kvms.instruction_finished[instruction_ring], phasebit);

        if (laneid < num_semaphores[instruction_ring]) {
            invalidate_semaphore(
                kvms.all_instructions[instruction_ring].semaphores[laneid]);
        }

        kvms.instruction_index = instruction_index;
        kvms.instruction_ring = instruction_ring;
        // record using the current ring
        if (laneid == 0)
            kvms.record(TEVENT_CONTROLLER_END);

        // technically don't need to reset, whatevs?
        store_timings_and_reset<config, globals>(
            &kvms.all_instructions[instruction_ring].timings[0],
            instruction_index, g);
    }
}

} // namespace controller
} // namespace megakernel



================================================
FILE: include/controller/instruction_fetch.cuh
================================================
#pragma once

#include "kittens.cuh"

#include "../util.cuh"

namespace megakernel {
namespace controller {

template <typename config, typename globals>
__device__ void inline load_instructions(int *instruction,
                                         int instruction_index,
                                         const globals &g) {
    auto laneid = ::kittens::laneid();

    auto src_ptr = &g.instructions[kittens::coord<>{(int)(get_worker_id()),
                                                    instruction_index, 0}];
    // static assert it's an int*
    static_assert(std::is_same<decltype(src_ptr), int *>::value,
                  "src_ptr is not an int*");

    static_assert(config::INSTRUCTION_WIDTH <= 32);

    if (laneid < config::INSTRUCTION_WIDTH) {
        instruction[laneid] = src_ptr[laneid];
    }
}

template <typename config, typename globals>
__device__ void inline instruction_fetch_loop(
    const globals &g, ::megakernel::state<config> &kvms) {
    static_assert(config::INSTRUCTION_PIPELINE_STAGES <= 16,
                  "This would be an absurd thing to do.");
    int num_iters = g.instructions.rows();
    for (kvms.instruction_index = 0, kvms.instruction_ring = 0;
         kvms.instruction_index < num_iters;
         kvms.instruction_index++,
        kvms.instruction_ring =
             ring_advance<config::INSTRUCTION_PIPELINE_STAGES>(
                 kvms.instruction_ring)) {
        int phasebit =
            (kvms.instruction_index / config::INSTRUCTION_PIPELINE_STAGES - 1) &
            1;
        if (kvms.instruction_index >= config::INSTRUCTION_PIPELINE_STAGES) {
            kittens::wait(kvms.instruction_finished[kvms.instruction_ring], phasebit);
        }
        load_instructions<config, globals>(
            &kvms.instruction()[0], kvms.instruction_index, g,
            kvms.instruction_arrived[kvms.instruction_ring]);
    }
}

} // namespace controller
} // namespace megakernel



================================================
FILE: include/controller/page_allocator.cuh
================================================
#pragma once

#include "kittens.cuh"

#include "../util.cuh"

namespace megakernel {
namespace controller {

template <typename config, typename globals>
struct page_allocator_op_dispatcher {
    template <typename op> struct dispatcher {
        __device__ static inline int
        run(const globals &g, typename config::instruction_t &instruction,
            int &query) {
            return op::controller::release_lid(g, instruction, query);
        }
    };
};

template <typename config, typename globals, typename... ops>
__device__ void inline page_allocator_loop(const globals &g,
                                           ::megakernel::state<config> &kvms) {
    static_assert(config::INSTRUCTION_PIPELINE_STAGES <= 16,
                  "This would be an absurd thing to do.");
    constexpr uint32_t membermask = 0xFFFFFFFF >> (32 - config::NUM_PAGES);
    int num_iters = g.instructions.rows();
    for (kvms.instruction_index = 0, kvms.instruction_ring = 0;
         kvms.instruction_index < num_iters;
         kvms.instruction_index++,
        kvms.instruction_ring =
             ring_advance<config::INSTRUCTION_PIPELINE_STAGES>(
                 kvms.instruction_ring)) {

        int phasebit =
            (kvms.instruction_index / config::INSTRUCTION_PIPELINE_STAGES - 1) &
            1;
        if (kvms.instruction_index >= config::INSTRUCTION_PIPELINE_STAGES)
            kittens::wait(kvms.instruction_finished[kvms.instruction_ring], phasebit);

        int next_pid;
        if (kvms.instruction_index == 0)
            next_pid = kittens::laneid();
        else {
            int last_instruction_ring =
                (kvms.instruction_ring + config::INSTRUCTION_PIPELINE_STAGES -
                 1) %
                config::INSTRUCTION_PIPELINE_STAGES;
            kittens::wait(kvms.instruction_arrived[last_instruction_ring],
                 ((kvms.instruction_index - 1) /
                  config::INSTRUCTION_PIPELINE_STAGES) &
                     1);
            int lane = kittens::laneid();
            int opcode =
                kvms.all_instructions[last_instruction_ring].instructions[0];
            int lid = dispatch_op<
                page_allocator_op_dispatcher<config, globals>::dispatcher,
                ops...>::template run<int, config, globals,
                                      config::instruction_t, int>(
                opcode, g,
                kvms.all_instructions[last_instruction_ring].instructions,
                lane);
            next_pid =
                kvms.all_instructions[last_instruction_ring].pid_order[lid];
        }
        kvms.pid_order()[kittens::laneid()] = next_pid;
        asm volatile("bar.warp.sync %0;\n" ::"n"(membermask));
        if (kittens::laneid() == 0)
            kittens::arrive(kvms.instruction_arrived[kvms.instruction_ring], 1);
    }
}

} // namespace controller
} // namespace megakernel



================================================
FILE: include/controller/semaphore_constructor.cuh
================================================
#pragma once

#include "kittens.cuh"

#include "../util.cuh"

namespace megakernel {
namespace controller {

template <typename config, typename globals>
struct semaphore_constructor_op_dispatcher {
    template <typename op> struct dispatcher {
        __device__ static inline int
        run(const globals &g, ::megakernel::state<config> &kvms) {
            auto out = op::controller::init_semaphores(g, kvms);
            asm volatile("fence.proxy.async.shared::cta;\n" ::: "memory");
            return out;
        }
    };
};

template <typename config, typename globals, typename... ops>
__device__ void inline semaphore_constructor_loop(
    const globals &g, ::megakernel::state<config> &kvms) {
    static_assert(config::INSTRUCTION_PIPELINE_STAGES == 2,
                  "Need to be changed.");
    int num_iters = g.instructions.rows();
    int tic = 0;
    int last_num_semaphores;
    for (kvms.instruction_index = 0, kvms.instruction_ring = 0;
         kvms.instruction_index < num_iters;
         kvms.instruction_index++,
        kvms.instruction_ring =
             ring_advance<config::INSTRUCTION_PIPELINE_STAGES>(
                 kvms.instruction_ring),
        tic = 1 - tic) {

        kittens::wait(kvms.instruction_arrived[kvms.instruction_ring],
             (kvms.instruction_index / config::INSTRUCTION_PIPELINE_STAGES) &
                 1);
        int opcode = kvms.instruction()[0];
        int next_num_semaphores;
        if (opcode == 0) {
            next_num_semaphores = 0;
        } else {
            next_num_semaphores = dispatch_op<
                semaphore_constructor_op_dispatcher<config,
                                                    globals>::dispatcher,
                ops...>::template run<int, config, globals,
                                      ::megakernel::state<config>>(
                opcode, g, kvms);
        }
        arrive(kvms.semaphores_ready);
        if (kvms.instruction_index > 0) {
            int last_ring = ring_retreat<config::INSTRUCTION_PIPELINE_STAGES>(
                kvms.instruction_ring);
            kittens::wait(kvms.instruction_finished[last_ring],
                 ((kvms.instruction_index - 1) /
                  config::INSTRUCTION_PIPELINE_STAGES) &
                     1);
            for (int i = 0; i < last_num_semaphores; i++) {
                invalidate_semaphore(
                    kvms.all_instructions[last_ring].semaphores[i]);
            }
        }
        last_num_semaphores = next_num_semaphores;
    }
    // if(blockIdx.x == 0) printf("110\n");
    if (num_iters > 0) {
        int last_ring = ring_retreat<config::INSTRUCTION_PIPELINE_STAGES>(
            kvms.instruction_ring);
        kittens::wait(kvms.instruction_finished[last_ring],
             ((kvms.instruction_index - 1) /
              config::INSTRUCTION_PIPELINE_STAGES) &
                 1);
        for (int i = 0; i < last_num_semaphores; i++) {
            invalidate_semaphore(
                kvms.all_instructions[last_ring].semaphores[i]);
        }
    }
}

} // namespace controller
} // namespace megakernel



================================================
FILE: include/controller/timings_store.cuh
================================================
#pragma once

#include "kittens.cuh"

#include "../util.cuh"

namespace megakernel {
namespace controller {

template <typename config, typename globals>
__device__ void inline store_timings(int *timings, int instruction_index,
                                     const globals &g) {
    constexpr int bytes = config::TIMING_WIDTH * sizeof(int);
    uint32_t src_ptr = static_cast<uint32_t>(__cvta_generic_to_shared(timings));
    uint64_t dst_ptr = (uint64_t)(&g.timings[kittens::coord<>{
        (int)(get_worker_id()), instruction_index, 0}]);
    asm volatile("fence.proxy.async.shared::cta;\n" ::: "memory");
    asm volatile("cp.async.bulk.global.shared::cta.bulk_group [%0], [%1], %2;\n"
                 :
                 : "l"(dst_ptr), "r"(src_ptr), "n"(bytes)
                 : "memory");
    kittens::tma::store_commit_group();
}

template <typename config, typename globals>
__device__ void inline store_timings_and_reset(int *timings,
                                               int instruction_index,
                                               const globals &g) {
    if (kittens::laneid() == 0) {
        store_timings<config, globals>(timings, instruction_index, g);
        kittens::tma::store_async_read_wait();
#ifdef KITTENS_BLACKWELL
        uint32_t src_ptr =
            static_cast<uint32_t>(__cvta_generic_to_shared(timings));
        asm volatile("st.bulk.weak [%0], %1, 0;\n" ::"r"(src_ptr),
                     "n"(config::TIMING_WIDTH *
                         sizeof(int))); // Reinitialize timing memory as zeros.
#endif
    }
#ifndef KITTENS_BLACKWELL
    __syncwarp();
    for (int i = kittens::laneid(); i < config::TIMING_WIDTH; i += kittens::WARP_THREADS) {
        timings[i] = 0;
    }
#endif
}

} // namespace controller
} // namespace megakernel



================================================
FILE: megakernels/__init__.py
================================================
[Empty file]


================================================
FILE: megakernels/dispatch.py
================================================
from pathlib import Path

from megakernels.demos.latency.mk import LatencyMK_Interpreter
from megakernels.demos.latency.python_vm import (
    INSTRUCTION_TO_SOLVER as LATENCY_INSTRUCTION_TO_SOLVER,
)
from megakernels.demos.latency.scheduler import LatencyScheduleBuilder
from megakernels.demos.throughput.mk import ThroughputMK_Interpreter
from megakernels.demos.throughput.python_vm import (
    INSTRUCTION_TO_SOLVER as THROUGHPUT_INSTRUCTION_TO_SOLVER,
)
from megakernels.demos.throughput.scheduler import ThroughputScheduleBuilder
from megakernels.mk import MK_Interpreter
from megakernels.python_vm import PyVM_Interpreter
from megakernels.scheduler import ScheduleBuilder

BUILDER_MAP = {
    "latency": LatencyScheduleBuilder,
    "throughput": ThroughputScheduleBuilder,
}

MK_INTERPRETER_MAP = {
    "latency": LatencyMK_Interpreter,
    "throughput": ThroughputMK_Interpreter,
}

INSTRUCTION_TO_SOLVER_MAP = {
    "latency": LATENCY_INSTRUCTION_TO_SOLVER,
    "throughput": THROUGHPUT_INSTRUCTION_TO_SOLVER,
}


def make_schedule_builder(mode: str) -> ScheduleBuilder:
    return BUILDER_MAP[mode]()


def make_mk_interpreter(mode: str, mk_dir: Path) -> MK_Interpreter:
    return MK_INTERPRETER_MAP[mode](mk_dir)


def make_pyvm_interpreter(mode: str) -> PyVM_Interpreter:
    return PyVM_Interpreter(INSTRUCTION_TO_SOLVER_MAP[mode])



================================================
FILE: megakernels/generators.py
================================================
import torch
from torch import Tensor

from megakernels.llama import LlamaForCausalLM
from megakernels.mk import MK_Interpreter
from megakernels.model_types import BatchState
from megakernels.python_vm import PyVM_Interpreter
from megakernels.scheduler import Schedule


class Generator:
    def generate(
        self,
        output_tokens: Tensor,
        prompt_len: int,
        ntok: int,
        ntok_already_generated: int = 1,
    ):
        raise NotImplementedError

    def generate_with_eos(
        self,
        output_tokens: Tensor,
        prompt_len: int,
        ntok: int,
        eos_token_check_interval: int,
        eos_token_ids: list[int],
    ):
        """
        Return pos id with first eos token, and total num tokens generated
        """
        assert output_tokens.shape[0] == 1, "batch size must be 1"

        for ntok_already_generated in range(
            1,
            ntok,
            eos_token_check_interval,
        ):
            ntok_for_chunk = min(
                eos_token_check_interval, ntok - ntok_already_generated
            )
            self.generate(
                output_tokens,
                prompt_len=prompt_len,
                ntok=ntok_for_chunk,
                ntok_already_generated=ntok_already_generated,
            )

            start_out_idx = ntok_already_generated
            end_out_idx = ntok_already_generated + ntok_for_chunk

            to_cpu = output_tokens[0, start_out_idx:end_out_idx].cpu()
            for j, token in enumerate(to_cpu):
                if token in eos_token_ids:
                    # -1 because we didn't generate the first token
                    return start_out_idx + j, end_out_idx - 1

        return ntok, ntok - 1


class PyTorchGenerator(Generator):
    def __init__(
        self,
        model: LlamaForCausalLM,
    ):
        self.model = model

    def generate(
        self,
        output_tokens: Tensor,
        prompt_len: int,
        ntok: int,
        ntok_already_generated: int = 1,
    ):
        bs = output_tokens.shape[0]
        starting_seq_len = prompt_len + ntok_already_generated
        start_position_ids = torch.ones(
            bs, 1, dtype=torch.long, device=self.model.device
        ) * (starting_seq_len - 1)

        for i in range(ntok):
            position_ids = start_position_ids + i
            input_token_pos = i + ntok_already_generated - 1
            decode_inp = BatchState(
                input_ids=output_tokens[:, input_token_pos : input_token_pos + 1],
                position_ids=position_ids,
                seq_len=starting_seq_len + i + 1,
            )
            decode_output: BatchState = self.model(decode_inp)
            assert decode_output.output_ids is not None
            output_pos = input_token_pos + 1
            output_tokens[:, output_pos] = decode_output.output_ids.squeeze(-1)


class MK_Generator(Generator):
    def __init__(
        self,
        model: LlamaForCausalLM,
        interpreter: MK_Interpreter,
        schedule: Schedule,
        barrier_fill_val: int = 0,
        skip_mk: bool = False,
        skip_rest: bool = False,
    ):
        self.model = model
        self.interpreter = interpreter
        self.schedule = schedule

        self.barrier_fill_val = barrier_fill_val
        self.skip_mk = skip_mk
        self.skip_rest = skip_rest

        self.fill()

    def fill(self):
        self.schedule.globs.barriers.fill_(self.barrier_fill_val)

    def replace_with_noops(self):
        self.schedule.globs.instructions.zero_()

    def run(self, input_ids: Tensor, pos_id: int):
        if not self.skip_rest:
            batch_state = BatchState(
                input_ids=input_ids,
            )

            post_embedding: BatchState = self.model.model.embed_tokens(batch_state)
            hiddens = post_embedding.hidden_states
            assert hiddens is not None
            self.schedule.globs.hidden_states[:] = hiddens.squeeze(1)

        self.fill()
        self.schedule.globs.pos_id = pos_id
        if not self.skip_mk:
            self.interpreter.interpret(self.schedule.globs)

        if self.skip_rest:
            return input_ids

        logits = self.schedule.globs.logits
        output_ids = torch.argmax(logits, dim=-1)

        return output_ids

    def generate(
        self,
        output_tokens: Tensor,
        prompt_len: int,
        ntok: int,
        ntok_already_generated: int = 1,
    ):
        """
        Return num tokens until stop seq, and total num tokens generated
        """
        for i in range(ntok):
            input_token_pos = ntok_already_generated + i - 1
            output_token_pos = input_token_pos + 1

            input_ids = output_tokens[:, input_token_pos : input_token_pos + 1]

            pos_id = prompt_len + ntok_already_generated + i - 1
            output_ids = self.run(input_ids, pos_id=pos_id)
            output_tokens[:, output_token_pos] = output_ids.squeeze(-1)


class PyVM_Generator(MK_Generator):
    def __init__(
        self,
        model: LlamaForCausalLM,
        interpreter: PyVM_Interpreter,
        schedule: Schedule,
    ):
        self.model = model
        self.interpreter = interpreter
        self.schedule = schedule

        self.instructions = self.schedule.get_linear_instructions()

    def run(self, input_ids: Tensor, pos_id: int):
        batch_state = BatchState(
            input_ids=input_ids,
        )

        post_embedding: BatchState = self.model.model.embed_tokens(batch_state)
        hiddens = post_embedding.hidden_states
        assert hiddens is not None
        self.schedule.globs.hidden_states[:] = hiddens
        self.schedule.globs.barriers.zero_()
        self.schedule.globs.pos_id = pos_id

        self.interpreter.interpret(self.schedule.globs, self.instructions)

        output_hiddens = self.schedule.globs.hidden_states

        post_embedding.hidden_states = output_hiddens

        post_lm_head: BatchState = self.model.lm_head(post_embedding)

        output_ids = post_lm_head.output_ids
        assert output_ids is not None
        return output_ids



================================================
FILE: megakernels/instructions.py
================================================
from dataclasses import dataclass, fields
from typing import Any

from torch import Tensor

from megakernels.model_types import DeviceType
from megakernels.utils import get_sm_count


@dataclass
class BaseGlobals:
    # model parameters, all layers stacked together in order
    qkv_proj_weights: Tensor
    attn_ln_weights: Tensor
    o_proj_weights: Tensor
    mlp_ln_weights: Tensor
    up_proj_weights: Tensor
    gate_proj_weights: Tensor
    down_proj_weights: Tensor
    lm_head_norm_weights: Tensor
    lm_head_weights: Tensor
    k_cache: Tensor
    v_cache: Tensor

    # not stacked for each layer
    rope_cos: Tensor
    rope_sin: Tensor

    # model constants
    num_hidden_layers: int
    num_attention_heads: int
    num_kv_heads: int
    head_dim: int
    hidden_size: int
    intermediate_size: int
    vocab_size: int

    attn_scale: float
    rms_norm_eps: float
    device: DeviceType

    hidden_states: Tensor
    barriers: Tensor

    pos_id: int

    def __post_init__(self):
        self.instructions: Tensor | None = None
        self.timings: Tensor | None = None

    def sm_count(self) -> int:
        return get_sm_count(self.device)

    def diff(self, other: "BaseGlobals", skip_kv_cache: bool = False):
        for field in fields(self):
            name = field.name
            attr = getattr(self, name)
            other_attr = getattr(other, name)
            if (
                not isinstance(attr, Tensor)
                or "weights" in name
                or name in ["rope_cos", "rope_sin"]
                or (skip_kv_cache and "cache" in name)
            ):
                continue
            diff_tensors(attr, other_attr, name)

    def num_total_heads(self) -> int:
        return self.num_attention_heads + self.num_kv_heads * 2


def diff_tensors(a: Tensor, b: Tensor, name: str):
    a = a.float()
    b = b.float()

    diff = a - b
    adiff = diff.abs()
    rdiff = 2 * adiff / (a.abs() + b.abs() + 1e-6)
    print(f"{name}: max adiff: {adiff.max()}, mean rdiff: {rdiff.mean()}")
    return diff, adiff, rdiff


@dataclass
class Instruction:
    @classmethod
    def opcode(cls) -> int:
        raise NotImplementedError

    @classmethod
    def prev_opcode(cls) -> int:
        raise NotImplementedError

    @classmethod
    def tags(cls) -> dict[str, Any]:
        return {}

    def serialize(self):
        words = [self.opcode()]
        for field in fields(self):
            name = field.name
            if name == "global_idx":
                continue
            attr = getattr(self, name)

            if isinstance(attr, int):
                words.append(attr)
            elif isinstance(attr, tuple):
                words.append(len(attr))
                words.extend(attr)
            elif isinstance(attr, list):
                words.append(len(attr))
                words.extend(attr)
            # for convenience
            elif attr is None:
                words.append(0)
            else:
                raise ValueError(f"Unsupported field type: {attr}")

        return words


@dataclass
class NoOp(Instruction):
    @classmethod
    def opcode(cls) -> int:
        return 0


@dataclass
class PrintInfo:
    layer_filter: list[int] | None = None
    name_filter: list[str] | None = None
    state_filter: list[str] | None = None


@dataclass
class PrintState(Instruction):
    layer_idx: int
    name: str
    print_info: PrintInfo



================================================
FILE: megakernels/llama.py
================================================
from dataclasses import dataclass
from pathlib import Path

import huggingface_hub
import torch
import torch.nn.functional as F
from accelerate import init_empty_weights
from einops import rearrange
from torch import Tensor, nn
from torch.distributed import _functional_collectives as funcol
from transformers import LlamaConfig
from transformers.models.llama.modeling_llama import (
    LlamaRotaryEmbedding,
    apply_rotary_pos_emb,
)

from megakernels.model_types import (
    BatchState,
    DeviceType,
    ExtraModelConfig,
)
from megakernels.utils import (
    load_safetensors_repo,
)

KV_Cache = tuple[Tensor, Tensor]


class RMSNorm(nn.Module):
    def __init__(self, config: LlamaConfig):
        """
        Taken from LlamaRMSNorm.
        """
        super().__init__()
        self.config = config
        self.weight = nn.Parameter(torch.ones(config.hidden_size))

    def forward(self, hidden_states: Tensor):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.config.rms_norm_eps)

        if self.weight is not None:
            return self.weight * hidden_states.to(input_dtype)
        else:
            return hidden_states.to(input_dtype)


def all_gather(x: Tensor, extra_config: ExtraModelConfig):
    if extra_config.tp_size == 1:
        return x

    assert extra_config.tp_group is not None

    # funcol + no compile + tp > 1 and pp > 1 leads to some nccl crash
    if extra_config.torch_compile:
        return funcol.all_gather_tensor(x, gather_dim=0, group=extra_config.tp_group)

    out = torch.empty(
        (extra_config.tp_size * x.shape[0], *x.shape[1:]),
        device=x.device,
        dtype=x.dtype,
    )
    torch.distributed.all_gather_into_tensor(out, x, group=extra_config.tp_group)
    return out


def reduce_scatter(x: Tensor, extra_config: ExtraModelConfig):
    if extra_config.tp_size == 1:
        return x

    assert extra_config.tp_group is not None

    # funcol + no compile + tp > 1 and pp > 1 leads to some nccl crash
    if extra_config.torch_compile:
        return funcol.reduce_scatter_tensor(
            x, reduceOp="sum", scatter_dim=0, group=extra_config.tp_group
        )

    out = torch.empty(
        (x.shape[0] // extra_config.tp_size, *x.shape[1:]),
        device=x.device,
        dtype=x.dtype,
    )
    torch.distributed.reduce_scatter_tensor(out, x, group=extra_config.tp_group)
    return out


def attention(
    query_states: Tensor,
    key_states: Tensor,
    value_states: Tensor,
    kv_cache: KV_Cache,
    position_ids: Tensor,
    seq_len: int,
) -> Tensor:
    bsz, new_tok_seq_len = query_states.shape[:2]

    k_cache, v_cache = kv_cache

    k_cache[:, position_ids] = key_states
    v_cache[:, position_ids] = value_states

    def shape_for_sdpa(x: Tensor):
        return rearrange(x, "b l h d -> b h l d")

    def unshape_for_sdpa(x: Tensor):
        return rearrange(x, "b h l d -> b l h d")

    if new_tok_seq_len > 1:
        k_for_sdpa = shape_for_sdpa(key_states)
        v_for_sdpa = shape_for_sdpa(value_states)

        q_for_sdpa = shape_for_sdpa(query_states)

        # assume running prefill from scratch
        attn_output = F.scaled_dot_product_attention(
            q_for_sdpa, k_for_sdpa, v_for_sdpa, is_causal=True, enable_gqa=True
        )
    else:
        # decode
        k_for_sdpa = shape_for_sdpa(k_cache[:, :seq_len])
        v_for_sdpa = shape_for_sdpa(v_cache[:, :seq_len])

        q_for_sdpa = shape_for_sdpa(query_states)

        attn_output = F.scaled_dot_product_attention(
            q_for_sdpa, k_for_sdpa, v_for_sdpa, is_causal=False, enable_gqa=True
        )

    reshaped_attn_output = unshape_for_sdpa(attn_output)
    return reshaped_attn_output


def rotate_half_interleaved(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., ::2]
    x2 = x[..., 1::2]

    new_x1 = -x2
    new_x2 = x1

    stacked = torch.stack((new_x1, new_x2), dim=-1)
    return stacked.view_as(x)


def apply_rotary_pos_emb_interleaved(
    q, k, cos, sin, position_ids=None, unsqueeze_dim=1
):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half_interleaved(q) * sin)
    k_embed = (k * cos) + (rotate_half_interleaved(k) * sin)
    return q_embed, k_embed


class LlamaAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(
        self, config: LlamaConfig, extra_config: ExtraModelConfig, layer_idx: int
    ):
        super().__init__()
        self.config = config
        self.extra_config = extra_config
        self.layer_idx = layer_idx

        self.input_layernorm = RMSNorm(config)

        self.tp_size = extra_config.tp_size or 1

        assert config.num_attention_heads % self.tp_size == 0
        head_dim = config.hidden_size // config.num_attention_heads
        self.head_dim = head_dim

        assert self.config.num_attention_heads % self.tp_size == 0
        assert (
            self.config.num_key_value_heads % self.tp_size == 0
            or self.config.num_key_value_heads == 1
        )

        self.num_attention_heads = config.num_attention_heads // self.tp_size
        self.num_kv_heads = (
            config.num_key_value_heads // self.tp_size
            if config.num_key_value_heads > 1
            else 1
        )

        self.q_proj = nn.Linear(
            self.config.hidden_size,
            self.num_attention_heads * head_dim,
            bias=False,
        )
        self.k_proj = nn.Linear(
            self.config.hidden_size,
            self.num_kv_heads * head_dim,
            bias=False,
        )
        self.v_proj = nn.Linear(
            self.config.hidden_size,
            self.num_kv_heads * head_dim,
            bias=False,
        )
        self.o_proj = nn.Linear(
            self.num_attention_heads * head_dim,
            config.hidden_size,
            bias=False,
        )

        self.kv_cache: KV_Cache | None = None

    def forward(
        self,
        batch_state: BatchState,
    ):
        assert batch_state.hidden_states is not None
        assert batch_state.position_embeddings is not None
        assert batch_state.position_ids is not None
        assert self.kv_cache is not None
        assert batch_state.seq_len is not None

        inp = batch_state.hidden_states
        residual = inp

        hidden_states = self.input_layernorm(inp)

        hidden_states = all_gather(hidden_states, self.extra_config)
        bsz, seq_len = hidden_states.shape[:2]

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, seq_len, self.num_attention_heads, -1)
        key_states = key_states.view(bsz, seq_len, self.num_kv_heads, -1)
        value_states = value_states.view(bsz, seq_len, self.num_kv_heads, -1)

        cos, sin = batch_state.position_embeddings

        dtype = query_states.dtype

        if self.extra_config.interleave_rope:
            rope_fn = apply_rotary_pos_emb_interleaved
        else:
            rope_fn = apply_rotary_pos_emb

        query_states, key_states = rope_fn(
            query_states,
            key_states,
            cos,
            sin,
            unsqueeze_dim=-2,  # unsqueeze dim = head dim on q/k
        )

        query_states = query_states.to(dtype)
        key_states = key_states.to(dtype)

        raw_attn_output = attention(
            query_states,
            key_states,
            value_states,
            self.kv_cache,
            batch_state.position_ids,
            seq_len=batch_state.seq_len,
        )

        attn_output = raw_attn_output.reshape(bsz, seq_len, -1)

        o_proj = self.o_proj(attn_output)

        o_proj = reduce_scatter(o_proj, self.extra_config)

        with_residual = residual + o_proj

        batch_state.hidden_states = with_residual
        return batch_state


class LlamaMLP(nn.Module):
    def __init__(
        self, config: LlamaConfig, extra_config: ExtraModelConfig, layer_idx: int
    ):
        super().__init__()
        self.config = config
        self.extra_config = extra_config
        self.layer_idx = layer_idx

        self.tp_size = extra_config.tp_size
        assert self.config.intermediate_size % self.tp_size == 0
        self.intermediate_size = self.config.intermediate_size // self.tp_size

        self.up_proj = nn.Linear(
            self.config.hidden_size,
            self.intermediate_size,
            bias=False,
        )
        self.gate_proj = nn.Linear(
            config.hidden_size, self.intermediate_size, bias=False
        )
        self.down_proj = nn.Linear(
            self.intermediate_size,
            config.hidden_size,
            bias=False,
        )

        self.input_layernorm = RMSNorm(config)

    def forward(
        self,
        batch_state: BatchState,
    ):
        inp = batch_state.hidden_states
        assert inp is not None
        hidden_states = self.input_layernorm(inp)

        hidden_states = all_gather(hidden_states, self.extra_config)

        up = self.up_proj(hidden_states)
        gate = self.gate_proj(hidden_states)
        prod = F.silu(gate) * up
        down = self.down_proj(prod)

        down = reduce_scatter(down, self.extra_config)

        with_residual = inp + down

        batch_state.hidden_states = with_residual
        return batch_state


class LlamaBlock(nn.Module):
    def __init__(
        self, config: LlamaConfig, extra_config: ExtraModelConfig, layer_idx: int
    ):
        super().__init__()
        self.config = config
        self.extra_config = extra_config
        self.layer_idx = layer_idx

        self.self_attn = LlamaAttention(config, extra_config, layer_idx)
        self.mlp = LlamaMLP(config, extra_config, layer_idx)

    def forward(self, batch_state: BatchState):
        out = self.self_attn(batch_state)
        out = self.mlp(out)
        return out


class LlamaLMHead(nn.Module):
    def __init__(self, config: LlamaConfig, extra_config: ExtraModelConfig):
        super().__init__()
        self.config = config
        self.extra_config = extra_config

        self.input_norm = RMSNorm(config)

        self.tp_size = extra_config.tp_size or 1

        assert config.vocab_size % self.tp_size == 0
        head_size = config.vocab_size

        self.lm_head = nn.Linear(config.hidden_size, head_size, bias=False)

    def forward(self, batch_state: BatchState):
        assert batch_state.hidden_states is not None

        hidden_states = batch_state.hidden_states

        if self.extra_config.tp_size > 1:
            hidden_states = all_gather(hidden_states, self.extra_config)

        hidden_states = self.input_norm(hidden_states)

        logits = self.lm_head(hidden_states)

        next_token_ids = logits.argmax(dim=-1)

        if self.tp_size > 1:
            # TODO: fusion
            next_token_ids = all_gather(next_token_ids, self.extra_config)

        batch_state.output_ids = next_token_ids
        return batch_state


class LlamaEmbeddings(nn.Module):
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.config = config

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)

    def forward(self, batch_state: BatchState):
        hidden_states = self.embed_tokens(batch_state.input_ids)

        batch_state.hidden_states = hidden_states
        return batch_state


class LlamaModel(nn.Module):
    rope_cos: Tensor
    rope_sin: Tensor

    def __init__(
        self,
        config: LlamaConfig,
        extra_config: ExtraModelConfig,
    ):
        super().__init__()
        self.config = config
        self.extra_config = extra_config
        self.embed_tokens = LlamaEmbeddings(config)

        layers = []
        for i in range(config.num_hidden_layers):
            layers.append(LlamaBlock(config, extra_config, i))

        self.layers = nn.ModuleList(layers)

        self.rope = LlamaRotaryEmbedding(
            config=config,
        )

        position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)
        dummy_float_input = torch.empty((0, config.hidden_size), dtype=torch.float32)

        cos, sin = self.rope(dummy_float_input, position_ids)
        assert cos.dtype == torch.float32
        assert sin.dtype == torch.float32
        self.register_buffer("rope_cos", cos.squeeze(0), persistent=False)
        self.register_buffer("rope_sin", sin.squeeze(0), persistent=False)

    def interleave_rope(self):
        indices_for_q_list = []
        half_head_dim = self.config.head_dim // 2
        for n in range(self.config.num_attention_heads):
            offset = n * self.config.head_dim
            for i in range(half_head_dim):
                indices_for_q_list.append(i + offset)
                indices_for_q_list.append(i + half_head_dim + offset)

        indices_for_q = torch.tensor(indices_for_q_list, device=self.rope_cos.device)
        one_head_indices = indices_for_q[: self.config.head_dim]

        self.rope_cos = self.rope_cos[..., one_head_indices]
        self.rope_sin = self.rope_sin[..., one_head_indices]

        indices_for_k = indices_for_q[
            : self.config.head_dim * self.config.num_key_value_heads
        ]

        for mod in self.modules():
            if isinstance(mod, LlamaAttention):
                mod.q_proj.weight[:] = mod.q_proj.weight[indices_for_q]
                mod.k_proj.weight[:] = mod.k_proj.weight[indices_for_k]

    def forward(self, batch_state: BatchState):
        out: BatchState = self.embed_tokens(batch_state)
        assert self.rope_cos.dtype == torch.float32
        assert self.rope_sin.dtype == torch.float32
        cos = self.rope_cos[batch_state.position_ids]
        sin = self.rope_sin[batch_state.position_ids]
        out.position_embeddings = (cos, sin)

        for layer in self.layers:
            out = layer(out)
        return out


@dataclass
class StackedParams:
    qkv_proj: Tensor
    o_proj: Tensor
    attn_ln_weight: Tensor
    mlp_ln_weight: Tensor
    up_proj: Tensor
    gate_proj: Tensor
    down_proj: Tensor


class LlamaForCausalLM(nn.Module):
    def __init__(
        self,
        config: LlamaConfig,
        extra_config: ExtraModelConfig,
    ):
        super().__init__()
        self.config = config
        self.extra_config = extra_config
        self.device = torch.get_default_device()
        self.dtype = torch.get_default_dtype()

        self.model = LlamaModel(config, extra_config)

        self.lm_head = LlamaLMHead(config, extra_config)

    def num_kv_heads(self):
        all_heads = self.config.num_key_value_heads
        tp_size = self.extra_config.tp_size
        assert all_heads % tp_size == 0
        return all_heads // tp_size

    def num_qo_heads(self):
        all_heads = self.config.num_attention_heads
        tp_size = self.extra_config.tp_size
        assert all_heads % tp_size == 0
        return all_heads // tp_size

    def forward(
        self,
        batch_state: BatchState,
    ):
        input_ids = batch_state.input_ids
        if input_ids.ndim == 1:
            input_ids = input_ids.unsqueeze(0)
        position_ids = batch_state.position_ids
        if position_ids.ndim == 1:
            position_ids = position_ids.unsqueeze(0)

        out = BatchState(
            input_ids=input_ids,
            position_ids=position_ids,
            hidden_states=batch_state.hidden_states,
            seq_len=batch_state.seq_len,
        )

        out = self.model(out)
        out = self.lm_head(out)

        return out

    def setup_caches(self):
        k_cache = torch.zeros(
            (
                self.config.num_hidden_layers,
                self.extra_config.max_batch_size,
                self.extra_config.max_len_override
                or self.config.max_position_embeddings,
                self.config.num_key_value_heads,
                self.config.head_dim,
            ),
            device=self.device,
            dtype=self.dtype,
        )
        v_cache = k_cache.clone()

        self.stacked_kv_cache = (k_cache, v_cache)

        for layer_idx in range(self.config.num_hidden_layers):
            layer: LlamaBlock = self.model.layers[layer_idx]  # type: ignore
            layer.self_attn.kv_cache = (
                self.stacked_kv_cache[0][layer_idx],
                self.stacked_kv_cache[1][layer_idx],
            )

    def to(self, device: DeviceType | None = None, dtype: torch.dtype | None = None):  # type: ignore
        if device is not None:
            self.device = device
        if dtype is not None:
            self.dtype = dtype
        return super().to(device=device, dtype=dtype)

    @classmethod
    def from_pretrained(
        cls,
        model_name_or_path: str,
        extra_config: ExtraModelConfig | None = None,
        device: DeviceType | None = None,
        dtype: torch.dtype | None = None,
    ):
        if extra_config is None:
            extra_config = ExtraModelConfig()

        config: LlamaConfig = LlamaConfig.from_pretrained(model_name_or_path)  # type: ignore
        if extra_config.rope_scaling is not None:
            config.rope_scaling = extra_config.rope_scaling

        if dtype is None:
            dtype = config.torch_dtype

        with init_empty_weights(include_buffers=False):
            model = cls(
                config,
                extra_config,
            )
        model.dtype = dtype
        model.device = device

        if (as_path := Path(model_name_or_path)).exists():
            model_path = as_path
        else:
            snapshot_path_str = huggingface_hub.snapshot_download(
                model_name_or_path,
                allow_patterns=["*.safetensors", "*.json"],
            )

            model_path = Path(snapshot_path_str)

        model.load_from_safetensors(model_path)

        # SE (10/18/24): It is important not to call model.to(device, dtype) because
        # this will convert the `inv_freq` buffer in the rotary embeddings to fp16
        # the HF load from pretrained is careful to not do this and keeps it in fp32.
        # The dtype for the parameters is already handled by the load calls above, but
        # it's possible that there are other buffers which *should* be converted to fp16.
        # TODO: it's probably easiest to figure out how we can just use HFs `load_from_pretrained`
        # to load the model weights so we can ensure that there are no other subtle differences
        model.to(device=device)

        model.requires_grad_(False)

        if extra_config.interleave_rope:
            model.model.interleave_rope()

        model.stack_params()
        model.setup_caches()

        return model

    def make_name_to_hf_name(self):
        keys = self.state_dict().keys()

        name_to_hf_name = {k: k for k in keys}

        for layer_idx in range(self.config.num_hidden_layers):
            name_to_hf_name[
                f"model.layers.{layer_idx}.self_attn.input_layernorm.weight"
            ] = f"model.layers.{layer_idx}.input_layernorm.weight"
            name_to_hf_name[f"model.layers.{layer_idx}.mlp.input_layernorm.weight"] = (
                f"model.layers.{layer_idx}.post_attention_layernorm.weight"
            )

        name_to_hf_name["model.embed_tokens.embed_tokens.weight"] = (
            "model.embed_tokens.weight"
        )
        name_to_hf_name["lm_head.input_norm.weight"] = "model.norm.weight"

        if self.config.tie_word_embeddings:
            name_to_hf_name["lm_head.lm_head.weight"] = "model.embed_tokens.weight"
        else:
            name_to_hf_name["lm_head.lm_head.weight"] = "lm_head.weight"

        return name_to_hf_name

    def make_tp_map(self):
        """
        Maps parameter names to the dimension they should be split on.
        Parameters that are not included in the map should not be split.
        """

        tp_map = {}
        for param_name, _ in self.named_parameters():
            if any(
                param_name.endswith(suffix)
                for suffix in [
                    "q_proj.weight",
                    "k_proj.weight",
                    "v_proj.weight",
                    "up_proj.weight",
                    "gate_proj.weight",
                ]
            ):
                tp_map[param_name] = 0

            elif any(
                param_name.endswith(suffix)
                for suffix in ["o_proj.weight", "down_proj.weight"]
            ):
                tp_map[param_name] = 1

        return tp_map

    def load_from_safetensors(
        self,
        model_path: Path,
    ):
        name_to_hf_name = self.make_name_to_hf_name()
        all_hf_names = set(name_to_hf_name.values())

        hf_state_dict = load_safetensors_repo(
            model_path,
            include_parameters=all_hf_names,
            device=self.device,
            tp_rank=self.extra_config.tp_rank,
            tp_size=self.extra_config.tp_size,
            tp_map=self.make_tp_map(),
        )

        state_dict = {k: hf_state_dict[v] for k, v in name_to_hf_name.items()}

        self.load_state_dict(state_dict, assign=True, strict=True)

    def stack_params(self):
        def stack_and_reassign(modules, prop: str):
            params = [getattr(m, prop) for m in modules]
            stacked = torch.stack(params, dim=0)
            for i, m in enumerate(modules):
                getattr(m, prop)[:] = stacked[i]
            return stacked

        layers: list[LlamaBlock] = self.model.layers  # type: ignore
        self_attns = [x.self_attn for x in layers]
        mlps = [x.mlp for x in layers]

        o_projs = [x.o_proj for x in self_attns]
        self_attn_lns = [x.input_layernorm for x in self_attns]

        mlp_lns = [x.input_layernorm for x in mlps]
        up_projs = [x.up_proj for x in mlps]
        gate_projs = [x.gate_proj for x in mlps]
        down_projs = [x.down_proj for x in mlps]

        stacked_o_proj = stack_and_reassign(o_projs, "weight")
        stacked_self_attn_ln_weights = stack_and_reassign(self_attn_lns, "weight")
        stacked_mlp_ln_weights = stack_and_reassign(mlp_lns, "weight")
        stacked_up_proj = stack_and_reassign(up_projs, "weight")
        stacked_gate_proj = stack_and_reassign(gate_projs, "weight")
        stacked_down_proj = stack_and_reassign(down_projs, "weight")

        qkv_weights = []
        for self_attn in self_attns:
            cat_weight = torch.cat(
                [
                    self_attn.q_proj.weight,
                    self_attn.k_proj.weight,
                    self_attn.v_proj.weight,
                ],
                dim=0,
            )
            qkv_weights.append(cat_weight)

        stacked_qkv_weights = torch.stack(qkv_weights, dim=0)

        for i, self_attn in enumerate(self_attns):
            qkv_weight = stacked_qkv_weights[i]
            q_weight, k_weight, v_weight = qkv_weight.split(
                [
                    self.config.num_attention_heads * self.config.head_dim,
                    self.config.num_key_value_heads * self.config.head_dim,
                    self.config.num_key_value_heads * self.config.head_dim,
                ],
                dim=0,
            )

            self_attn.q_proj.weight[:] = q_weight
            self_attn.k_proj.weight[:] = k_weight
            self_attn.v_proj.weight[:] = v_weight

        self.stacked_params = StackedParams(
            qkv_proj=stacked_qkv_weights,
            o_proj=stacked_o_proj,
            attn_ln_weight=stacked_self_attn_ln_weights,
            mlp_ln_weight=stacked_mlp_ln_weights,
            up_proj=stacked_up_proj,
            gate_proj=stacked_gate_proj,
            down_proj=stacked_down_proj,
        )



================================================
FILE: megakernels/mk.py
================================================
import sys
from pathlib import Path


def get_mk_func(mk_dir: Path):
    sys.path.append(str(mk_dir.expanduser().absolute()))
    from mk_llama import mk_llama  # type: ignore

    return mk_llama


class MK_Interpreter:
    def __init__(self, mk_dir: Path):
        self.mk_func = get_mk_func(mk_dir)

    def interpret(self, globs):
        raise NotImplementedError



================================================
FILE: megakernels/model_types.py
================================================
from dataclasses import dataclass
from typing import Any

import torch
import torch.distributed as dist
from torch import Tensor

KV_Cache = tuple[Tensor, Tensor]
DeviceType = torch.device | str


@dataclass
class ModelOutput:
    schedule_id: str
    output_tokens: list[int]
    logprobs: list[float]
    microbatch_index: int | None = None


@dataclass
class BatchState:
    input_ids: Tensor
    position_ids: Tensor | None = None
    seq_len: int | None = None
    output_ids: Tensor | None = None
    hidden_states: Tensor | None = None
    position_embeddings: tuple[Tensor, Tensor] | None = None

    kv_indices: Tensor | None = None
    kv_indptr: Tensor | None = None
    kv_last_page_lens: Tensor | None = None
    kv_seqlens: Tensor | None = None
    qo_indptr: Tensor | None = None
    prefill_wrapper: Any | None = None
    decode_wrapper: Any | None = None

    def __post_init__(self):
        if self.seq_len is None:
            self.seq_len = self.input_ids.shape[1]


@dataclass
class ExtraModelConfig:
    """
    For flags that we define that aren't in the hf config.
    """

    tp_size: int = 1
    tp_rank: int = 0
    tp_group: dist.ProcessGroup | None = None

    torch_compile: bool = False

    rope_scaling: dict | None = None

    interleave_rope: bool = False

    max_len_override: int | None = None

    max_batch_size: int = 1



================================================
FILE: megakernels/python_vm.py
================================================
import torch
from einops import einsum
from torch import Tensor

from megakernels.instructions import BaseGlobals, Instruction, PrintState
from megakernels.utils import trepr


def get_start_end(block_size: int, block_idx: int):
    start = block_size * block_idx
    end = start + block_size
    return start, end


def matvec(
    mat: Tensor,
    vec: Tensor,
    block_size: int,
    block_idx: int,
    reduce: bool = False,
    reduction_size: int = 0,
    reduction_idx: int = 0,
):
    start, end = get_start_end(block_size, block_idx)
    if reduce:
        red_start, red_end = get_start_end(reduction_size, reduction_idx)
        mat = mat[start:end, red_start:red_end]
        vec = vec[red_start:red_end]
    else:
        mat = mat[start:end]

    out = einsum(mat, vec, "o i, i -> o")
    return out, start, end


def rms_norm(inp: Tensor, weight: Tensor, eps: float):
    input_dtype = inp.dtype
    inp = inp.to(torch.float32)
    variance = inp.pow(2).mean(-1, keepdim=True)
    inp = inp * torch.rsqrt(variance + eps)

    return weight * inp.to(input_dtype)


def matvec_with_residual(
    mat: Tensor,
    vec: Tensor,
    residual: Tensor,
    block_size: int,
    start_block_idx: int,
    end_block_idx: int,
    reduction_size: int,
    reduction_block_idx: int,
):
    for block_idx in range(start_block_idx, end_block_idx):
        matvec_out, start, end = matvec(
            mat=mat,
            vec=vec,
            block_size=block_size,
            block_idx=block_idx,
            reduce=True,
            reduction_size=reduction_size,
            reduction_idx=reduction_block_idx,
        )

        residual[start:end] += matvec_out.to(residual.dtype)


def print_state(globals: BaseGlobals, instruction: PrintState):
    print_info = instruction.print_info
    if (
        print_info.layer_filter is None
        or instruction.layer_idx in print_info.layer_filter
    ) and (
        print_info.name_filter is None or instruction.name in print_info.name_filter
    ):
        print(f"State at layer={instruction.layer_idx}, op={instruction.name}")
        for state in print_info.state_filter:
            attr = getattr(globals, state)
            print(f"{state}: {trepr(attr) if isinstance(attr, Tensor) else attr}")


def interpret_with_pyvm(
    globals: BaseGlobals, instructions: list[Instruction], instruction_to_solver: dict
):
    for instruction in instructions:
        instruction_to_solver[type(instruction)](globals, instruction)


class PyVM_Interpreter:
    def __init__(self, instruction_to_solver: dict):
        self.instruction_to_solver = instruction_to_solver

    def interpret(self, globs: BaseGlobals, instructions: list[Instruction]):
        interpret_with_pyvm(globs, instructions, self.instruction_to_solver)



================================================
FILE: megakernels/scheduler.py
================================================
import heapq
from dataclasses import dataclass, field, replace

import torch

from megakernels.instructions import (
    BaseGlobals,
    Instruction,
    NoOp,
)
from megakernels.llama import LlamaForCausalLM

INTS_PER_INSTRUCTION = 32
TIMING_SLOTS = 128


@dataclass
class DAG_Node:
    def __hash__(self):
        return hash(tuple(self.instruction.serialize()))

    instruction: Instruction
    dependencies: list["DAG_Node"]

    children: set["DAG_Node"] = field(default_factory=set)
    start_time: float = float("inf")
    end_time: float = float("inf")
    remaining_dependencies: set["DAG_Node"] = field(default_factory=set)
    priority: float = 0

    def earliest_ready_time(self, globs: BaseGlobals):
        if len(self.dependencies) == 0:
            return 0

        return max(dep.end_time for dep in self.dependencies)

    def register_with_parents(self):
        for dep in self.dependencies:
            dep.children.add(self)

    def calc_priority(self, globs: BaseGlobals):
        cur_cost = self.priority
        for dep in self.dependencies:
            pri = cur_cost + dep.instruction.cost(globs)
            dep.priority = max(pri, dep.priority)
            dep.calc_priority(globs)


@dataclass
class Schedule:
    globs: BaseGlobals
    dag_nodes: list[DAG_Node]
    end_node: DAG_Node

    def get_linear_instructions(self):
        # NOTE: assumes this is in topological order
        return [node.instruction for node in self.dag_nodes]

    def smart_assign_to_sms(self):
        return assign_dag_to_sms(self)

    def round_robin_assign_to_sms(self):
        instructions = self.get_linear_instructions()
        return round_robin_assign_to_sms(instructions, self.globs.sm_count())


class ScheduleBuilder:
    @classmethod
    def make_globals(cls, model):
        raise NotImplementedError

    @classmethod
    def make_dag(
        cls, globs, stop_after_op: str | None = None, layer_limit: int | None = None
    ):
        raise NotImplementedError

    @classmethod
    def build(
        cls,
        model: LlamaForCausalLM,
        stop_after_op: str | None = None,
        layer_limit: int | None = None,
    ):
        globs = cls.make_globals(model)
        dag_nodes, end_node = cls.make_dag(globs, stop_after_op, layer_limit)
        return Schedule(globs, dag_nodes, end_node)

    @classmethod
    def with_new_globals(cls, schedule: Schedule, model: LlamaForCausalLM):
        return replace(schedule, globs=cls.make_globals(model))


def assign_dag_to_sms(schedule: Schedule) -> list[list[Instruction]]:
    nodes = schedule.dag_nodes
    globs = schedule.globs

    for node in nodes:
        node.register_with_parents()

    for node in nodes:
        node.remaining_dependencies = set(node.dependencies)

    # schedule.end_node.calc_priority(globs)

    sm_count = globs.sm_count()
    sm_queues = [[] for _ in range(sm_count)]

    sm_heap = [(0, i) for i in range(sm_count)]
    heapq.heapify(sm_heap)

    ready_nodes = [n for n in nodes if len(n.dependencies) == 0]

    idx_to_node = {i: n for i, n in enumerate(nodes)}
    node_to_idx = {n: i for i, n in enumerate(nodes)}

    ready_heap = []
    for node in ready_nodes:
        idx = node_to_idx[node]
        # max cost first
        ready_heap.append((-node.instruction.cost(globs), idx))

    heapq.heapify(ready_heap)

    while ready_heap:
        ready_time, idx = heapq.heappop(ready_heap)
        node = idx_to_node[idx]

        sm_time, sm_idx = heapq.heappop(sm_heap)

        # print(f"assigning instruction {node.instruction} to sm {sm_idx}")

        # start_time = max(ready_time, sm_time)
        start_time = sm_time

        end_time = start_time + node.instruction.cost(globs)

        node.start_time = start_time
        node.end_time = end_time

        sm_queues[sm_idx].append(node.instruction)

        heapq.heappush(sm_heap, (end_time, sm_idx))

        for child in node.children:
            child.remaining_dependencies.remove(node)
            if len(child.remaining_dependencies) == 0:
                idx = node_to_idx[child]
                heapq.heappush(ready_heap, (-child.instruction.cost(globs), idx))

    return sm_queues


def round_robin_assign_to_sms(
    instructions: list[Instruction], sm_count: int
) -> list[list[Instruction]]:
    sm_queues = [[] for _ in range(sm_count)]
    for i, instruction in enumerate(instructions):
        sm_queues[i % sm_count].append(instruction)

    return sm_queues


def zig_zag_assign_to_sms(
    instructions: list[Instruction], sm_count: int
) -> list[list[Instruction]]:
    sm_queues = [[] for _ in range(sm_count)]
    for i, instruction in enumerate(instructions):
        base_id = i % (sm_count * 2)
        if base_id < sm_count:
            sm_queues[base_id].append(instruction)
        else:
            sm_queues[sm_count - 1 - (base_id - sm_count)].append(instruction)

    return sm_queues


def collect_into_waves(instructions: list[Instruction]):
    waves: list[list[Instruction]] = []
    cur = []
    for instruction in instructions:
        if cur == [] or cur[-1].opcode() == instruction.opcode():
            cur.append(instruction)
        else:
            waves.append(cur)
            cur = [instruction]

    if len(cur) > 0:
        waves.append(cur)

    return waves


def wave_assign_to_sms(
    schedule: Schedule,
) -> list[list[Instruction]]:
    instructions = schedule.get_linear_instructions()
    globs = schedule.globs
    sm_count = globs.sm_count()

    waves = collect_into_waves(instructions)

    sm_queues = [[] for _ in range(sm_count)]

    sm_heap = [(0, i) for i in range(sm_count)]
    heapq.heapify(sm_heap)

    for wave in waves:
        sorted_by_biggest_cost = sorted(wave, key=lambda x: x.cost(globs), reverse=True)

        for ins in sorted_by_biggest_cost:
            sm_cost, sm_idx = heapq.heappop(sm_heap)
            sm_cost += ins.cost(globs)
            heapq.heappush(sm_heap, (sm_cost, sm_idx))
            sm_queues[sm_idx].append(ins)

    return sm_queues


def pool_assign_to_sms(
    instructions: list[Instruction], sm_count: int, memory_fraction: float
) -> list[list[Instruction]]:
    memory_instructions = []
    compute_instructions = []

    for ins in instructions:
        pool = ins.tags()["pool"]
        match pool:
            case "memory":
                memory_instructions.append(ins)
            case "compute":
                compute_instructions.append(ins)
            case _:
                raise ValueError(f"Unknown pool: {pool}")

    mem_sms = round(sm_count * memory_fraction)
    compute_sms = sm_count - mem_sms

    memory_queues = round_robin_assign_to_sms(memory_instructions, mem_sms)
    compute_queues = round_robin_assign_to_sms(compute_instructions, compute_sms)

    return memory_queues + compute_queues


def assign_to_sms(
    mode: str,
    schedule: Schedule | None = None,
    instructions: list[Instruction] | None = None,
    sm_count: int | None = None,
    memory_fraction: float | None = None,
):
    if schedule is not None:
        instructions = schedule.get_linear_instructions()
        sm_count = schedule.globs.sm_count()

    match mode:
        case "rr":
            return round_robin_assign_to_sms(instructions, sm_count)
        case "zz":
            return zig_zag_assign_to_sms(instructions, sm_count)
        case "wave":
            return wave_assign_to_sms(schedule)
        case "dag":
            return assign_dag_to_sms(schedule)
        case "pool":
            assert memory_fraction is not None
            return pool_assign_to_sms(
                instructions, sm_count, memory_fraction=memory_fraction
            )
        case _:
            raise ValueError(f"Unknown mode: {mode}")


def serialize_and_pad(instruction: Instruction):
    serialized = instruction.serialize()
    num_padding = INTS_PER_INSTRUCTION - len(serialized)
    assert num_padding >= 0
    return serialized + [0] * num_padding


def tensorize_instructions(
    globs: BaseGlobals,
    instruction_queues: list[list[Instruction]],
):
    num_sms = globs.sm_count()

    max_queue_len = max(len(queue) for queue in instruction_queues)
    for queue in instruction_queues:
        queue.extend([NoOp()] * (max_queue_len - len(queue)))

    flattened = []
    for queue in instruction_queues:
        flattened.extend(serialize_and_pad(instruction) for instruction in queue)

    device = globs.device

    serialized = torch.tensor(flattened, dtype=torch.int32, device=device).view(
        num_sms, -1, INTS_PER_INSTRUCTION
    )

    timings = torch.zeros(
        [num_sms, max_queue_len, TIMING_SLOTS],
        dtype=torch.int32,
        device=device,
    )

    globs.instructions = serialized
    globs.timings = timings



================================================
FILE: megakernels/utils.py
================================================
import json
from pathlib import Path
from typing import List

import torch
from megakernels.model_types import DeviceType
from safetensors import safe_open
from torch import Tensor
from tqdm import tqdm


def assert_div(a, b):
    assert a % b == 0, f"{a} is not divisible by {b}"
    return a // b


def compute_shard_bounds(
    tensor_shape: List[int], dim: int, num_shards: int, shard_index: int
):
    dim_size = tensor_shape[dim]
    base_shard_size = dim_size // num_shards
    remainder = dim_size % num_shards

    start_idx = shard_index * base_shard_size + min(shard_index, remainder)

    if shard_index < remainder:
        end_idx = start_idx + base_shard_size + 1
    else:
        end_idx = start_idx + base_shard_size

    return slice(start_idx, end_idx)


def load_safetensors_repo(
    repo_path: Path,
    include_parameters: set[str],
    device: DeviceType,
    tp_rank: int = 0,
    tp_size: int = 1,
    tp_map: dict[str, int] | None = None,
):
    if tp_map is None:
        tp_map = {}

    single_file = repo_path / "model.safetensors"
    if single_file.exists():
        files_to_load = [single_file]

    else:
        safetensors_index = repo_path / "model.safetensors.index.json"

        if not safetensors_index.exists():
            raise FileNotFoundError(
                f"Could not find model.safetensors or model.safetensors.index.json in {repo_path}"
            )

        with open(safetensors_index, "r") as f:
            index = json.load(f)

        param_to_path = index["weight_map"]

        files_to_load_set = set()

        for param_name, path in param_to_path.items():
            if param_name in include_parameters:
                files_to_load_set.add(repo_path / path)

        files_to_load = list(sorted(files_to_load_set))

    state_dict = {}

    for file in tqdm(
        files_to_load,
        desc="Loading safetensors files",
    ):
        with safe_open(file, framework="pt", device=device) as f:
            for k in f.keys():
                if k in include_parameters:
                    if tp_size > 1 and (split_dim := tp_map.get(k)) is not None:
                        tensor_slice = f.get_slice(k)
                        shard_bounds = compute_shard_bounds(
                            tensor_slice.get_shape(), split_dim, tp_size, tp_rank
                        )
                        # TODO: there's gotta be a better way to do this
                        match split_dim:
                            case 0:
                                state_dict[k] = tensor_slice[shard_bounds]
                            case 1:
                                state_dict[k] = tensor_slice[:, shard_bounds]
                            case _:
                                raise ValueError(
                                    f"Unsupported split dimension: {split_dim}"
                                )
                    else:
                        state_dict[k] = f.get_tensor(k)

    return state_dict


def trepr(t: Tensor):
    """
    Tensor representation.
    """
    return f"shape={t.shape}, sum={t.sum()}, vals={t}"


def get_sm_count(device: str) -> int:
    device_props = torch.cuda.get_device_properties(device)
    return device_props.multi_processor_count



================================================
FILE: megakernels/demos/latency/instructions.py
================================================
from dataclasses import dataclass
from typing import Optional

from torch import Tensor

from megakernels.instructions import BaseGlobals, Instruction


@dataclass
class Globals(BaseGlobals):
    # activation buffers
    post_ln_rope_q: Tensor
    attn_out: Tensor
    attn_lse_intermediates: Tensor
    attn_out_intermediates: Tensor
    silu_out: Tensor
    logits: Tensor

    skip_attn_reduction: bool

    # block size constants
    up_gate_proj_block_size: int
    down_proj_block_size: int
    o_proj_block_size: int
    lm_head_block_size: int
    matvec_reduction_size: int
    qkv_block_size: int
    attn_kv_block_size: int
    attn_reduction_size: int


@dataclass
class LayerNorm_QKV_MatVecRopeAppend(Instruction):
    """
    attention: layernorm + qkv matvec + rope on q and k + append k and v to kv cache
    """

    layer_idx: int
    start_output_block_idx: int
    end_output_block_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 1

    @classmethod
    def prev_opcode(cls) -> int:
        return DownProjResidual.opcode()

    def block_indices(self):
        return list(range(self.start_output_block_idx, self.end_output_block_idx))

    def cost(self, globs: Globals):
        return (
            (self.end_output_block_idx - self.start_output_block_idx)
            * globs.qkv_block_size
            * globs.hidden_size
        )


@dataclass
class PartialAttention(Instruction):
    layer_idx: int
    kv_head_idx: int
    num_partials: int
    partial_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 2

    @classmethod
    def prev_opcode(cls) -> int:
        return LayerNorm_QKV_MatVecRopeAppend.opcode()

    def cost(self, globs: Globals):
        seq_len = globs.pos_id + 1
        loaded_seq_len = seq_len / self.num_partials

        # num loaded elements from kv cache
        return loaded_seq_len * globs.head_dim * 2


@dataclass
class AttentionReduction(Instruction):
    layer_idx: int
    head_start_idx: int
    # the original number of attention partitions
    num_partials: int
    is_terminal: bool
    # TODO: make sure reduction_list can't go beyond instruction
    reduction_list: list[int]
    # Not required for the last reduction
    output_partial_idx: Optional[int] = None

    @classmethod
    def opcode(cls) -> int:
        return 3

    @classmethod
    def prev_opcode(cls) -> int:
        return PartialAttention.opcode()


@dataclass
class MatVecAdd(Instruction):
    layer_idx: int
    start_block_idx: int
    end_block_idx: int
    reduction_block_idx: int


# denoting these with separate opcodes so that know what inputs to read from


@dataclass
class O_ProjResidual(MatVecAdd):
    @classmethod
    def opcode(cls) -> int:
        return 4

    @classmethod
    def prev_opcode(cls) -> int:
        return AttentionReduction.opcode()

    def cost(self, globs: Globals):
        return (
            (self.end_block_idx - self.start_block_idx)
            * globs.o_proj_block_size
            * globs.hidden_size
        )


@dataclass
class LayerNormDoubleMatVecSiLU(Instruction):
    """
    layernorm + double matvec + silu
    """

    layer_idx: int
    block_idxs: list[int]

    @classmethod
    def opcode(cls) -> int:
        return 5

    @classmethod
    def prev_opcode(cls) -> int:
        return O_ProjResidual.opcode()

    def cost(self, globs: Globals):
        return (
            len(self.block_idxs)
            * globs.up_gate_proj_block_size
            * globs.hidden_size
            * 2  # gate and up
        )


@dataclass
class DownProjResidual(MatVecAdd):
    @classmethod
    def opcode(cls) -> int:
        return 6

    @classmethod
    def prev_opcode(cls) -> int:
        return LayerNormDoubleMatVecSiLU.opcode()

    def cost(self, globs: Globals):
        return (
            (self.end_block_idx - self.start_block_idx)
            * globs.down_proj_block_size
            * globs.hidden_size
        )


@dataclass
class RMS_LM_Head(Instruction):
    start_output_block_idx: int
    end_output_block_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 7

    @classmethod
    def prev_opcode(cls) -> int:
        return DownProjResidual.opcode()

    def cost(self, globs: Globals):
        return (
            (self.end_output_block_idx - self.start_output_block_idx)
            * globs.lm_head_block_size
            * globs.hidden_size
        )



================================================
FILE: megakernels/demos/latency/mk.py
================================================
import torch
from einops import rearrange

from megakernels.demos.latency.instructions import Globals
from megakernels.mk import MK_Interpreter


def interpret_with_mk(
    globs: Globals,
    mk_func,
):
    fourD_k_cache = rearrange(globs.k_cache, "l b t h d -> (l b) t h d")
    fourD_v_cache = rearrange(globs.v_cache, "l b t h d -> (l b) t h d")

    mk_func(
        # vm stuff
        globs.barriers,
        globs.instructions,
        globs.timings,
        # weights
        globs.qkv_proj_weights,
        globs.attn_ln_weights,
        globs.o_proj_weights,
        globs.mlp_ln_weights,
        globs.up_proj_weights,
        globs.gate_proj_weights,
        globs.down_proj_weights,
        globs.lm_head_norm_weights.data,
        globs.lm_head_weights.data,
        fourD_k_cache,
        fourD_v_cache,
        # rope
        globs.rope_cos,
        globs.rope_sin,
        # activations
        globs.hidden_states,
        globs.post_ln_rope_q,
        globs.attn_out,
        globs.attn_lse_intermediates,
        globs.attn_out_intermediates,
        globs.silu_out,
        globs.logits,
        # scalars
        globs.pos_id,
        globs.attn_scale,
        globs.rms_norm_eps,
        globs.skip_attn_reduction,
        stream=torch.cuda.current_stream(),
    )


class LatencyMK_Interpreter(MK_Interpreter):
    def interpret(self, globs: Globals):
        interpret_with_mk(globs, self.mk_func)



================================================
FILE: megakernels/demos/latency/python_vm.py
================================================
import math

import torch
import torch.nn.functional as F
from einops import einsum
from torch import Tensor

from megakernels.demos.latency.instructions import (
    AttentionReduction,
    DownProjResidual,
    Globals,
    LayerNorm_QKV_MatVecRopeAppend,
    LayerNormDoubleMatVecSiLU,
    O_ProjResidual,
    PartialAttention,
    RMS_LM_Head,
)
from megakernels.llama import (
    apply_rotary_pos_emb_interleaved,
)


def get_start_end(block_size: int, block_idx: int):
    start = block_size * block_idx
    end = start + block_size
    return start, end


def matvec(
    mat: Tensor,
    vec: Tensor,
    block_size: int,
    block_idx: int,
    reduce: bool = False,
    reduction_size: int = 0,
    reduction_idx: int = 0,
):
    start, end = get_start_end(block_size, block_idx)
    if reduce:
        red_start, red_end = get_start_end(reduction_size, reduction_idx)
        mat = mat[start:end, red_start:red_end]
        vec = vec[red_start:red_end]
    else:
        mat = mat[start:end]

    out = einsum(mat, vec, "o i, i -> o")
    return out, start, end


def rms_norm(inp: Tensor, weight: Tensor, eps: float):
    input_dtype = inp.dtype
    inp = inp.to(torch.float32)
    variance = inp.pow(2).mean(-1, keepdim=True)
    inp = inp * torch.rsqrt(variance + eps)

    return weight * inp.to(input_dtype)


def matvec_with_residual(
    mat: Tensor,
    vec: Tensor,
    residual: Tensor,
    block_size: int,
    start_block_idx: int,
    end_block_idx: int,
    reduction_size: int,
    reduction_block_idx: int,
):
    for block_idx in range(start_block_idx, end_block_idx):
        matvec_out, start, end = matvec(
            mat=mat,
            vec=vec,
            block_size=block_size,
            block_idx=block_idx,
            reduce=True,
            reduction_size=reduction_size,
            reduction_idx=reduction_block_idx,
        )

        residual[start:end] += matvec_out.to(residual.dtype)


def o_proj_residual(globals: Globals, instruction: O_ProjResidual):
    # Barrier check
    op_barriers = globals.barriers[instruction.layer_idx, instruction.prev_opcode() - 1]
    assert op_barriers[0] == globals.num_attention_heads

    assert instruction.start_block_idx == instruction.end_block_idx - 1
    assert instruction.reduction_block_idx == 0

    matvec_with_residual(
        mat=globals.o_proj_weights[instruction.layer_idx],
        vec=globals.attn_out,
        residual=globals.hidden_states,
        block_size=globals.o_proj_block_size,
        start_block_idx=instruction.start_block_idx,
        end_block_idx=instruction.end_block_idx,
        reduction_size=globals.matvec_reduction_size,
        reduction_block_idx=instruction.reduction_block_idx,
    )

    # Barrier update
    next_op_barriers = globals.barriers[instruction.layer_idx, instruction.opcode() - 1]
    next_op_barriers[0] += instruction.end_block_idx - instruction.start_block_idx


def down_proj_residual(globals: Globals, instruction: DownProjResidual):
    # Barrier check
    op_barriers = globals.barriers[instruction.layer_idx, instruction.prev_opcode() - 1]
    assert op_barriers[0] == 512  # 8192 / 16

    matvec_with_residual(
        mat=globals.down_proj_weights[instruction.layer_idx],
        vec=globals.silu_out,
        residual=globals.hidden_states,
        block_size=globals.down_proj_block_size,
        start_block_idx=instruction.start_block_idx,
        end_block_idx=instruction.end_block_idx,
        reduction_size=globals.matvec_reduction_size,
        reduction_block_idx=instruction.reduction_block_idx,
    )

    next_op_barriers = globals.barriers[instruction.layer_idx, instruction.opcode() - 1]
    next_op_barriers[0] += instruction.end_block_idx - instruction.start_block_idx


def layer_norm_double_matvec_silu(
    globals: Globals, instruction: LayerNormDoubleMatVecSiLU
):
    # Barrier check
    op_barriers = globals.barriers[instruction.layer_idx, instruction.prev_opcode() - 1]
    assert op_barriers[0] == 128

    post_ln = rms_norm(
        inp=globals.hidden_states,
        weight=globals.mlp_ln_weights[instruction.layer_idx],
        eps=globals.rms_norm_eps,
    )

    block_size = globals.up_gate_proj_block_size

    barriers = globals.barriers[instruction.layer_idx, instruction.opcode() - 1]

    for block_idx in instruction.block_idxs:
        start, end = get_start_end(block_size, block_idx)

        up_matvec, start, end = matvec(
            mat=globals.up_proj_weights[instruction.layer_idx],
            vec=post_ln,
            block_size=block_size,
            block_idx=block_idx,
        )

        gate_matvec, _, _ = matvec(
            mat=globals.gate_proj_weights[instruction.layer_idx],
            vec=post_ln,
            block_size=block_size,
            block_idx=block_idx,
        )

        post_silu = F.silu(gate_matvec) * up_matvec

        globals.silu_out[start:end] = post_silu

        barriers[0] += 1


def layer_norm_matvec_rope_append(
    globals: Globals, instruction: LayerNorm_QKV_MatVecRopeAppend
):
    layer_idx = instruction.layer_idx

    # Barrier check
    if layer_idx > 0:
        op_barriers = globals.barriers[layer_idx - 1, instruction.prev_opcode() - 1]
        assert op_barriers[0] == 512

    post_ln = rms_norm(
        inp=globals.hidden_states,
        weight=globals.attn_ln_weights[layer_idx],
        eps=globals.rms_norm_eps,
    )

    pos_id = globals.pos_id

    k_start = globals.num_attention_heads * globals.head_dim
    v_start = k_start + globals.num_kv_heads * globals.head_dim

    barriers = globals.barriers[instruction.layer_idx, instruction.opcode() - 1]

    for block_idx in range(
        instruction.start_output_block_idx, instruction.end_output_block_idx
    ):
        start, end = get_start_end(globals.qkv_block_size, block_idx)

        if start < k_start:
            mode = "q"
        elif start < v_start:
            mode = "k"
        else:
            mode = "v"

        matmul_output = einsum(
            globals.qkv_proj_weights[layer_idx][start:end],
            post_ln,
            "o i, i -> o",
        )

        out = matmul_output

        if mode in ["q", "k"]:
            full_head = torch.zeros(
                1,
                globals.head_dim,
                device=globals.hidden_states.device,
                dtype=out.dtype,
            )
            head_segment = start % globals.head_dim
            full_head_start = head_segment
            full_head_end = full_head_start + (end - start)

            full_head[:, full_head_start:full_head_end] = out
            full_head_with_rope, _ = apply_rotary_pos_emb_interleaved(
                q=full_head,
                k=full_head,
                cos=globals.rope_cos[pos_id],
                sin=globals.rope_sin[pos_id],
                unsqueeze_dim=0,
            )
            out = full_head_with_rope[:, full_head_start:full_head_end].view(-1)

        match mode:
            case "q":
                globals.post_ln_rope_q[start:end] = out
            case "k":
                start_in_k = start - k_start
                end_in_k = end - k_start
                globals.k_cache[layer_idx, :, pos_id].view(-1)[start_in_k:end_in_k] = (
                    out
                )
            case "v":
                start_in_v = start - v_start
                end_in_v = end - v_start
                globals.v_cache[layer_idx, :, pos_id].view(-1)[start_in_v:end_in_v] = (
                    out
                )

        barriers[block_idx // 4] += 1


def rms_lm_head(globals: Globals, instruction: RMS_LM_Head):
    op_barriers = globals.barriers[
        globals.num_hidden_layers - 1, instruction.prev_opcode() - 1
    ]
    assert op_barriers[0] == 512

    post_ln = rms_norm(
        inp=globals.hidden_states,
        weight=globals.lm_head_norm_weights,
        eps=globals.rms_norm_eps,
    )

    for block_idx in range(
        instruction.start_output_block_idx, instruction.end_output_block_idx
    ):
        start, end = get_start_end(globals.lm_head_block_size, block_idx)

        matmul_output = einsum(
            globals.lm_head_weights[start:end],
            post_ln,
            "o i, i -> o",
        )

        globals.logits[start:end] = matmul_output


def partial_attention(globals: Globals, instruction: PartialAttention):
    gqa_ratio = globals.num_attention_heads // globals.num_kv_heads

    # Barrier check
    op_barriers = globals.barriers[instruction.layer_idx, instruction.prev_opcode() - 1]
    for i in range(gqa_ratio):
        assert op_barriers[instruction.kv_head_idx * gqa_ratio + i] == 4
    assert op_barriers[globals.num_attention_heads + instruction.kv_head_idx] == 4
    assert (
        op_barriers[
            globals.num_attention_heads + globals.num_kv_heads + instruction.kv_head_idx
        ]
        == 4
    )

    kv_block_size = globals.attn_kv_block_size
    seq_len = globals.pos_id + 1
    layer_idx = instruction.layer_idx
    kv_head_idx = instruction.kv_head_idx

    total_blocks = math.ceil(seq_len / kv_block_size)
    blocks_per_partial = math.ceil(total_blocks / instruction.num_partials)

    start_block = instruction.partial_idx * blocks_per_partial
    end_block = min(start_block + blocks_per_partial, total_blocks)

    start_token = start_block * kv_block_size
    end_token = min(end_block * kv_block_size, seq_len)

    k = globals.k_cache[layer_idx, 0, start_token:end_token, kv_head_idx]
    v = globals.v_cache[layer_idx, 0, start_token:end_token, kv_head_idx]

    head_start = kv_head_idx * gqa_ratio
    head_end = head_start + gqa_ratio

    q = globals.post_ln_rope_q.view(globals.num_attention_heads, -1)[
        head_start:head_end
    ]

    qk = einsum(q.float(), k.float(), "h i, k i -> h k")
    scaled_qk = qk * globals.attn_scale

    # casting the output of the softmax to 16-bit causes small numerical differences
    softmax = torch.softmax(scaled_qk, dim=-1)

    # lse = torch.logsumexp(scaled_qk, dim=-1)
    lse = torch.log2(torch.sum(torch.exp(scaled_qk), dim=-1))

    out = einsum(softmax.float(), v.float(), "h k, k o -> h o")

    if globals.skip_attn_reduction:
        globals.attn_out.view(globals.num_attention_heads, -1)[
            head_start:head_end, :
        ] = out
        barriers = globals.barriers[
            instruction.layer_idx, AttentionReduction.opcode() - 1
        ]
        barriers[0] += head_end - head_start

    else:
        globals.attn_lse_intermediates[head_start:head_end, instruction.partial_idx] = (
            lse
        )
        globals.attn_out_intermediates[head_start:head_end, instruction.partial_idx] = (
            out
        )

        # Barrier update
        barriers = globals.barriers[instruction.layer_idx, instruction.opcode() - 1]
        barriers[head_start:head_end] += 1


def attention_reduction(globals: Globals, instruction: AttentionReduction):
    head_start_idx = instruction.head_start_idx

    # Barrier check
    op_barriers = globals.barriers[instruction.layer_idx, instruction.prev_opcode() - 1]
    assert op_barriers[head_start_idx] == instruction.num_partials

    indices_to_reduce = torch.tensor(
        instruction.reduction_list,
        dtype=torch.long,
        device=globals.hidden_states.device,
    )

    lses = globals.attn_lse_intermediates[
        head_start_idx : head_start_idx + globals.attn_reduction_size, indices_to_reduce
    ]
    outs = globals.attn_out_intermediates[
        head_start_idx : head_start_idx + globals.attn_reduction_size, indices_to_reduce
    ]

    max_lse = torch.max(lses, dim=-1, keepdim=True).values

    # adjusted_factors = (lses - max_lse).exp()
    adjusted_factors = (lses - max_lse).exp2()
    new_denominator = adjusted_factors.sum(dim=-1, keepdim=True)

    reduced = (outs * adjusted_factors.unsqueeze(-1)).sum(dim=1) / new_denominator

    if instruction.is_terminal:
        globals.attn_out.view(globals.num_attention_heads, -1)[
            head_start_idx : head_start_idx + globals.attn_reduction_size
        ] = reduced
    else:
        new_lse = new_denominator.log()
        output_slot = instruction.output_partial_idx
        globals.attn_lse_intermediates[
            head_start_idx : head_start_idx + globals.attn_reduction_size, output_slot
        ] = new_lse
        globals.attn_out_intermediates[
            head_start_idx : head_start_idx + globals.attn_reduction_size, output_slot
        ] = reduced

    # Barrier update
    next_op_barriers = globals.barriers[instruction.layer_idx, instruction.opcode() - 1]
    next_op_barriers[0] += globals.attn_reduction_size  # the dumb way


INSTRUCTION_TO_SOLVER = {
    O_ProjResidual: o_proj_residual,
    DownProjResidual: down_proj_residual,
    LayerNormDoubleMatVecSiLU: layer_norm_double_matvec_silu,
    LayerNorm_QKV_MatVecRopeAppend: layer_norm_matvec_rope_append,
    RMS_LM_Head: rms_lm_head,
    PartialAttention: partial_attention,
    AttentionReduction: attention_reduction,
}



================================================
FILE: megakernels/demos/latency/scheduler.py
================================================
import math

import torch

from megakernels.demos.latency.instructions import (
    DownProjResidual,
    Globals,
    Instruction,
    LayerNorm_QKV_MatVecRopeAppend,
    LayerNormDoubleMatVecSiLU,
    O_ProjResidual,
    PartialAttention,
    RMS_LM_Head,
)
from megakernels.instructions import NoOp
from megakernels.llama import LlamaForCausalLM
from megakernels.scheduler import DAG_Node, ScheduleBuilder
from megakernels.utils import assert_div, get_sm_count


def pick_num_attention_partitions(prompt_len: int, ntok: int, device: torch.device):
    min_chunk_size = 256
    full_len = prompt_len + ntok

    num_divisions = math.ceil(full_len / min_chunk_size)

    # TODO limitation until we have a better reduction tree
    num_attention_partitions = min(num_divisions, 24)

    # sm_count = get_sm_count(device)
    # num_attention_partitions = min(sm_count, num_divisions)

    assert num_attention_partitions >= 1

    return num_attention_partitions


def make_globals(
    model: LlamaForCausalLM,
    skip_attn_reduction: bool = True,
):
    config = model.config
    device = model.device
    dtype = model.dtype

    def make_buffer(shape, buffer_dtype=dtype):
        return torch.zeros(shape, device=device, dtype=buffer_dtype)

    stacked_params = model.stacked_params

    max_attn_partitions = get_sm_count(device)

    barriers = torch.zeros(
        [
            config.num_hidden_layers,
            10,  # more than the number of opcodes we have
            config.num_attention_heads + config.num_key_value_heads * 2,
        ],
        dtype=torch.int32,
        device=device,
    )

    return Globals(
        # model params
        qkv_proj_weights=stacked_params.qkv_proj,
        o_proj_weights=stacked_params.o_proj,
        attn_ln_weights=stacked_params.attn_ln_weight,
        mlp_ln_weights=stacked_params.mlp_ln_weight,
        up_proj_weights=stacked_params.up_proj,
        gate_proj_weights=stacked_params.gate_proj,
        down_proj_weights=stacked_params.down_proj,
        lm_head_norm_weights=model.lm_head.input_norm.weight,
        lm_head_weights=model.lm_head.lm_head.weight,
        k_cache=model.stacked_kv_cache[0],
        v_cache=model.stacked_kv_cache[1],
        rope_cos=model.model.rope_cos,
        rope_sin=model.model.rope_sin,
        # activation buffers
        hidden_states=make_buffer(config.hidden_size),
        post_ln_rope_q=make_buffer(config.hidden_size),
        attn_out=make_buffer(config.hidden_size),
        attn_out_intermediates=make_buffer(
            [config.num_attention_heads, max_attn_partitions, config.head_dim],
            buffer_dtype=torch.float32,
        ),
        attn_lse_intermediates=make_buffer(
            [config.num_attention_heads, max_attn_partitions],
            buffer_dtype=torch.float32,
        ),
        silu_out=make_buffer(config.intermediate_size),
        logits=make_buffer(config.vocab_size),
        # scalars
        pos_id=0,
        attn_scale=1 / math.sqrt(config.head_dim),
        rms_norm_eps=config.rms_norm_eps,
        skip_attn_reduction=skip_attn_reduction,
        num_hidden_layers=config.num_hidden_layers,
        num_attention_heads=config.num_attention_heads,
        num_kv_heads=config.num_key_value_heads,
        head_dim=config.head_dim,
        hidden_size=config.hidden_size,
        intermediate_size=config.intermediate_size,
        # block sizes
        up_gate_proj_block_size=16,
        down_proj_block_size=16,
        qkv_block_size=16,
        o_proj_block_size=16,
        lm_head_block_size=16,
        matvec_reduction_size=2048,
        attn_kv_block_size=16,
        attn_reduction_size=4,
        vocab_size=config.vocab_size,
        device=device,
        barriers=barriers,
    )


def schedule_qkv(
    globs: Globals, layer_idx: int
) -> list[LayerNorm_QKV_MatVecRopeAppend]:
    instructions = []

    qkv_outdim = (globs.num_attention_heads + 2 * globs.num_kv_heads) * globs.head_dim

    num_qkv_blocks = assert_div(qkv_outdim, globs.qkv_block_size)
    sm_count = globs.sm_count()

    blocks_per_sm = num_qkv_blocks / sm_count
    assert blocks_per_sm > 1

    for sm_idx in range(sm_count):
        start = round(sm_idx * blocks_per_sm)
        end = round((sm_idx + 1) * blocks_per_sm)
        instructions.append(
            LayerNorm_QKV_MatVecRopeAppend(
                layer_idx=layer_idx,
                start_output_block_idx=start,
                end_output_block_idx=end,
            )
        )

    return instructions


def schedule_upgate(globs: Globals, layer_idx: int):
    instructions: list[Instruction] = []
    num_up_gate_blocks = assert_div(
        globs.intermediate_size, globs.up_gate_proj_block_size
    )

    sm_count = globs.sm_count()

    blocks_per_sm = num_up_gate_blocks / sm_count
    assert blocks_per_sm > 1

    for sm_idx in range(sm_count):
        instructions.append(
            LayerNormDoubleMatVecSiLU(
                layer_idx=layer_idx,
                block_idxs=list(range(sm_idx, num_up_gate_blocks, sm_count)),
            )
        )

    return instructions


def schedule_downproj(globs: Globals, layer_idx: int):
    instructions: list[Instruction] = []

    num_down_blocks = assert_div(globs.hidden_size, globs.down_proj_block_size)
    num_col_splits = globs.intermediate_size // globs.hidden_size
    sm_count = globs.sm_count()

    jobs = []
    for col_idx in range(num_col_splits):
        for down_block_idx in range(num_down_blocks):
            jobs.append((col_idx, down_block_idx))

    num_assigned_jobs = 0
    for sm_idx in range(sm_count):
        jobs_left = len(jobs) - num_assigned_jobs
        sms_left = sm_count - sm_idx
        jobs_per_sm = jobs_left / sms_left
        assert jobs_per_sm > 1

        jobs_for_this_sm = round(jobs_per_sm)
        raw_sliced_jobs = jobs[num_assigned_jobs : num_assigned_jobs + jobs_for_this_sm]

        col_idx = raw_sliced_jobs[0][0]
        sliced_jobs = [job for job in raw_sliced_jobs if job[0] == col_idx]
        assert len(sliced_jobs) > 0

        start_output_block_idx = sliced_jobs[0][1]
        output_block_indices = [job[1] for job in sliced_jobs]
        assert output_block_indices == list(
            range(
                start_output_block_idx,
                start_output_block_idx + len(sliced_jobs),
            )
        )

        instructions.append(
            DownProjResidual(
                layer_idx=layer_idx,
                start_block_idx=start_output_block_idx,
                end_block_idx=start_output_block_idx + len(sliced_jobs),
                reduction_block_idx=col_idx,
            )
        )

        num_assigned_jobs += len(sliced_jobs)

    return instructions


def schedule_lm_head(globs: Globals):
    instructions: list[Instruction] = []

    num_logit_blocks = assert_div(globs.vocab_size, globs.lm_head_block_size)
    sm_count = globs.sm_count()

    blocks_per_sm = num_logit_blocks / sm_count
    assert blocks_per_sm > 1

    for sm_idx in range(sm_count):
        start = round(sm_idx * blocks_per_sm)
        end = round((sm_idx + 1) * blocks_per_sm)
        instructions.append(
            RMS_LM_Head(start_output_block_idx=start, end_output_block_idx=end)
        )

    return instructions


def make_dag(
    globs: Globals, stop_after_op: str | None = None, layer_limit: int | None = None
):
    nodes: list[DAG_Node] = []

    if layer_limit is not None:
        nlayers = layer_limit
    else:
        nlayers = globs.num_hidden_layers

    last_outputs = []
    for layer_idx in range(nlayers):
        new_nodes, new_outputs = make_dag_layer(
            globs=globs,
            layer_idx=layer_idx,
            prev_layer_outputs=last_outputs,
            stop_after_op=stop_after_op,
        )
        nodes.extend(new_nodes)
        last_outputs = new_outputs

    if nlayers == globs.num_hidden_layers:
        lm_head_instructions = schedule_lm_head(globs)
        lm_head_nodes: list[DAG_Node] = []
        for ins in lm_head_instructions:
            lm_head_nodes.append(DAG_Node(ins, last_outputs))

        nodes.extend(lm_head_nodes)
        last_outputs = lm_head_nodes

    end_node = DAG_Node(NoOp(), last_outputs)

    return nodes, end_node


def make_dag_layer(
    globs: Globals,
    layer_idx: int,
    prev_layer_outputs: list[DAG_Node],
    stop_after_op: str | None = None,
):
    assert globs.skip_attn_reduction
    num_attention_partitions = 1

    new_nodes: list[DAG_Node] = []

    # qkv
    qkv_instructions = schedule_qkv(globs, layer_idx)
    qkv_nodes: list[DAG_Node] = []
    for ins in qkv_instructions:
        qkv_nodes.append(DAG_Node(ins, prev_layer_outputs))

    qkv_deps = {}

    for node in qkv_nodes:
        ins: LayerNorm_QKV_MatVecRopeAppend = node.instruction
        for block_idx in ins.block_indices():
            qkv_deps[(layer_idx, ins.opcode(), block_idx)] = node

    new_nodes.extend(qkv_nodes)

    if stop_after_op == "qkv":
        return new_nodes, qkv_nodes

    # partial
    partial_nodes: list[DAG_Node] = []

    for kv_head_idx in range(globs.num_kv_heads):
        for partial_idx in range(num_attention_partitions):
            ins = PartialAttention(
                layer_idx=layer_idx,
                kv_head_idx=kv_head_idx,
                num_partials=num_attention_partitions,
                partial_idx=partial_idx,
            )

            block_indices = []

            k_start_dim = (globs.num_attention_heads + kv_head_idx) * globs.head_dim
            v_start_dim = (
                globs.num_attention_heads + globs.num_kv_heads + kv_head_idx
            ) * globs.head_dim

            dims_per_block = assert_div(globs.head_dim, globs.qkv_block_size)

            k_start_block = k_start_dim // globs.qkv_block_size
            v_start_block = v_start_dim // globs.qkv_block_size

            block_indices.extend(
                list(range(k_start_block, k_start_block + dims_per_block))
            )
            block_indices.extend(
                list(range(v_start_block, v_start_block + dims_per_block))
            )

            dep_set = {
                qkv_deps[(layer_idx, PartialAttention.prev_opcode(), block_idx)]
                for block_idx in block_indices
            }
            deps = list(dep_set)

            partial_nodes.append(DAG_Node(ins, deps))

    new_nodes.extend(partial_nodes)

    if stop_after_op == "partial":
        return new_nodes, partial_nodes

    # oproj
    num_o_blocks = assert_div(globs.hidden_size, globs.o_proj_block_size)
    o_proj_nodes: list[DAG_Node] = []
    for o_block_idx in range(num_o_blocks):
        ins = O_ProjResidual(
            layer_idx=layer_idx,
            start_block_idx=o_block_idx,
            end_block_idx=o_block_idx + 1,
            reduction_block_idx=0,
        )

        o_proj_nodes.append(DAG_Node(ins, partial_nodes))

    new_nodes.extend(o_proj_nodes)

    if stop_after_op == "oproj":
        return new_nodes, o_proj_nodes

    # upgate
    upgate_instructions = schedule_upgate(globs, layer_idx)
    upgate_nodes: list[DAG_Node] = []
    for ins in upgate_instructions:
        upgate_nodes.append(DAG_Node(ins, o_proj_nodes))

    new_nodes.extend(upgate_nodes)

    if stop_after_op == "upgate":
        return new_nodes, upgate_nodes

    # downproj
    # TODO we can do better - we can start a reduction col's work once that fraction of the upgate work is done
    downproj_instructions = schedule_downproj(globs, layer_idx)
    downproj_nodes: list[DAG_Node] = []
    for ins in downproj_instructions:
        downproj_nodes.append(DAG_Node(ins, upgate_nodes))

    new_nodes.extend(downproj_nodes)

    if stop_after_op == "downproj":
        return new_nodes, downproj_nodes

    assert stop_after_op is None

    return new_nodes, downproj_nodes


class LatencyScheduleBuilder(ScheduleBuilder):
    @classmethod
    def make_globals(cls, model):
        return make_globals(model)

    @classmethod
    def make_dag(
        cls, globs, stop_after_op: str | None = None, layer_limit: int | None = None
    ):
        return make_dag(globs, stop_after_op, layer_limit)



================================================
FILE: megakernels/demos/throughput/instructions.py
================================================
from dataclasses import dataclass

from torch import Tensor

from megakernels.instructions import BaseGlobals, Instruction
from megakernels.utils import assert_div


@dataclass
class Globals(BaseGlobals):
    # activation buffers
    rms_rope_intermediates: Tensor
    rms_gate_intermediates: Tensor
    rms_lm_head_intermediates: Tensor

    post_ln_rope_q: Tensor
    attn_out: Tensor
    silu_out: Tensor
    logits: Tensor

    batch_size: int

    matmul_batch_block_size: int
    matmul_output_block_size: int
    norm_block_size: int

    def qkv_dim(self) -> int:
        return (self.num_attention_heads + self.num_kv_heads * 2) * self.head_dim

    def num_batch_blocks(self) -> int:
        return assert_div(self.batch_size, self.matmul_batch_block_size)

    def num_output_blocks(self) -> int:
        return assert_div(self.hidden_size, self.matmul_output_block_size)

    def num_intermediate_blocks(self) -> int:
        return assert_div(self.intermediate_size, self.matmul_output_block_size)

    def num_vocab_blocks(self) -> int:
        return assert_div(self.vocab_size, self.matmul_output_block_size)


@dataclass
class ComputeInstruction(Instruction):
    @classmethod
    def tags(cls):
        return {"pool": "compute"}


@dataclass
class MemoryInstruction(Instruction):
    @classmethod
    def tags(cls):
        return {"pool": "memory"}


@dataclass
class PreAttnLayerNorm(MemoryInstruction):
    """
    pre-attention layernorm
    """

    layer_idx: int
    batch_start_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 1

    @classmethod
    def prev_opcode(cls) -> int:
        return DownProjResidual.opcode()


@dataclass
class QKV_MatMulRopeAppend(ComputeInstruction):
    """
    attention: qkv matmul + rope on q and k + append k and v to kv cache
    """

    layer_idx: int
    batch_start_idx: int
    qkv_block_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 2

    @classmethod
    def prev_opcode(cls) -> int:
        return PreAttnLayerNorm.opcode()


@dataclass
class AttentionDecode(MemoryInstruction):
    layer_idx: int
    batch_start_idx: int
    kv_head_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 3

    @classmethod
    def prev_opcode(cls) -> int:
        return QKV_MatMulRopeAppend.opcode()


@dataclass
class MatMulAdd(ComputeInstruction):
    layer_idx: int
    batch_start_idx: int
    output_block_idx: int


# denoting these with separate opcodes so that know what inputs to read from


@dataclass
class O_ProjResidual(MatMulAdd):
    @classmethod
    def opcode(cls) -> int:
        return 4

    @classmethod
    def prev_opcode(cls) -> int:
        return AttentionDecode.opcode()


@dataclass
class PreMLP_Norm(MemoryInstruction):
    """
    layernorm pre-mlp
    """

    layer_idx: int
    batch_start_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 5

    @classmethod
    def prev_opcode(cls) -> int:
        return O_ProjResidual.opcode()


@dataclass
class GateSilu(ComputeInstruction):
    """
    matmul + silu
    """

    layer_idx: int
    batch_start_idx: int
    output_block_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 6

    @classmethod
    def prev_opcode(cls) -> int:
        return PreMLP_Norm.opcode()


@dataclass
class UpMatMul(ComputeInstruction):
    """
    matmul + gate
    """

    layer_idx: int
    batch_start_idx: int
    output_block_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 7

    @classmethod
    def prev_opcode(cls) -> int:
        return GateSilu.opcode()


@dataclass
class DownProjResidual(MatMulAdd):
    @classmethod
    def opcode(cls) -> int:
        return 8

    @classmethod
    def prev_opcode(cls) -> int:
        return UpMatMul.opcode()


@dataclass
class PreLMHeadRMS(MemoryInstruction):
    # layer idx is fake but convenient so we can reuse cuda code
    layer_idx: int
    batch_start_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 9

    @classmethod
    def prev_opcode(cls) -> int:
        return DownProjResidual.opcode()


@dataclass
class LM_Head(ComputeInstruction):
    batch_start_idx: int
    output_block_idx: int

    @classmethod
    def opcode(cls) -> int:
        return 10

    @classmethod
    def prev_opcode(cls) -> int:
        return PreLMHeadRMS.opcode()



================================================
FILE: megakernels/demos/throughput/mk.py
================================================
from einops import rearrange

from megakernels.demos.throughput.instructions import Globals
from megakernels.mk import MK_Interpreter


def interpret_with_mk(
    globs: Globals,
    mk_func,
):
    fourD_k_cache = rearrange(globs.k_cache, "l b t h d -> (l b) t h d")
    fourD_v_cache = rearrange(globs.v_cache, "l b t h d -> (l b) t h d")

    mk_func(
        # vm stuff
        globs.barriers,
        globs.instructions,
        globs.timings,
        # weights
        globs.qkv_proj_weights,
        globs.attn_ln_weights,
        globs.o_proj_weights,
        globs.mlp_ln_weights,
        globs.up_proj_weights,
        globs.gate_proj_weights,
        globs.down_proj_weights,
        globs.lm_head_norm_weights.data,
        globs.lm_head_weights.data,
        fourD_k_cache,
        fourD_v_cache,
        # rope
        globs.rope_cos,
        globs.rope_sin,
        # activations
        globs.hidden_states,
        globs.rms_rope_intermediates,
        globs.rms_gate_intermediates,
        # globs.gate_silu_intermediates,
        globs.silu_out,
        globs.post_ln_rope_q,
        globs.attn_out,
        globs.silu_out,
        globs.rms_lm_head_intermediates,
        globs.logits,
        globs.pos_id,
        globs.attn_scale,
        globs.rms_norm_eps,
        globs.batch_size,
    )

    # mk_llama(
    #     Bar,
    #     instructions,
    #     timings,
    #     qkv_weights,
    #     attn_norm_weights,
    #     o_weights,
    #     mlp_norm_weights,
    #     up_weights,
    #     gate_weights,
    #     down_weights,
    #     lm_head_norm_weights,
    #     lm_head_weights,
    #     k_cache,
    #     v_cache,
    #     rope_cos,
    #     rope_sin,
    #     hidden_states,
    #     rms_rope_intermediates,
    #     rms_gate_intermediates,
    #     gate_silu_intermediates,
    #     q_post_rope,
    #     attn_out,
    #     silu_out,
    #     logits,
    #     pos_id,
    #     attn_scale,
    #     rms_norm_eps
    # )

    # mk_func(
    #     # vm stuff
    #     globs.barriers,
    #     globs.instructions,
    #     globs.timings,
    #     # weights
    #     globs.qkv_proj,
    #     globs.attn_ln_weight,
    #     globs.o_proj,
    #     globs.mlp_ln_weight,
    #     globs.up_proj,
    #     globs.gate_proj,
    #     globs.down_proj,
    #     globs.lm_head_norm_weights.data,
    #     globs.lm_head_weights.data,
    #     globs.k_cache,
    #     globs.v_cache,
    #     # rope
    #     globs.rope_cos,
    #     globs.rope_sin,
    #     # activations
    #     globs.hidden_states,
    #     globs.post_ln_rope_q,
    #     globs.attn_out,
    #     globs.silu_out,
    #     globs.logits,
    #     # scalars
    #     globs.pos_id,
    #     globs.attn_scale,
    #     globs.rms_norm_eps,
    # )


class ThroughputMK_Interpreter(MK_Interpreter):
    def interpret(self, globs: Globals):
        interpret_with_mk(globs, self.mk_func)



================================================
FILE: megakernels/demos/throughput/python_vm.py
================================================
import torch
import torch.nn.functional as F
from einops import einsum, rearrange
from torch import Tensor

from megakernels.demos.throughput.instructions import (
    AttentionDecode,
    DownProjResidual,
    GateSilu,
    Globals,
    LM_Head,
    O_ProjResidual,
    PreAttnLayerNorm,
    PreLMHeadRMS,
    PreMLP_Norm,
    QKV_MatMulRopeAppend,
    UpMatMul,
)
from megakernels.llama import (
    apply_rotary_pos_emb,
)
from megakernels.python_vm import get_start_end, rms_norm
from megakernels.utils import assert_div


def matmul(
    matA: Tensor,
    matB: Tensor,
):
    out = einsum(matA, matB, "a i, b i -> a b")
    return out


def matmul_with_residual(
    matA: Tensor,
    matB: Tensor,
    residual: Tensor,
):
    matmul_out = matmul(matA, matB)

    residual += matmul_out


def pre_attn_layer_norm(
    globals: Globals,
    instruction: PreAttnLayerNorm,
):
    layer_idx = instruction.layer_idx
    batch_idx = instruction.batch_start_idx
    batch_block_idx = batch_idx // globals.matmul_batch_block_size

    if layer_idx > 0:
        assert (
            globals.barriers[
                layer_idx - 1, instruction.prev_opcode() - 1, batch_block_idx, 0
            ]
            == globals.num_output_blocks()
        )

    pre_attn_ln = rms_norm(
        inp=globals.hidden_states[batch_idx],
        weight=globals.attn_ln_weights[layer_idx],
        eps=globals.rms_norm_eps,
    )

    globals.rms_rope_intermediates[batch_idx] = pre_attn_ln

    # barrier update
    batch_block_idx = batch_idx // globals.matmul_batch_block_size
    globals.barriers[layer_idx, instruction.opcode() - 1, batch_block_idx, 0] += 1


def qkv_matmul_rope_append(
    globals: Globals,
    instruction: QKV_MatMulRopeAppend,
):
    layer_idx = instruction.layer_idx
    batch_start_row = instruction.batch_start_idx * globals.matmul_batch_block_size
    batch_end_row = batch_start_row + globals.matmul_batch_block_size

    output_block_idx = instruction.qkv_block_idx
    output_start_col = output_block_idx * globals.matmul_output_block_size
    output_end_col = output_start_col + globals.matmul_output_block_size

    pos_id = globals.pos_id

    assert (
        globals.barriers[
            instruction.layer_idx,
            instruction.prev_opcode() - 1,
            instruction.batch_start_idx,
            0,
        ]
        == globals.matmul_batch_block_size
    )

    matmul_output = einsum(
        globals.qkv_proj_weights[layer_idx, output_start_col:output_end_col],
        globals.rms_rope_intermediates[batch_start_row:batch_end_row],
        "o i, b i -> b o",
    )

    k_start = globals.num_attention_heads * globals.head_dim
    v_start = k_start + globals.num_kv_heads * globals.head_dim

    start, end = get_start_end(globals.matmul_output_block_size, output_block_idx)

    if start < k_start:
        mode = "q"
    elif start < v_start:
        mode = "k"
    else:
        mode = "v"

    output = matmul_output

    if mode in "qk":
        arr = rearrange(output, "... (h d) -> ... h d", d=globals.head_dim)

        # not interleaved for big-batch version
        with_rope, _ = apply_rotary_pos_emb(
            q=arr,
            k=arr,
            cos=globals.rope_cos[pos_id],
            sin=globals.rope_sin[pos_id],
            unsqueeze_dim=-2,
        )

        output = rearrange(with_rope, "... h d -> ... (h d)")

    num_generated_heads = assert_div(globals.matmul_output_block_size, globals.head_dim)

    if mode == "q":
        assert end <= k_start
        globals.post_ln_rope_q[batch_start_row:batch_end_row, start:end] = output

    else:
        arr = rearrange(output, "... (h d) -> ... h d", d=globals.head_dim)

        # Determine which head to put the key/value in
        if mode == "k":  # Key
            k_head_idx = assert_div(start - k_start, globals.head_dim)
            globals.k_cache[
                layer_idx,
                batch_start_row:batch_end_row,
                pos_id,
                k_head_idx : k_head_idx + num_generated_heads,
            ] = arr
        else:  # Value
            v_head_idx = (start - v_start) // globals.head_dim
            globals.v_cache[
                layer_idx,
                batch_start_row:batch_end_row,
                pos_id,
                v_head_idx : v_head_idx + num_generated_heads,
            ] = arr

    # Barrier update
    start_bar = output_start_col // globals.head_dim
    end_bar = start_bar + num_generated_heads

    # the asserts are checking 1, so we can assign and not increment
    globals.barriers[
        instruction.layer_idx,
        instruction.opcode() - 1,
        instruction.batch_start_idx,
        start_bar:end_bar,
    ] = 1


def attention_decode(
    globals: Globals,
    instruction: AttentionDecode,
):
    layer_idx = instruction.layer_idx
    batch_idx = instruction.batch_start_idx
    kv_head_idx = instruction.kv_head_idx

    gqa_ratio = globals.num_attention_heads // globals.num_kv_heads
    seq_len = globals.pos_id + 1

    q_head_start, q_head_end = get_start_end(gqa_ratio, kv_head_idx)

    # barrier check
    batch_block_idx = batch_idx // globals.matmul_batch_block_size
    bars = globals.barriers[layer_idx, instruction.prev_opcode() - 1, batch_block_idx]
    for i in range(gqa_ratio):
        assert bars[kv_head_idx * gqa_ratio + i] == 1
    assert bars[globals.num_attention_heads + kv_head_idx] == 1
    assert bars[globals.num_attention_heads + globals.num_kv_heads + kv_head_idx] == 1

    all_q = rearrange(
        globals.post_ln_rope_q[batch_idx],
        "(h d) -> h d",
        d=globals.head_dim,
    )

    q = all_q[q_head_start:q_head_end]

    k = globals.k_cache[layer_idx, batch_idx, :seq_len, kv_head_idx]
    v = globals.v_cache[layer_idx, batch_idx, :seq_len, kv_head_idx]

    qk = einsum(q.float(), k.float(), "qheads i, seqlen i -> qheads seqlen")
    scaled_qk = qk * globals.attn_scale

    # casting the output of the softmax to 16-bit causes small numerical differences
    softmax = torch.softmax(scaled_qk, dim=-1)

    # lse = torch.logsumexp(scaled_qk, dim=-1)
    # lse = torch.log2(torch.sum(torch.exp(scaled_qk), dim=-1))

    out = einsum(softmax.float(), v.float(), "qheads seqlen, seqlen o -> qheads o")

    all_out = rearrange(globals.attn_out[batch_idx], "(h d) -> h d", d=globals.head_dim)
    all_out[q_head_start:q_head_end] = out

    globals.barriers[layer_idx, instruction.opcode() - 1, batch_block_idx, 0] += 1


def o_proj_residual(
    globals: Globals,
    instruction: O_ProjResidual,
):
    layer_idx = instruction.layer_idx
    batch_idx = instruction.batch_start_idx
    batch_start_row, batch_end_row = get_start_end(
        globals.matmul_batch_block_size, batch_idx
    )
    output_start_col, output_end_col = get_start_end(
        globals.matmul_output_block_size, instruction.output_block_idx
    )

    assert (
        globals.barriers[layer_idx, instruction.prev_opcode() - 1, batch_idx, 0]
        == globals.matmul_batch_block_size * globals.num_kv_heads
    )

    matmul_with_residual(
        matA=globals.attn_out[batch_start_row:batch_end_row],  # [batch, hidden_size]
        matB=globals.o_proj_weights[
            layer_idx, output_start_col:output_end_col
        ],  # [block_size, hidden_size]
        residual=globals.hidden_states[
            batch_start_row:batch_end_row, output_start_col:output_end_col
        ],  # [batch, block_size]
    )

    globals.barriers[layer_idx, instruction.opcode() - 1, batch_idx, 0] += 1


def pre_mlp_layer_norm(
    globals: Globals,
    instruction: PreMLP_Norm,
):
    layer_idx = instruction.layer_idx
    batch_idx = instruction.batch_start_idx
    batch_block_idx = batch_idx // globals.matmul_batch_block_size

    assert (
        globals.barriers[layer_idx, instruction.prev_opcode() - 1, batch_block_idx, 0]
        == globals.num_output_blocks()
    )

    post_norm = rms_norm(
        inp=globals.hidden_states[batch_idx],
        weight=globals.mlp_ln_weights[layer_idx],
        eps=globals.rms_norm_eps,
    )

    globals.rms_gate_intermediates[batch_idx] = post_norm

    # barrier update
    batch_block_idx = batch_idx // globals.matmul_batch_block_size
    globals.barriers[layer_idx, instruction.opcode() - 1, batch_block_idx, 0] += 1


def gate_silu(
    globals: Globals,
    instruction: GateSilu,
):
    layer_idx = instruction.layer_idx
    batch_idx = instruction.batch_start_idx
    out_idx = instruction.output_block_idx
    batch_start_row, batch_end_row = get_start_end(
        globals.matmul_batch_block_size, batch_idx
    )
    output_start_col, output_end_col = get_start_end(
        globals.matmul_output_block_size, out_idx
    )

    assert (
        globals.barriers[layer_idx, instruction.prev_opcode() - 1, batch_idx, 0]
        == globals.matmul_batch_block_size
    )

    # project into gate & apply SiLU
    matmul_out = matmul(  # [B, intermediate]
        matA=globals.rms_gate_intermediates[
            batch_start_row:batch_end_row
        ],  # [B, hidden]
        matB=globals.gate_proj_weights[
            layer_idx, output_start_col:output_end_col
        ],  # [intermediate, hidden]
    )
    post_silu = F.silu(matmul_out)

    globals.silu_out[batch_start_row:batch_end_row, output_start_col:output_end_col] = (
        post_silu
    )

    globals.barriers[layer_idx, instruction.opcode() - 1, batch_idx, out_idx] += 1


def up_matmul(
    globals: Globals,
    instruction: UpMatMul,
):
    layer_idx = instruction.layer_idx
    batch_idx = instruction.batch_start_idx
    out_idx = instruction.output_block_idx
    batch_start_row, batch_end_row = get_start_end(
        globals.matmul_batch_block_size, batch_idx
    )
    output_start_col, output_end_col = get_start_end(
        globals.matmul_output_block_size, out_idx
    )

    assert (
        globals.barriers[layer_idx, instruction.prev_opcode() - 1, batch_idx, out_idx]
        == 1
    )

    # matmul up_proj  and * the SiLU gate
    matmul_out = matmul(  # [B, intermediate]
        matA=globals.rms_gate_intermediates[
            batch_start_row:batch_end_row
        ],  # [B, hidden]
        matB=globals.up_proj_weights[
            layer_idx, output_start_col:output_end_col
        ],  # [intermediate, hidden]
    )
    gated = (
        matmul_out
        * globals.silu_out[
            batch_start_row:batch_end_row, output_start_col:output_end_col
        ]
    )

    globals.silu_out[batch_start_row:batch_end_row, output_start_col:output_end_col] = (
        gated
    )

    globals.barriers[layer_idx, instruction.opcode() - 1, batch_idx, 0] += 1


def down_proj_residual(
    globals: Globals,
    instruction: DownProjResidual,
):
    layer_idx = instruction.layer_idx
    batch_idx = instruction.batch_start_idx
    out_idx = instruction.output_block_idx
    batch_start_row, batch_end_row = get_start_end(
        globals.matmul_batch_block_size, batch_idx
    )
    output_start_col, output_end_col = get_start_end(
        globals.matmul_output_block_size, out_idx
    )

    assert (
        globals.barriers[layer_idx, instruction.prev_opcode() - 1, batch_idx, 0]
        == globals.num_intermediate_blocks()
    )

    matmul_with_residual(
        matA=globals.silu_out[
            batch_start_row:batch_end_row
        ],  # [batch, intermediate_size]
        matB=globals.down_proj_weights[
            layer_idx, output_start_col:output_end_col
        ],  # [block_size, intermediate_size]
        residual=globals.hidden_states[
            batch_start_row:batch_end_row, output_start_col:output_end_col
        ],  # [batch, block_size]
    )

    globals.barriers[layer_idx, instruction.opcode() - 1, batch_idx, 0] += 1


def pre_lm_head_rms(
    globals: Globals,
    instruction: PreLMHeadRMS,
):
    batch_idx = instruction.batch_start_idx
    batch_block_idx = batch_idx // globals.matmul_batch_block_size

    assert (
        globals.barriers[
            globals.num_hidden_layers - 1,
            instruction.prev_opcode() - 1,
            batch_block_idx,
            0,
        ]
        == globals.num_output_blocks()
    )

    post_ln = rms_norm(
        inp=globals.hidden_states[batch_idx],
        weight=globals.lm_head_norm_weights,
        eps=globals.rms_norm_eps,
    )

    globals.rms_lm_head_intermediates[batch_idx] = post_ln

    globals.barriers[
        globals.num_hidden_layers - 1,
        instruction.opcode() - 1,
        batch_block_idx,
        0,
    ] += 1


# TODO Split this out for the throughput setting into RMS and LM Head
def lm_head(
    globals: Globals,
    instruction: LM_Head,
):
    batch_idx = instruction.batch_start_idx
    out_idx = instruction.output_block_idx
    batch_start_row, batch_end_row = get_start_end(
        globals.matmul_batch_block_size, batch_idx
    )
    output_start_col, output_end_col = get_start_end(
        globals.matmul_output_block_size, out_idx
    )

    assert (
        globals.barriers[
            globals.num_hidden_layers - 1, instruction.prev_opcode() - 1, batch_idx, 0
        ]
        == globals.matmul_batch_block_size
    )

    matmul_output = matmul(
        globals.rms_lm_head_intermediates[batch_start_row:batch_end_row],
        globals.lm_head_weights[output_start_col:output_end_col],
    )

    globals.logits[batch_start_row:batch_end_row, output_start_col:output_end_col] = (
        matmul_output
    )


INSTRUCTION_TO_SOLVER = {
    PreAttnLayerNorm: pre_attn_layer_norm,
    QKV_MatMulRopeAppend: qkv_matmul_rope_append,
    AttentionDecode: attention_decode,
    O_ProjResidual: o_proj_residual,
    PreMLP_Norm: pre_mlp_layer_norm,
    GateSilu: gate_silu,
    UpMatMul: up_matmul,
    DownProjResidual: down_proj_residual,
    PreLMHeadRMS: pre_lm_head_rms,
    LM_Head: lm_head,
}



================================================
FILE: megakernels/demos/throughput/scheduler.py
================================================
import math

import torch

from megakernels.demos.throughput.instructions import (
    AttentionDecode,
    DownProjResidual,
    GateSilu,
    Globals,
    Instruction,
    LM_Head,
    O_ProjResidual,
    PreAttnLayerNorm,
    PreLMHeadRMS,
    PreMLP_Norm,
    QKV_MatMulRopeAppend,
    UpMatMul,
)
from megakernels.instructions import NoOp
from megakernels.llama import LlamaForCausalLM
from megakernels.scheduler import DAG_Node, ScheduleBuilder
from megakernels.utils import assert_div


def make_globals(
    model: LlamaForCausalLM,
):
    config = model.config
    device = model.device
    dtype = model.dtype

    def make_buffer(shape_bsz, shape, buffer_dtype=dtype):
        return torch.zeros((shape_bsz, shape), device=device, dtype=buffer_dtype)

    stacked_params = model.stacked_params

    extra_config = model.extra_config
    bs = extra_config.max_batch_size

    matmul_batch_block_size = 128
    matmul_output_block_size = 128
    norm_block_size = 16

    barriers = torch.zeros(
        [
            config.num_hidden_layers,
            10,  # more than the number of opcodes we have
            bs,
            max(
                config.num_attention_heads + config.num_key_value_heads * 2,
                assert_div(config.intermediate_size, matmul_output_block_size),
            ),
        ],
        dtype=torch.int32,
        device=device,
    )

    return Globals(
        # model params
        qkv_proj_weights=stacked_params.qkv_proj,
        o_proj_weights=stacked_params.o_proj,
        attn_ln_weights=stacked_params.attn_ln_weight,
        mlp_ln_weights=stacked_params.mlp_ln_weight,
        up_proj_weights=stacked_params.up_proj,
        gate_proj_weights=stacked_params.gate_proj,
        down_proj_weights=stacked_params.down_proj,
        lm_head_norm_weights=model.lm_head.input_norm.weight,
        lm_head_weights=model.lm_head.lm_head.weight,
        k_cache=model.stacked_kv_cache[0],
        v_cache=model.stacked_kv_cache[1],
        rope_cos=model.model.rope_cos,
        rope_sin=model.model.rope_sin,
        # activation buffers
        hidden_states=make_buffer(bs, config.hidden_size),
        rms_rope_intermediates=make_buffer(bs, config.hidden_size),
        rms_gate_intermediates=make_buffer(bs, config.hidden_size),
        rms_lm_head_intermediates=make_buffer(bs, config.hidden_size),
        post_ln_rope_q=make_buffer(bs, config.hidden_size),
        attn_out=make_buffer(bs, config.hidden_size),
        silu_out=make_buffer(bs, config.intermediate_size),
        logits=make_buffer(bs, config.vocab_size),
        # scalars
        pos_id=0,
        attn_scale=1 / math.sqrt(config.head_dim),
        rms_norm_eps=config.rms_norm_eps,
        num_hidden_layers=config.num_hidden_layers,
        num_attention_heads=config.num_attention_heads,
        num_kv_heads=config.num_key_value_heads,
        head_dim=config.head_dim,
        hidden_size=config.hidden_size,
        intermediate_size=config.intermediate_size,
        # block sizes
        batch_size=bs,
        matmul_batch_block_size=matmul_batch_block_size,
        matmul_output_block_size=matmul_output_block_size,
        norm_block_size=norm_block_size,
        vocab_size=config.vocab_size,
        device=device,
        barriers=barriers,
    )


def schedule_pre_attn_norm(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:  # Returns instructions and a flag to stop
    instructions: list[Instruction] = []

    for bidx in range(0, globs.batch_size):
        instructions.append(PreAttnLayerNorm(layer_idx=layer_idx, batch_start_idx=bidx))

    return instructions


def schedule_qkv_matmul_rope_append(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    num_batch_blocks = assert_div(globs.batch_size, globs.matmul_batch_block_size)
    num_qkv_blocks = assert_div(
        globs.qkv_dim(),
        globs.matmul_output_block_size,
    )

    for bidx in range(0, num_batch_blocks):
        for qkv_block_idx in range(num_qkv_blocks):
            instructions.append(
                QKV_MatMulRopeAppend(
                    layer_idx=layer_idx,
                    batch_start_idx=bidx,
                    qkv_block_idx=qkv_block_idx,
                )
            )

    return instructions


def schedule_attention_decode(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(0, globs.batch_size):
        for kv_head_idx in range(globs.num_kv_heads):
            instructions.append(
                AttentionDecode(
                    layer_idx=layer_idx,
                    batch_start_idx=bidx,
                    kv_head_idx=kv_head_idx,
                ),
            )
    return instructions


def schedule_o_proj_residual(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(globs.num_batch_blocks()):
        for o_block_idx in range(globs.num_output_blocks()):
            instructions.append(
                O_ProjResidual(
                    layer_idx=layer_idx,
                    batch_start_idx=bidx,
                    output_block_idx=o_block_idx,
                )
            )

    return instructions


def schedule_pre_mlp_norm(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(0, globs.batch_size):
        instructions.append(
            PreMLP_Norm(
                layer_idx=layer_idx,
                batch_start_idx=bidx,
            )
        )

    return instructions


def schedule_gate_silu(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(globs.num_batch_blocks()):
        for block_idx in range(globs.num_intermediate_blocks()):
            instructions.append(
                GateSilu(
                    layer_idx=layer_idx,
                    batch_start_idx=bidx,
                    output_block_idx=block_idx,
                )
            )

    return instructions


def schedule_up_matmul(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(globs.num_batch_blocks()):
        for block_idx in range(globs.num_intermediate_blocks()):
            instructions.append(
                UpMatMul(
                    layer_idx=layer_idx,
                    batch_start_idx=bidx,
                    output_block_idx=block_idx,
                )
            )

    return instructions


def schedule_down_proj_residual(
    globs: Globals,
    layer_idx: int,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(globs.num_batch_blocks()):
        for down_block_idx in range(globs.num_output_blocks()):
            instructions.append(
                DownProjResidual(
                    layer_idx=layer_idx,
                    batch_start_idx=bidx,
                    output_block_idx=down_block_idx,
                )
            )
    return instructions


def schedule_lm_head_norm(
    globs: Globals,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(0, globs.batch_size):
        instructions.append(
            PreLMHeadRMS(
                layer_idx=0,
                batch_start_idx=bidx,
            )
        )

    return instructions


def schedule_lm_head(
    globs: Globals,
) -> tuple[list[Instruction], bool]:
    instructions: list[Instruction] = []

    for bidx in range(globs.num_batch_blocks()):
        for logit_block_idx in range(globs.num_vocab_blocks()):
            instructions.append(
                LM_Head(
                    batch_start_idx=bidx,
                    output_block_idx=logit_block_idx,
                )
            )

    return instructions


def make_dag(
    globs: Globals, stop_after_op: str | None = None, layer_limit: int | None = None
):
    nodes: list[DAG_Node] = []

    if layer_limit is not None:
        nlayers = layer_limit
    else:
        nlayers = globs.num_hidden_layers

    last_outputs = []
    for layer_idx in range(nlayers):
        new_nodes, new_outputs = make_dag_layer(
            globs=globs,
            layer_idx=layer_idx,
            prev_layer_outputs=last_outputs,
            stop_after_op=stop_after_op,
        )
        nodes.extend(new_nodes)
        last_outputs = new_outputs

    if nlayers == globs.num_hidden_layers:
        lm_head_norm_instructions = schedule_lm_head_norm(globs)
        lm_head_norm_nodes: list[DAG_Node] = []
        for ins in lm_head_norm_instructions:
            lm_head_norm_nodes.append(DAG_Node(ins, last_outputs))

        nodes.extend(lm_head_norm_nodes)
        last_outputs = lm_head_norm_nodes

        if stop_after_op != "lm_head_norm":
            lm_head_instructions = schedule_lm_head(globs)
            lm_head_nodes: list[DAG_Node] = []

            # TODO: these dependencies are broken bc
            # the lm head norm and matmul is split into two instructions
            for ins in lm_head_instructions:
                lm_head_nodes.append(DAG_Node(ins, last_outputs))

            nodes.extend(lm_head_nodes)
            last_outputs = lm_head_nodes

    end_node = DAG_Node(NoOp(), last_outputs)

    return nodes, end_node


def make_dag_layer(
    globs: Globals,
    layer_idx: int,
    prev_layer_outputs: list[DAG_Node],
    stop_after_op: str | None = None,
):
    new_nodes: list[DAG_Node] = []

    # INSTRUCTION 1: PreAttnNorm
    ins = schedule_pre_attn_norm(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    if stop_after_op == "attn_norm":
        return new_nodes, []

    # INSTRUCTION 2: QKV_MatMulRopeAppend
    ins = schedule_qkv_matmul_rope_append(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    if stop_after_op == "qkv":
        return new_nodes, []

    # INSTRUCTION 3: AttentionDecode
    # Need to figure out how to give in kv_indices
    ins = schedule_attention_decode(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    if stop_after_op == "attn":
        return new_nodes, []

    # INSTRUCTION 4: O_ProjResidual
    ins = schedule_o_proj_residual(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    if stop_after_op == "oproj":
        return new_nodes, []

    ###########################################
    # MLP
    ###########################################

    # INSTRUCTION 5: PreMLPNorm
    ins = schedule_pre_mlp_norm(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    if stop_after_op == "mlp_norm":
        return new_nodes, []

    # INSTRUCTION 6: GateSilu
    ins = schedule_gate_silu(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    if stop_after_op == "gate":
        return new_nodes, []

    # INSTRUCTION 7: UpMatMul
    ins = schedule_up_matmul(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    if stop_after_op == "up":
        return new_nodes, []

    # INSTRUCTION 8: DownProjResidual
    ins = schedule_down_proj_residual(globs, layer_idx)
    new_nodes.extend([DAG_Node(i, prev_layer_outputs) for i in ins])

    assert stop_after_op is None

    return new_nodes, []


class ThroughputScheduleBuilder(ScheduleBuilder):
    @classmethod
    def make_globals(cls, model):
        return make_globals(model)

    @classmethod
    def make_dag(
        cls, globs, stop_after_op: str | None = None, layer_limit: int | None = None
    ):
        return make_dag(globs, stop_after_op, layer_limit)



================================================
FILE: megakernels/scripts/bench_engines.py
================================================
import subprocess
import time
from contextlib import contextmanager
from pathlib import Path
from statistics import mean, stdev

import psutil
import pydra
from openai import OpenAI
from tqdm import tqdm
from transformers import AutoTokenizer


class ScriptConfig(pydra.Config):
    model: str = "meta-llama/Llama-3.2-1B-Instruct"
    prompt_len: int | None = 64
    prompt: str | None = None
    output_len: int = 128
    batch_size: int = 1
    num_warmup: int = 5
    num_iters: int = 10
    env: str | None = None
    conda_activate_path: Path = Path("~/miniconda3/bin/activate").expanduser()
    port: int = 10210
    launch: str | None = None
    api_key: str = "letmein"
    temperature: float = 0.0

    def finalize(self):
        if self.prompt_len is None:
            assert self.prompt is not None
            tokenizer = AutoTokenizer.from_pretrained(self.model)
            self.prompt_len = len(tokenizer.encode(self.prompt))

    def l1(self):
        self.model = "meta-llama/Llama-3.2-1B-Instruct"

    def l8(self):
        self.model = "meta-llama/Llama-3.1-8B-Instruct"


def prepend_conda_activate(command: str, activate_path: str, env: str):
    return f"source {activate_path} && conda activate {env} && {command}"


def kill_process_tree(pid):
    try:
        parent = psutil.Process(pid)
        children = parent.children(recursive=True)

        for child in children:
            child.kill()

        parent.kill()

    except psutil.NoSuchProcess:
        pass


def wait_for_startup(
    process: subprocess.Popen,
    port: int,
    model: str,
    max_retries: int = 500,
    retry_seconds: float = 2,
):
    client = OpenAI(
        base_url=f"http://localhost:{port}/v1",
        api_key="letmein",
        max_retries=0,
        timeout=20,
    )

    for i in range(max_retries):
        if process.poll() is not None:
            raise RuntimeError(f"Server crashed with returncode {process.returncode}")

        try:
            client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "tell me a funny joke about cookies"}
                ],
                max_tokens=10,
            )
            return
        except Exception:
            print(f"Server not yet started (attempt {i}) retrying...")
            time.sleep(retry_seconds)

    raise RuntimeError(f"Server not started after {max_retries} attempts.")


@contextmanager
def launch_server(config: ScriptConfig):
    if config.launch is None:
        yield None
        return

    if config.launch == "input":
        config.launch = input("Enter the command to launch the server: ")

    command = config.launch
    if config.env is not None:
        command = prepend_conda_activate(
            command, config.conda_activate_path, config.env
        )

    print(f"Starting server with command: '{command}'")
    server_process = subprocess.Popen(command, shell=True, executable="/bin/bash")
    print(f"Started server with pid {server_process.pid}")

    try:
        wait_for_startup(
            server_process, config.port, config.model, max_retries=500, retry_seconds=2
        )
        yield
    finally:
        print(f"Killing server (pid {server_process.pid})...")
        kill_process_tree(server_process.pid)
        print("Done killing server.")


def go(config: ScriptConfig, client: OpenAI, n_in: int, n_out: int, batch_size: int):
    times = []

    for i in tqdm(range(config.num_warmup + config.num_iters)):
        start = time.time()
        resp = client.completions.create(
            model=config.model,
            prompt=[0] * n_in,
            max_tokens=n_out,
            temperature=config.temperature,
            n=batch_size,
            extra_body={"ignore_eos": True},
        )
        end = time.time()
        assert resp.usage.completion_tokens == batch_size * n_out

        if i >= config.num_warmup:
            times.append(end - start)

    return mean(times), stdev(times)


def main(config: ScriptConfig):
    print(f"Running with config: {config.to_dict()}")

    with launch_server(config):
        client = OpenAI(
            api_key="fake-key",
            base_url=f"http://0.0.0.0:{config.port}/v1",
        )

        baseline_mean, baseline_stdev = go(
            config,
            client,
            n_in=config.prompt_len,
            n_out=1,
            batch_size=config.batch_size,
        )

        run_mean, run_stdev = go(
            config,
            client,
            n_in=config.prompt_len,
            n_out=config.output_len,
            batch_size=config.batch_size,
        )

        print(f"Baseline: {baseline_mean}  {baseline_stdev}")
        print(f"Run: {run_mean}  {run_stdev}")

        diff = run_mean - baseline_mean
        tokens = config.batch_size * (config.output_len - 1)
        tps = tokens / diff
        print(f"Throughput: {tps} tokens/s")


if __name__ == "__main__":
    pydra.run(main)



================================================
FILE: megakernels/scripts/diff_test.py
================================================
import pickle
import time
from pathlib import Path

import pydra
import torch
from torch.nn.init import normal_
from tqdm import tqdm

from megakernels.dispatch import (
    make_mk_interpreter,
    make_pyvm_interpreter,
    make_schedule_builder,
)
from megakernels.llama import ExtraModelConfig, LlamaForCausalLM
from megakernels.scheduler import (
    assign_to_sms,
    tensorize_instructions,
)


class ScriptConfig(pydra.Config):
    mk_dir: Path = Path(__file__).parent.parent.parent / "demos" / "low-latency-llama"
    model: str = "meta-llama/Llama-3.2-1B-Instruct"
    device: str = "cuda:0"
    prompt_len: int = 10
    ntok: int = 10
    stop_after_op: str | None = None
    start_after_op: str | None = None
    layer_limit: int | None = 1
    skip_pyvm: bool = False
    instruction_reps: int = 1
    exec_reps: int = 1
    skip_starting_instructions: bool = False
    barrier_init_val: int = 0
    truncate_instructions: int | None = None
    bp: bool = False
    outfile: Path | None = None
    noops: bool = False
    max_len_override: int | None = 16384
    sched: str = "rr"
    setting: str = "latency"
    batch_size: int = 1
    skip_cost: bool = False
    interleave_rope: bool = True

    def full(self):
        self.layer_limit = None

    def th(self, bs=1024, sl=128):
        self.setting = "throughput"
        self.mk_dir = (
            Path(__file__).parent.parent.parent.parent
            / "tests"
            / "batch-vm"
            / "llama_official"
        )
        self.batch_size = bs
        self.skip_cost = True
        self.max_len_override = sl
        self.interleave_rope = False
        self.l8()

    def l1(self):
        self.model = "meta-llama/Llama-3.2-1B-Instruct"

    def l8(self):
        self.model = "meta-llama/Llama-3.1-8B-Instruct"


def main(config: ScriptConfig):
    torch.manual_seed(0)
    torch.cuda.set_device(config.device)

    extra_config = ExtraModelConfig(
        interleave_rope=config.interleave_rope,
        max_len_override=config.max_len_override,
        max_batch_size=config.batch_size,
    )

    model = LlamaForCausalLM.from_pretrained(
        config.model, extra_config=extra_config, device=config.device
    )

    builder = make_schedule_builder(config.setting)
    mk_interpreter = make_mk_interpreter(config.setting, config.mk_dir)
    pyvm_interpreter = make_pyvm_interpreter(config.setting)

    spy = builder.build(
        model=model,
        layer_limit=config.layer_limit,
        stop_after_op=config.stop_after_op,
    )

    smk = builder.with_new_globals(spy, model)

    gpy = spy.globs
    gmk = smk.globs

    seq_len = config.prompt_len + config.ntok
    pos_id = seq_len - 1

    gpy.pos_id = pos_id
    gmk.pos_id = pos_id

    normal_(gpy.hidden_states)
    gmk.hidden_states.copy_(gpy.hidden_states)

    # NOTE: important to clone the KV caches since these originally come from the model
    # and so are the same tensor (and therefore we don't want the kv cache
    # append from one to affect the other).
    normal_(gpy.k_cache[:, :, :seq_len])
    normal_(gpy.v_cache[:, :, :seq_len])
    normal_(gpy.k_cache[:, :, seq_len:], std=100)
    normal_(gpy.v_cache[:, :, seq_len:], std=100)

    smk.globs.k_cache = spy.globs.k_cache.clone()
    smk.globs.v_cache = spy.globs.v_cache.clone()

    instructions = spy.get_linear_instructions()

    if config.start_after_op is not None:
        start_schedule = builder.build(
            model=model,
            stop_after_op=config.start_after_op,
            layer_limit=config.layer_limit,
        )

        starting_instructions = start_schedule.get_linear_instructions()

        assert len(starting_instructions) < len(instructions), (
            f"num starting instructions {len(starting_instructions)} should be less than num total instructions {len(instructions)}"
        )
        for i, i2 in zip(starting_instructions, instructions):
            assert i == i2

        instructions = instructions[len(starting_instructions) :]

        if config.instruction_reps > 1:
            print(f"repeating instructions {config.instruction_reps} times")
            instructions = instructions * config.instruction_reps

        if config.truncate_instructions is not None:
            print(f"truncating instructions to {config.truncate_instructions}")
            instructions = instructions[: config.truncate_instructions]

        assigned_to_sms = assign_to_sms(
            config.sched,
            instructions=instructions,
            sm_count=spy.globs.sm_count(),
        )

    else:
        starting_instructions = []

        start = time.time()
        print(f"assigning to sms with mode {config.sched}...")
        assigned_to_sms = assign_to_sms(
            mode=config.sched,
            schedule=smk,
        )
        end = time.time()
        print(f"assign time: {end - start}")

    queue_lengths = [len(q) for q in assigned_to_sms]
    print(
        f"sm queue lengths: min={min(queue_lengths)}, max={max(queue_lengths)}, mean={sum(queue_lengths) / len(queue_lengths)}"
    )

    if not config.skip_cost:
        cost_per_sm = []
        for sm_queue in assigned_to_sms:
            cost = 0
            for instruction in sm_queue:
                cost += instruction.cost(gpy)
            cost_per_sm.append(cost)

        cost_tensor = torch.tensor(cost_per_sm)
        relative_cost_tensor = cost_tensor / cost_tensor.max()

        print(
            f"cost per sm: min={relative_cost_tensor.min():.2f}, mean={relative_cost_tensor.mean():.2f}"
        )
    else:
        cost_per_sm = None

    tensorize_instructions(gpy, assigned_to_sms)
    tensorize_instructions(gmk, assigned_to_sms)

    gpy.barriers.fill_(config.barrier_init_val)
    gmk.barriers.fill_(config.barrier_init_val)

    if config.noops:
        gpy.instructions.zero_()
        gmk.instructions.zero_()

    for _ in tqdm(range(config.exec_reps)):
        if len(starting_instructions) > 0 and not config.skip_starting_instructions:
            print("running starting instructions...")

            # run all the starting instructions with pyvm
            start = time.time()
            pyvm_interpreter.interpret(gpy, starting_instructions)
            pyvm_interpreter.interpret(gmk, starting_instructions)
            torch.cuda.synchronize()
            end = time.time()
            print(f"starting instructions time: {end - start}")

        if not config.skip_pyvm:
            print("interpreting with pyvm...")
            start = time.time()
            pyvm_interpreter.interpret(gpy, instructions)
            torch.cuda.synchronize()
            end = time.time()
            print(f"pyvm time: {end - start}")

        print("interpreting with mk...")
        start = time.time()
        mk_interpreter.interpret(gmk)
        torch.cuda.synchronize()
        end = time.time()
        print(f"mk time: {end - start}")

        print("done! diffing tensors:")

        gpy.diff(gmk)

    if config.bp:
        breakpoint()

    if config.outfile is not None:
        outdata = {
            "timings": gmk.timings.cpu(),
            "instructions": gmk.instructions.cpu(),
            "python_instructions": assigned_to_sms,
            "cost_per_sm": cost_per_sm,
        }

        print(f"Saving to {config.outfile}")

        with open(config.outfile, "wb") as f:
            pickle.dump(outdata, f)


if __name__ == "__main__":
    pydra.run(main)



================================================
FILE: megakernels/scripts/generate.py
================================================
import time
from pathlib import Path

import pydra
import torch
from tabulate import tabulate
from tqdm import tqdm
from transformers import AutoTokenizer

from megakernels.dispatch import (
    make_mk_interpreter,
    make_pyvm_interpreter,
    make_schedule_builder,
)
from megakernels.generators import (
    MK_Generator,
    PyTorchGenerator,
    PyVM_Generator,
)
from megakernels.llama import LlamaForCausalLM
from megakernels.model_types import BatchState, ExtraModelConfig
from megakernels.scheduler import (
    assign_to_sms,
    tensorize_instructions,
)


class ScriptConfig(pydra.Config):
    model: str = "meta-llama/Llama-3.2-1B-Instruct"
    device: str = "cuda:0"
    prompt: str = "tell me a funny joke about cookies"
    chat: bool = False
    ntok: int = 100
    mode: str = "model"
    interleave_rope: bool = True
    mk_dir: Path = Path(__file__).parent.parent.parent / "demos" / "low-latency-llama"
    token_details: bool = False
    tokens: bool = True
    num_warmup: int = 5
    num_iters: int = 10
    barrier_fill_val: int = 0
    batch_size: int = 1
    max_len_override: int | None = 16384
    noops: bool = False
    skip_mk: bool = False
    skip_rest: bool = False
    sched: str = "rr"
    setting: str = "latency"
    memory_fraction: float | None = None

    def finalize(self):
        if self.setting == "latency" and self.mode in ["mk", "pyvm"]:
            assert self.interleave_rope, "interleave_rope must be True for mk mode"

    def once(self):
        self.num_warmup = 0
        self.num_iters = 1

    def th(self, bs=1024, sl=128):
        self.setting = "throughput"
        self.mk_dir = (
            Path(__file__).parent.parent.parent.parent
            / "tests"
            / "batch-vm"
            / "llama_official"
        )
        self.batch_size = bs
        self.max_len_override = sl
        self.interleave_rope = False
        self.l8()

        if self.mode == "mk":
            assert self.batch_size == 1024, (
                "must recompile the kernel with new BATCH_SIZE"
            )

    def l1(self):
        self.model = "meta-llama/Llama-3.2-1B-Instruct"

    def l8(self):
        self.model = "meta-llama/Llama-3.1-8B-Instruct"


@torch.inference_mode()
def main(config: ScriptConfig):
    torch.cuda.set_device(config.device)

    tokenizer = AutoTokenizer.from_pretrained(config.model)
    extra_config = ExtraModelConfig(
        interleave_rope=config.interleave_rope,
        max_len_override=config.max_len_override,
        max_batch_size=config.batch_size,
    )
    model = LlamaForCausalLM.from_pretrained(
        config.model, device=config.device, extra_config=extra_config
    )

    if config.chat:
        messages = [
            {"role": "user", "content": config.prompt},
        ]
        tok_inp = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        input_ids_cpu = tokenizer(
            tok_inp, return_tensors="pt", add_special_tokens=False
        )["input_ids"]
    else:
        tok_inp = config.prompt
        input_ids_cpu = tokenizer(
            tok_inp, return_tensors="pt", add_special_tokens=True
        )["input_ids"]

    input_ids = input_ids_cpu.to(model.device)
    prompt_len = input_ids.shape[-1]

    print(f"Input ids shape: {input_ids.shape}")

    position_ids = (
        torch.arange(prompt_len).to(model.device)
        # .unsqueeze(0)
        # .expand(config.batch_size, -1)
    )

    prefill_inp = BatchState(
        input_ids=input_ids,
        position_ids=position_ids,
    )

    prefill_output: BatchState = model(prefill_inp)
    assert prefill_output.output_ids is not None
    new_input_token = prefill_output.output_ids[:, -1:]

    output_tokens = torch.zeros(
        config.batch_size, config.ntok, device=model.device, dtype=torch.long
    )
    output_tokens[:, 0] = new_input_token

    schedule_builder = make_schedule_builder(config.setting)
    schedule = schedule_builder.build(model)
    assigned_to_sms = assign_to_sms(
        config.sched, schedule=schedule, memory_fraction=config.memory_fraction
    )
    tensorize_instructions(schedule.globs, assigned_to_sms)

    match config.mode:
        case "torch":
            gen = PyTorchGenerator(model)
        case "pyvm":
            interpreter = make_pyvm_interpreter(config.setting)
            gen = PyVM_Generator(model, interpreter, schedule)
        case "mk":
            interpreter = make_mk_interpreter(config.setting, config.mk_dir)
            gen = MK_Generator(
                model,
                interpreter,
                schedule,
                barrier_fill_val=config.barrier_fill_val,
                skip_mk=config.skip_mk,
                skip_rest=config.skip_rest,
            )
            if config.noops:
                gen.replace_with_noops()
        case _:
            raise ValueError(f"Invalid mode: {config.mode}")

    times = []
    cpu_times = []
    for _ in tqdm(range(config.num_warmup + config.num_iters)):
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        start_event.record()
        cpu_start = time.time()
        gen.generate(output_tokens, prompt_len, config.ntok - 1)
        cpu_end = time.time()
        end_event.record()
        torch.cuda.synchronize()
        times.append(start_event.elapsed_time(end_event) / 1000)
        cpu_times.append(cpu_end - cpu_start)

    non_warmup_times = times[config.num_warmup :]
    non_warmup_cpu_times = cpu_times[config.num_warmup :]
    elapsed = sum(non_warmup_times) / len(non_warmup_times)
    elapsed_cpu = sum(non_warmup_cpu_times) / len(non_warmup_cpu_times)
    print(f"Average time: {(elapsed * 1000):.2f}ms (CPU: {(elapsed_cpu * 1000):.2f}ms)")

    if config.tokens:
        to_cpu = output_tokens.cpu()
        print("Output ids: ", to_cpu)
        print("Output text: ", tokenizer.batch_decode(to_cpu))

    if config.token_details:
        ids_list = to_cpu.tolist()
        tokens = tokenizer.convert_ids_to_tokens(ids_list)

        table = []
        for i, token in enumerate(tokens):
            pos_id = i + prompt_len
            table.append([i, pos_id, token])

        print("More detailed output:")
        print(tabulate(table, headers=["output id", "position id", "token"]))

    fwd_per_second = (config.ntok - 1) / elapsed
    print(f"Fwd per second: {fwd_per_second:.2f}")
    tokens_per_second = config.batch_size * fwd_per_second
    print(f"Tokens per second: {tokens_per_second:.2f}")


if __name__ == "__main__":
    pydra.run(main)



================================================
FILE: megakernels/scripts/llama_repl.py
================================================
from pathlib import Path

import pydra
import torch
from art import text2art
from transformers import AutoTokenizer, GenerationConfig

from megakernels.dispatch import (
    make_mk_interpreter,
    make_schedule_builder,
)
from megakernels.generators import MK_Generator, PyTorchGenerator
from megakernels.llama import LlamaForCausalLM
from megakernels.model_types import BatchState, ExtraModelConfig
from megakernels.scheduler import (
    assign_to_sms,
    tensorize_instructions,
)


class ScriptConfig(pydra.Config):
    model: str = "meta-llama/Llama-3.2-1B-Instruct"
    device: str = "cuda:0"
    max_tokens_per_turn: int = 1024
    mk_dir: Path = Path(__file__).parent.parent.parent / "demos" / "low-latency-llama"
    mode: str = "mk"
    sched: str = "rr"
    setting: str = "latency"
    memory_fraction: float | None = None
    print_prompt_len: bool = False


@torch.inference_mode()
def main(config: ScriptConfig):
    torch.cuda.set_device(config.device)

    tokenizer = AutoTokenizer.from_pretrained(config.model)

    generation_config = GenerationConfig.from_pretrained(config.model)
    eos_token_ids = generation_config.eos_token_id
    if isinstance(eos_token_ids, int):
        eos_token_ids = [eos_token_ids]

    extra_config = ExtraModelConfig(
        interleave_rope=True,
        max_batch_size=1,
    )
    model = LlamaForCausalLM.from_pretrained(
        config.model, device=config.device, extra_config=extra_config
    )

    messages = []

    schedule_builder = make_schedule_builder(config.setting)
    schedule = schedule_builder.build(model)
    assigned_to_sms = assign_to_sms(
        config.sched, schedule=schedule, memory_fraction=config.memory_fraction
    )
    tensorize_instructions(schedule.globs, assigned_to_sms)

    match config.mode:
        case "mk":
            interpreter = make_mk_interpreter(config.setting, config.mk_dir)
            gen = MK_Generator(
                model,
                interpreter,
                schedule,
                barrier_fill_val=0,
                skip_mk=False,
                skip_rest=False,
            )
        case "torch":
            gen = PyTorchGenerator(model)
        case _:
            raise ValueError(f"Invalid mode: {config.mode}")

    output_tokens = torch.zeros(
        1, config.max_tokens_per_turn, device=model.device, dtype=torch.long
    )

    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    def generate(messages):
        tok_inp = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        input_ids = tokenizer(tok_inp, return_tensors="pt", add_special_tokens=False)[
            "input_ids"
        ].to(model.device)
        prompt_len = input_ids.shape[-1]
        if config.print_prompt_len:
            print(f"Prompt length: {prompt_len}")

        position_ids = torch.arange(prompt_len).to(model.device)

        prefill_inp = BatchState(
            input_ids=input_ids,
            position_ids=position_ids,
        )

        prefill_output: BatchState = model(prefill_inp)
        assert prefill_output.output_ids is not None
        new_input_token = prefill_output.output_ids[:, -1:]

        output_tokens[:, 0] = new_input_token

        start_event.record()
        until_eos, num_generated = gen.generate_with_eos(
            output_tokens=output_tokens,
            prompt_len=prompt_len,
            ntok=config.max_tokens_per_turn,
            eos_token_ids=eos_token_ids,
            eos_token_check_interval=16,
        )
        end_event.record()

        torch.cuda.synchronize()
        elapsed = start_event.elapsed_time(end_event) / 1000

        tokens_per_second = num_generated / elapsed

        to_cpu = output_tokens.cpu()[0, :until_eos]

        output_text = tokenizer.decode(to_cpu, skip_special_tokens=True)

        return output_text, tokens_per_second

    # warmup
    generate([{"role": "user", "content": "hi"}])

    startup_message = "you have\nbeen granted\nan audience\nwith the\nmegakernel"
    print(text2art(startup_message.replace(" ", "   ")))

    while True:
        user_input = input(">>> ")
        messages.append({"role": "user", "content": user_input})

        output_text, tokens_per_second = generate(messages)

        print("Response: ", output_text)
        print(f"Speed: {tokens_per_second:.2f} tokens/s")

        messages.append({"role": "assistant", "content": output_text})


if __name__ == "__main__":
    pydra.run(main)



================================================
FILE: megakernels/scripts/make_torch_profile.py
================================================
import pydra
import torch
from tqdm import tqdm

from megakernels.llama import LlamaForCausalLM
from megakernels.model_types import BatchState, ExtraModelConfig


class ScriptConfig(pydra.Config):
    outfile: str = "proj.json"
    model: str = "meta-llama/Llama-3.2-1B-Instruct"
    device: str = "cuda:0"
    compile: bool = True
    prompt: str = "Hello, world!"
    fullgraph: bool = True
    dynamic: bool = False
    bs: int = 1
    seq_len: int = 1
    num_warmup: int = 3
    num_iters: int = 10
    num_profile_repeat: int = 3
    prof: bool = True


def main(config: ScriptConfig):
    model = LlamaForCausalLM.from_pretrained(
        config.model,
        device=config.device,
        extra_config=ExtraModelConfig(
            torch_compile=config.compile,
            max_batch_size=config.bs,
            max_len_override=config.seq_len,
        ),
    )

    if config.compile:
        print("Compiling model...")
        model = torch.compile(model, fullgraph=config.fullgraph, dynamic=config.dynamic)

    input_ids = torch.ones(
        config.bs, config.seq_len, dtype=torch.long, device=config.device
    )

    position_ids = (
        torch.arange(config.seq_len, device=config.device)
        .unsqueeze(0)
        .expand(config.bs, -1)
    )
    batch_state = BatchState(input_ids=input_ids, position_ids=position_ids)

    num_iters = config.num_profile_repeat * (config.num_iters + config.num_warmup + 1)

    if not config.prof:
        for _ in tqdm(range(num_iters)):
            model(batch_state)

        return

    with torch.profiler.profile(
        activities=[
            torch.profiler.ProfilerActivity.CPU,
            torch.profiler.ProfilerActivity.CUDA,
        ],
        schedule=torch.profiler.schedule(
            wait=1,
            warmup=config.num_warmup,
            active=config.num_iters,
            repeat=config.num_profile_repeat,
        ),
        record_shapes=True,
        profile_memory=True,
        with_stack=True,
    ) as prof:
        for _ in tqdm(
            range(
                config.num_profile_repeat * (config.num_iters + config.num_warmup + 1),
            ),
        ):
            model(batch_state)
            prof.step()

    prof.export_chrome_trace(config.outfile)


if __name__ == "__main__":
    pydra.run(main)



================================================
FILE: util/pyproject.toml
================================================
[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "mk-init"
version = "0.1.0"
description = "Megakernel initialization"
authors = [{name = "bfs", email = "benjaminfspector@gmail.com"}]

[project.scripts]
mk-init = "mk_init.main:main"

[tool.setuptools.packages.find]
include = ["mk_init*"]

[tool.setuptools.package-data]
mk_init = ["sources/**/*"]


================================================
FILE: util/mk_init/__init__.py
================================================
"""Megakernel initialization package."""


================================================
FILE: util/mk_init/main.py
================================================
#!/usr/bin/env python3
"""
Megakernel Project Initialization Script
Similar to 'npm init', this script sets up a new megakernel project structure.
"""

import sys
import argparse
from pathlib import Path


def prompt_user(question, default=None):
    """Prompt user for input with optional default value."""
    if default:
        response = input(f"{question} ({default}): ").strip()
        return response if response else default
    else:
        response = input(f"{question}: ").strip()
        while not response:
            response = input(f"{question} (required): ").strip()
        return response

def replace_placeholders(content, project_name):
    """Replace placeholders in a string with actual values."""
    content = content.replace('{{PROJECT_NAME_LOWER}}', project_name.lower())
    content = content.replace('{{PROJECT_NAME_UPPER}}', project_name.upper())
    content = content.replace('{{PROJECT_NAME}}', project_name)
    return content

def copy_template_file(source_dir, filename, target_dir, project_name):
    """Copy a template file and replace placeholders."""
    source_file = source_dir / filename
    target_file = target_dir / replace_placeholders(filename, project_name)
    print(f"Copying {source_file} to {target_file}")

    if source_file.exists():
        # Read the template file
        with open(source_file, 'r') as f:
            content = f.read()

        # Replace placeholders
        content = replace_placeholders(content, project_name)

        # Write to target
        with open(target_file, 'w') as f:
            f.write(content)

        print(f" Created {filename}")
    else:
        print(f" Template file {filename} not found in sources directory")


def create_project_structure(project_name, target_dir):
    """Create the basic project directory structure."""
    directories = [
        'src',
        'tests',
    ]

    for directory in directories:
        dir_path = target_dir / directory
        dir_path.mkdir(parents=True, exist_ok=True)
        print(f" Created directory {directory}/")


def main():
    parser = argparse.ArgumentParser(description='Initialize a new megakernel project')
    parser.add_argument('--name', help='Project name')
    parser.add_argument('--target', help='Target directory (default: current directory)')
    args = parser.parse_args()

    print(" Megakernel Project Initialization")
    print("=" * 40)

    # Get project name
    if args.name:
        project_name = args.name
    else:
        project_name = prompt_user("Project name")

    # Validate project name
    if not project_name.replace('_', '').replace('-', '').isalnum():
        print(" Project name should only contain letters, numbers, hyphens, and underscores")
        sys.exit(1)

    # Determine target directory
    if args.target:
        target_dir = Path(args.target).resolve()
    else:
        target_dir = Path.cwd() / project_name

    # Check if target directory exists
    if target_dir.exists() and any(target_dir.iterdir()):
        overwrite = input(f"Directory {target_dir} exists and is not empty. Continue? (y/N): ")
        if overwrite.lower() != 'y':
            print("Aborted.")
            sys.exit(1)

    # Create target directory
    target_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n Creating project in: {target_dir}")
    print(f" Project name: {project_name}")
    print()

    # Get the directory where this script is located
    script_dir = Path(__file__).parent
    sources_dir = script_dir / "sources"

    if not sources_dir.exists():
        print(f" Sources directory not found at {sources_dir}")
        print("Please ensure the template files are available in the sources/ directory")
        sys.exit(1)

    # Create project structure
    print("Creating project structure...")
    create_project_structure(project_name, target_dir)

    print("\nCopying template files...")

    # Copy template files
    template_files = [
